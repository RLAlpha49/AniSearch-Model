{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AniSearchModel","text":"<p>  AniSearchModel leverages Sentence-BERT (SBERT) models to generate embeddings for anime and manga synopses, enabling the calculation of semantic similarities between descriptions. This project facilitates the preprocessing, merging, and analysis of various anime and manga datasets to identify the most similar synopses.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Datasets Used</li> <li>Setup</li> <li>Usage</li> <li>Merging Datasets</li> <li>Generating Embeddings<ul> <li>For a Specific Model</li> <li>Generating Embeddings for All Models</li> </ul> </li> <li>Testing Embeddings</li> <li>Running the Flask Application</li> <li>Project Structure</li> <li>Dependencies</li> <li>Contributing</li> <li>License</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>AniSearchModel performs the following operations:</p> <ul> <li>Data Loading and Preprocessing: Loads multiple anime and manga datasets, cleans synopses, consolidates titles, and removes duplicates.</li> <li>Data Merging: Merges datasets based on common identifiers to create unified anime and manga datasets.</li> <li>Embedding Generation: Utilizes SBERT models to generate embeddings for synopses, facilitating semantic similarity calculations.</li> <li>Similarity Analysis: Calculates cosine similarities between embeddings to identify the most similar synopses or descriptions.</li> <li>API Integration: Provides a Flask-based API to interact with the model and retrieve similarity results.</li> <li>Testing: Implements a comprehensive test suite using <code>pytest</code> to ensure the reliability and correctness of all components.</li> </ul>"},{"location":"#datasets-used","title":"Datasets Used","text":""},{"location":"#anime-datasets","title":"Anime Datasets","text":"<ol> <li>MyAnimeList Dataset (<code>Anime.csv</code>): Kaggle</li> <li>Anime Dataset 2023 (<code>anime-dataset-2023.csv</code>): Kaggle</li> <li>Anime Database 2022 (<code>Anime-2022.csv</code>): Kaggle</li> <li>Anime Dataset (<code>animes.csv</code>): Kaggle</li> <li>Anime DataSet (<code>anime4500.csv</code>): Kaggle</li> <li>Anime Data (<code>anime_data.csv</code>): Kaggle</li> <li>Anime2 (<code>anime2.csv</code>): Kaggle</li> <li>MAL Anime (<code>mal_anime.csv</code>): Kaggle</li> <li>Anime 270: Hugging Face</li> <li>Wykonos Anime: Hugging Face</li> </ol>"},{"location":"#manga-datasets","title":"Manga Datasets","text":"<ol> <li>MyAnimeList Manga Dataset (<code>Manga.csv</code>): Kaggle</li> <li>MyAnimeList Jikan Database (<code>jikan.csv</code>): Kaggle</li> <li>Manga, Manhwa and Manhua Dataset (<code>data.csv</code>): Kaggle</li> </ol>"},{"location":"#setup","title":"Setup","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/RLAlpha49/AniSearchModel.git\ncd AniSearchModel\n</code></pre> <ol> <li>Create and activate a virtual environment:</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Linux/Mac\nvenv\\Scripts\\activate     # On Windows\n</code></pre> <ol> <li>Ensure <code>setuptools</code> is installed:</li> </ol> <p>Before running the setup script, make sure <code>setuptools</code> is installed in your virtual environment. This is typically included with Python, but you can update it with:</p> <pre><code>pip install --upgrade setuptools\n</code></pre> <ol> <li>Install the package and dependencies:</li> </ol> <p>Use the <code>setup.py</code> script to install the package along with its dependencies. This will also handle the installation of PyTorch with CUDA support:</p> <pre><code>python setup.py install\n</code></pre> <p>This command will:    - Install all required Python packages listed in <code>install_requires</code>.    - Execute the <code>PostInstallCommand</code> to install PyTorch with CUDA support.</p> <ol> <li>Verify the installation:</li> </ol> <p>After installation, you can verify that PyTorch is using CUDA by running:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre> <p>This should print <code>True</code> if CUDA is available and correctly configured.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#merging-datasets","title":"Merging Datasets","text":"<p>The repository already contains the merged datasets, but if you want to merge additional datasets, edit the <code>merge_datasets.py</code> file and run:</p> <pre><code>python merge_datasets.py --type anime\npython merge_datasets.py --type manga\n</code></pre>"},{"location":"#generating-embeddings","title":"Generating Embeddings","text":"<p>To generate SBERT embeddings for the anime and manga datasets, you can use the provided scripts.</p>"},{"location":"#for-a-specific-model","title":"For a Specific Model","text":"<pre><code>python sbert.py --model &lt;model_name&gt; --type &lt;dataset_type&gt;\n</code></pre> <p>Replace <code>&lt;model_name&gt;</code> with the desired SBERT model, e.g., <code>all-mpnet-base-v1</code>. Replace <code>&lt;dataset_type&gt;</code> with <code>anime</code> or <code>manga</code>.</p>"},{"location":"#generating-embeddings-for-all-models","title":"Generating Embeddings for All Models","text":"<p>You can use the provided scripts to generate embeddings for all models listed in <code>models.txt</code>.</p>"},{"location":"#linux","title":"Linux","text":"<p>The <code>generate_models.sh</code> script is available for Linux users. To run the script, follow these steps:</p> <ol> <li>Make the script executable:</li> </ol> <pre><code>chmod +x generate_models.sh\n</code></pre> <ol> <li>Run the script:</li> </ol> <pre><code>./scripts/generate_models.sh\n</code></pre> <ol> <li>Optionally, specify a starting model:</li> </ol> <pre><code>./scripts/generate_models.sh sentence-transformers/all-MiniLM-L6-v1\n</code></pre>"},{"location":"#windows-batch-script","title":"Windows (Batch Script)","text":"<ol> <li>Open Command Prompt and navigate to the directory containing the script.</li> <li>Run the script:</li> </ol> <pre><code>scripts\\generate_models.bat\n</code></pre> <ol> <li>Optionally, specify a starting model:</li> </ol> <pre><code>scripts\\generate_models.bat sentence-transformers/all-MiniLM-L6-v1\n</code></pre>"},{"location":"#windows-powershell-script","title":"Windows (PowerShell Script)","text":"<ol> <li>Open PowerShell and navigate to the directory containing the script.</li> <li>Run the script:</li> </ol> <pre><code>.\\scripts\\generate_models.ps1\n</code></pre> <ol> <li>Optionally, specify a starting model:</li> </ol> <pre><code>.\\scripts\\generate_models.ps1 -StartModel \"sentence-transformers/all-MiniLM-L6-v1\"\n</code></pre>"},{"location":"#notes","title":"Notes","text":"<ul> <li>The starting model parameter is optional. If not provided, the script will process all models from the beginning of the list.</li> <li>For PowerShell, you may need to adjust the execution policy to allow script execution. You can do this by running <code>Set-ExecutionPolicy RemoteSigned</code> in an elevated PowerShell session.</li> </ul>"},{"location":"#testing-embeddings","title":"Testing Embeddings","text":""},{"location":"#testing","title":"Testing","text":"<p>To ensure the reliability and correctness of the project, a comprehensive suite of tests has been implemented using <code>pytest</code>. The tests cover various components of the project, including:</p>"},{"location":"#unit-tests","title":"Unit Tests","text":"<ul> <li><code>tests/test_model.py</code>:</li> <li>Purpose: Tests the functionality of model loading, similarity calculations, and evaluation result saving.</li> <li> <p>Key Functions Tested:</p> <ul> <li><code>test_anime_model</code>: Verifies that the anime model loads correctly, calculates similarities, and saves evaluation results as expected.</li> <li><code>test_manga_model</code>: Similar to <code>test_anime_model</code> but for the manga dataset.</li> </ul> </li> <li> <p><code>tests/test_merge_datasets.py</code>:</p> </li> <li>Purpose: Validates the data preprocessing and merging functions, ensuring that names are correctly processed, synopses are cleaned, titles are consolidated, and duplicates are removed or handled appropriately.</li> <li> <p>Key Functions Tested:</p> <ul> <li><code>test_preprocess_name</code>: Ensures that names are preprocessed correctly by converting them to lowercase and stripping whitespace.</li> <li><code>test_clean_synopsis</code>: Checks that unwanted phrases are removed from synopses.</li> <li><code>test_consolidate_titles</code>: Verifies that multiple title columns are consolidated into a single 'title' column.</li> <li><code>test_remove_duplicate_infos</code>: Confirms that duplicate synopses are handled correctly.</li> <li><code>test_add_additional_info</code>: Tests the addition of additional synopsis information to the merged DataFrame.</li> </ul> </li> <li> <p><code>tests/test_sbert.py</code>:</p> </li> <li>Purpose: Checks the SBERT embedding generation process, verifying that embeddings are correctly created and saved for both anime and manga datasets.</li> <li>Key Functions Tested:<ul> <li><code>run_sbert_command_and_verify</code>: Runs the SBERT command-line script and verifies that embeddings and evaluation results are generated as expected.</li> <li>Parameterized tests for different dataset types (<code>anime</code>, <code>manga</code>) and their corresponding expected embedding files.</li> </ul> </li> </ul>"},{"location":"#api-tests","title":"API Tests","text":"<ul> <li><code>tests/test_api.py</code>:</li> <li>Purpose: Tests the Flask API endpoints, ensuring that the <code>/anisearchmodel/manga</code> endpoint behaves as expected with valid inputs, handles missing fields gracefully, and correctly responds to internal server errors.</li> <li>Key Functions Tested:<ul> <li><code>test_get_manga_similarities_success</code>: Verifies successful retrieval of similarities with valid inputs.</li> <li><code>test_get_manga_similarities_missing_model</code>: Checks the API's response when the model name is missing.</li> <li><code>test_get_manga_similarities_missing_description</code>: Ensures appropriate handling when the description is missing.</li> <li>Tests for internal server errors by simulating exceptions during processing.</li> </ul> </li> </ul>"},{"location":"#test-configuration","title":"Test Configuration","text":"<ul> <li><code>tests/conftest.py</code>:</li> <li>Purpose: Configures <code>pytest</code> options and fixtures, including command-line options for specifying the model name during tests.</li> <li>Key Features:<ul> <li>Adds a command-line option <code>--model-name</code> to specify the model used in tests.</li> <li>Provides a fixture <code>model_name</code> that retrieves the model name from the command-line options.</li> </ul> </li> </ul>"},{"location":"#running-the-tests","title":"Running the Tests","text":"<p>To run all the tests, navigate to the project's root directory and execute:</p> <pre><code>pytest\n</code></pre>"},{"location":"#running-specific-tests","title":"Running Specific Tests","text":"<p>You can run specific tests or test modules. For example, to run only the API tests:</p> <pre><code>pytest tests/test_api.py\n</code></pre> <p>To run tests for a specific model, use:</p> <pre><code>pytest tests/test_sbert.py --model-name &lt;model_name&gt;\n</code></pre> <p>Replace <code>&lt;model_name&gt;</code> with the name of the model you want to test.</p>"},{"location":"#note","title":"Note","text":"<ul> <li><code>--model-name</code> can be used when running all tests or specific tests.</li> </ul>"},{"location":"#running-the-flask-application","title":"Running the Flask Application","text":"<p>To run the Flask application, use the <code>run_server.py</code> script. This script automatically determines the operating system and uses the appropriate server. You can also specify whether to use CUDA or CPU for processing:</p> <ul> <li>On Linux, it uses Gunicorn.</li> <li>On Windows, it uses Waitress.</li> </ul> <p>Run the script with:</p> <pre><code>python src/run_server.py [cuda|cpu]\n</code></pre> <p>Replace <code>[cuda|cpu]</code> with your desired device. If no device is specified, it defaults to <code>cpu</code>.</p> <p>The application will be accessible at <code>http://0.0.0.0:5000/anisearchmodel</code>.</p>"},{"location":"#project-structure","title":"Project Structure","text":"<p>This includes files and directories generated by the project which are not part of the source code.</p> <pre><code>AniSearchModel\n\u251c\u2500\u2500 .github\n\u2502   \u2514\u2500\u2500 workflows\n\u2502       \u251c\u2500\u2500 codeql.yml\n\u2502       \u2514\u2500\u2500 ruff.yml\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anime\n\u2502   \u2502   \u251c\u2500\u2500 Anime_data.csv\n\u2502   \u2502   \u251c\u2500\u2500 Anime-2022.csv\n\u2502   \u2502   \u251c\u2500\u2500 anime-dataset-2023.csv\n\u2502   \u2502   \u251c\u2500\u2500 anime.csv\n\u2502   \u2502   \u251c\u2500\u2500 Anime2.csv\n\u2502   \u2502   \u251c\u2500\u2500 anime4500.csv\n\u2502   \u2502   \u251c\u2500\u2500 animes.csv\n\u2502   \u2502   \u2514\u2500\u2500 mal_anime.csv\n\u2502   \u2514\u2500\u2500 manga\n\u2502       \u251c\u2500\u2500 data.csv\n\u2502       \u251c\u2500\u2500 jikan.csv\n\u2502       \u2514\u2500\u2500 manga.csv\n\u251c\u2500\u2500 logs\n\u2502   \u2514\u2500\u2500 &lt;filename&gt;.log.&lt;#&gt;\n\u251c\u2500\u2500 models\n\u2502   \u251c\u2500\u2500 anime\n\u2502   \u2502   \u2514\u2500\u2500 &lt;model_name&gt;\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_anime_270_Dataset.npy\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_Anime_data_Dataset.npy\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_anime_dataset_2023.npy\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_Anime-2022_Dataset.npy\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_anime2_Dataset.npy\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_anime4500_Dataset.npy\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_animes_dataset.npy\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_mal_anime_Dataset.npy\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_wykonos_Dataset.npy\n\u2502   \u2502       \u2514\u2500\u2500 embeddings_synopsis.npy\n\u2502   \u251c\u2500\u2500 manga\n\u2502   \u2502   \u2514\u2500\u2500 &lt;model_name&gt;\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_data_Dataset.npy\n\u2502   \u2502       \u251c\u2500\u2500 embeddings_Synopsis_jikan_Dataset.npy\n\u2502   \u2502       \u2514\u2500\u2500 embeddings_synopsis.npy\n\u2502   \u251c\u2500\u2500 evaluation_results_anime.json\n\u2502   \u251c\u2500\u2500 evaluation_results_manga.json\n\u2502   \u251c\u2500\u2500 evaluation_results.json\n\u2502   \u251c\u2500\u2500 merged_anime_dataset.csv\n\u2502   \u2514\u2500\u2500 merged_manga_dataset.csv\n\u251c\u2500\u2500 scripts\n\u2502   \u251c\u2500\u2500 generate_models.bat\n\u2502   \u251c\u2500\u2500 generate_models.ps1\n\u2502   \u2514\u2500\u2500 generate_models.sh\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 api.py\n\u2502   \u251c\u2500\u2500 common.py\n\u2502   \u251c\u2500\u2500 merge_datasets.py\n\u2502   \u251c\u2500\u2500 run_server.py\n\u2502   \u251c\u2500\u2500 sbert.py\n\u2502   \u2514\u2500\u2500 test.py\n\u251c\u2500\u2500 tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u251c\u2500\u2500 test_api.py\n\u2502   \u251c\u2500\u2500 test_merge_datasets.py\n\u2502   \u251c\u2500\u2500 test_model.py\n\u2502   \u2514\u2500\u2500 test_sbert.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 architecture.txt\n\u251c\u2500\u2500 datasets.txt\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 models.txt\n\u251c\u2500\u2500 pytest.ini\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 setup.py\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"<ul> <li>Python 3.6+</li> <li>Python Packages:</li> <li>pandas</li> <li>numpy</li> <li>torch</li> <li>transformers</li> <li>sentence-transformers</li> <li>tqdm</li> <li>datasets</li> <li>flask</li> <li>flask-limiter</li> <li>waitress</li> <li>gunicorn</li> <li>pytest</li> <li>pytest-order</li> </ul> <p>Install all dependencies using:</p> <pre><code>python setup.py install\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please fork the repository and submit a pull request for any enhancements or bug fixes.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"API/","title":"API","text":"<p>This module implements a Flask application that provides API endpoints for finding similar anime or manga descriptions.</p> <p>The application uses Sentence Transformers and custom models to encode descriptions and calculate cosine similarities. It supports multiple synopsis columns from different datasets and returns paginated results of the most similar items.</p> Key Features <ul> <li>Supports multiple pre-trained and custom Sentence Transformer models</li> <li>Handles both anime and manga similarity searches</li> <li>Implements rate limiting and CORS</li> <li>Provides memory management for GPU resources</li> <li>Includes comprehensive logging</li> <li>Returns paginated results with similarity scores</li> </ul> The API endpoints are <ul> <li>POST /anisearchmodel/anime: Find similar anime based on description</li> <li>POST /anisearchmodel/manga: Find similar manga based on description</li> </ul>"},{"location":"API/#src.api.CONSOLE_LOGGING_LEVEL","title":"CONSOLE_LOGGING_LEVEL  <code>module-attribute</code>","text":"<pre><code>CONSOLE_LOGGING_LEVEL = INFO\n</code></pre>"},{"location":"API/#src.api.FILE_LOGGING_LEVEL","title":"FILE_LOGGING_LEVEL  <code>module-attribute</code>","text":"<pre><code>FILE_LOGGING_LEVEL = DEBUG\n</code></pre>"},{"location":"API/#src.api.allowed_models","title":"allowed_models  <code>module-attribute</code>","text":"<pre><code>allowed_models = ['sentence-transformers/all-distilroberta-v1', 'sentence-transformers/all-MiniLM-L6-v1', 'sentence-transformers/all-MiniLM-L12-v1', 'sentence-transformers/all-MiniLM-L6-v2', 'sentence-transformers/all-MiniLM-L12-v2', 'sentence-transformers/all-mpnet-base-v1', 'sentence-transformers/all-mpnet-base-v2', 'sentence-transformers/all-roberta-large-v1', 'sentence-transformers/gtr-t5-base', 'sentence-transformers/gtr-t5-large', 'sentence-transformers/gtr-t5-xl', 'sentence-transformers/multi-qa-distilbert-dot-v1', 'sentence-transformers/multi-qa-mpnet-base-cos-v1', 'sentence-transformers/multi-qa-mpnet-base-dot-v1', 'sentence-transformers/paraphrase-distilroberta-base-v2', 'sentence-transformers/paraphrase-mpnet-base-v2', 'sentence-transformers/sentence-t5-base', 'sentence-transformers/sentence-t5-large', 'sentence-transformers/sentence-t5-xl', 'sentence-transformers/sentence-t5-xxl', 'toobi/anime', 'sentence-transformers/fine_tuned_sbert_anime_model', 'fine_tuned_sbert_anime_model', 'fine_tuned_sbert_model_anime']\n</code></pre>"},{"location":"API/#src.api.anime_df","title":"anime_df  <code>module-attribute</code>","text":"<pre><code>anime_df = read_csv(parent / 'model/merged_anime_dataset.csv')\n</code></pre>"},{"location":"API/#src.api.anime_synopsis_columns","title":"anime_synopsis_columns  <code>module-attribute</code>","text":"<pre><code>anime_synopsis_columns = ['synopsis', 'Synopsis anime_dataset_2023', 'Synopsis animes dataset', 'Synopsis anime_270 Dataset', 'Synopsis Anime-2022 Dataset', 'Synopsis anime4500 Dataset', 'Synopsis wykonos Dataset', 'Synopsis Anime_data Dataset', 'Synopsis anime2 Dataset', 'Synopsis mal_anime Dataset']\n</code></pre>"},{"location":"API/#src.api.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = Flask(__name__)\n</code></pre>"},{"location":"API/#src.api.debug_mode","title":"debug_mode  <code>module-attribute</code>","text":"<pre><code>debug_mode = lower() in ['true', '1']\n</code></pre>"},{"location":"API/#src.api.device","title":"device  <code>module-attribute</code>","text":"<pre><code>device = 'cuda' if getenv('DEVICE', 'cpu') == 'cuda' and is_available() else 'cpu'\n</code></pre>"},{"location":"API/#src.api.file_formatter","title":"file_formatter  <code>module-attribute</code>","text":"<pre><code>file_formatter = Formatter('%(asctime)s - %(levelname)s - %(message)s')\n</code></pre>"},{"location":"API/#src.api.file_handler","title":"file_handler  <code>module-attribute</code>","text":"<pre><code>file_handler = ConcurrentRotatingFileHandler('./logs/api.log', maxBytes=10 * 1024 * 1024, backupCount=10, encoding='utf-8')\n</code></pre>"},{"location":"API/#src.api.last_request_time","title":"last_request_time  <code>module-attribute</code>","text":"<pre><code>last_request_time = time()\n</code></pre>"},{"location":"API/#src.api.last_request_time_lock","title":"last_request_time_lock  <code>module-attribute</code>","text":"<pre><code>last_request_time_lock = Lock()\n</code></pre>"},{"location":"API/#src.api.limiter","title":"limiter  <code>module-attribute</code>","text":"<pre><code>limiter = Limiter(get_remote_address, app=app, default_limits=['1 per second'])\n</code></pre>"},{"location":"API/#src.api.manga_df","title":"manga_df  <code>module-attribute</code>","text":"<pre><code>manga_df = read_csv(parent / 'model/merged_manga_dataset.csv')\n</code></pre>"},{"location":"API/#src.api.manga_synopsis_columns","title":"manga_synopsis_columns  <code>module-attribute</code>","text":"<pre><code>manga_synopsis_columns = ['synopsis', 'Synopsis jikan Dataset', 'Synopsis data Dataset']\n</code></pre>"},{"location":"API/#src.api.script_dir","title":"script_dir  <code>module-attribute</code>","text":"<pre><code>script_dir = parent\n</code></pre>"},{"location":"API/#src.api.stream_formatter","title":"stream_formatter  <code>module-attribute</code>","text":"<pre><code>stream_formatter = Formatter('%(asctime)s - %(levelname)s - %(message)s')\n</code></pre>"},{"location":"API/#src.api.stream_handler","title":"stream_handler  <code>module-attribute</code>","text":"<pre><code>stream_handler = StreamHandler(stdout)\n</code></pre>"},{"location":"API/#src.api.calculate_cosine_similarities","title":"calculate_cosine_similarities","text":"<pre><code>calculate_cosine_similarities(model: SentenceTransformer | CustomT5EncoderModel, model_name: str, new_embedding: ndarray, col: str, dataset_type: str) -&gt; ndarray\n</code></pre> <p>Calculates cosine similarities between a new embedding and existing embeddings.</p> <p>This function:</p> <ol> <li> <p>Loads pre-computed embeddings for the specified column</p> </li> <li> <p>Verifies embedding dimensions match</p> </li> <li> <p>Computes cosine similarity scores using GPU if available</p> </li> </ol> PARAMETER DESCRIPTION <code>model</code> <p>The transformer model used for encoding</p> <p> TYPE: <code>SentenceTransformer | CustomT5EncoderModel</code> </p> <code>model_name</code> <p>Name of the model</p> <p> TYPE: <code>str</code> </p> <code>new_embedding</code> <p>Embedding vector of the input description</p> <p> TYPE: <code>ndarray</code> </p> <code>col</code> <p>Name of the synopsis column</p> <p> TYPE: <code>str</code> </p> <code>dataset_type</code> <p>Type of dataset ('anime' or 'manga')</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>Array of cosine similarity scores between the new embedding and all existing embeddings</p> RAISES DESCRIPTION <code>ValueError</code> <p>If embedding dimensions don't match</p> Source code in <code>src/api.py</code> <pre><code>def calculate_cosine_similarities(\n    model: SentenceTransformer | CustomT5EncoderModel,\n    model_name: str,\n    new_embedding: np.ndarray,\n    col: str,\n    dataset_type: str,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculates cosine similarities between a new embedding and existing embeddings.\n\n    This function:\n\n    1. Loads pre-computed embeddings for the specified column\n\n    2. Verifies embedding dimensions match\n\n    3. Computes cosine similarity scores using GPU if available\n\n    Args:\n        model: The transformer model used for encoding\n        model_name: Name of the model\n        new_embedding: Embedding vector of the input description\n        col: Name of the synopsis column\n        dataset_type: Type of dataset ('anime' or 'manga')\n\n    Returns:\n        Array of cosine similarity scores between the new embedding and all existing embeddings\n\n    Raises:\n        ValueError: If embedding dimensions don't match\n    \"\"\"\n    model_name = model_name.replace(\"sentence-transformers/\", \"\")\n    model_name = model_name.replace(\"toobi/\", \"\")\n    existing_embeddings = load_embeddings(model_name, col, dataset_type)\n    if existing_embeddings.shape[1] != model.get_sentence_embedding_dimension():\n        raise ValueError(f\"Incompatible dimension for embeddings in {col}\")\n    new_embedding_tensor = torch.tensor(new_embedding).to(device)\n    existing_embeddings_tensor = torch.tensor(existing_embeddings).to(device)\n    return (\n        util.pytorch_cos_sim(new_embedding_tensor, existing_embeddings_tensor)\n        .flatten()\n        .cpu()\n        .numpy()\n    )\n</code></pre>"},{"location":"API/#src.api.clear_memory","title":"clear_memory","text":"<pre><code>clear_memory() -&gt; None\n</code></pre> <p>Frees up system memory and GPU cache.</p> <p>This function performs two cleanup operations:</p> <ol> <li> <p>Empties the GPU cache if CUDA is being used</p> </li> <li> <p>Runs Python's garbage collector to free memory</p> </li> </ol> Source code in <code>src/api.py</code> <pre><code>def clear_memory() -&gt; None:\n    \"\"\"\n    Frees up system memory and GPU cache.\n\n    This function performs two cleanup operations:\n\n    1. Empties the GPU cache if CUDA is being used\n\n    2. Runs Python's garbage collector to free memory\n    \"\"\"\n    torch.cuda.empty_cache()\n    gc.collect()\n</code></pre>"},{"location":"API/#src.api.find_top_similarities","title":"find_top_similarities","text":"<pre><code>find_top_similarities(cosine_similarities_dict: Dict[str, ndarray], num_similarities: int = 10) -&gt; List[Tuple[int, str]]\n</code></pre> <p>Finds the top N most similar descriptions across all synopsis columns.</p> <p>This function:</p> <ol> <li> <p>Processes similarity scores from all columns</p> </li> <li> <p>Sorts them in descending order</p> </li> <li> <p>Returns indices and column names for the top matches</p> </li> </ol> PARAMETER DESCRIPTION <code>cosine_similarities_dict</code> <p>Dictionary mapping column names to arrays of similarity scores</p> <p> TYPE: <code>Dict[str, ndarray]</code> </p> <code>num_similarities</code> <p>Number of top similarities to return (default: 10)</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> RETURNS DESCRIPTION <code>List[Tuple[int, str]]</code> <p>List of tuples containing (index, column_name) for the top similar descriptions,</p> <code>List[Tuple[int, str]]</code> <p>sorted by similarity score in descending order</p> Source code in <code>src/api.py</code> <pre><code>def find_top_similarities(\n    cosine_similarities_dict: Dict[str, np.ndarray], num_similarities: int = 10\n) -&gt; List[Tuple[int, str]]:\n    \"\"\"\n    Finds the top N most similar descriptions across all synopsis columns.\n\n    This function:\n\n    1. Processes similarity scores from all columns\n\n    2. Sorts them in descending order\n\n    3. Returns indices and column names for the top matches\n\n    Args:\n        cosine_similarities_dict: Dictionary mapping column names to arrays of similarity scores\n        num_similarities: Number of top similarities to return (default: 10)\n\n    Returns:\n        List of tuples containing (index, column_name) for the top similar descriptions,\n        sorted by similarity score in descending order\n    \"\"\"\n    all_top_indices = []\n    for col, cosine_similarities in cosine_similarities_dict.items():\n        top_indices_unsorted = np.argsort(cosine_similarities)[-num_similarities:]\n        top_indices = top_indices_unsorted[\n            np.argsort(cosine_similarities[top_indices_unsorted])[::-1]\n        ]\n        all_top_indices.extend([(idx, col) for idx in top_indices])\n    all_top_indices.sort(\n        key=lambda x: cosine_similarities_dict[x[1]][x[0]],  # type: ignore\n        reverse=True,\n    )  # type: ignore\n    return all_top_indices\n</code></pre>"},{"location":"API/#src.api.get_anime_similarities","title":"get_anime_similarities","text":"<pre><code>get_anime_similarities() -&gt; Response\n</code></pre> <p>API endpoint for finding similar anime based on a description.</p> <p>This endpoint:</p> <ol> <li> <p>Validates the request payload</p> </li> <li> <p>Processes the description using the specified model</p> </li> <li> <p>Returns paginated results of similar anime</p> </li> </ol> <p>Expected JSON payload: <pre><code>{\n    \"model\": str,          # Name of the model to use\n    \"description\": str,    # Input description to find similarities for\n    \"page\": int,           # Optional: Page number (default: 1)\n    \"resultsPerPage\": int  # Optional: Results per page (default: 10)\n}\n</code></pre></p> RETURNS DESCRIPTION <code>Response</code> <p>JSON response containing:</p> <code>Response</code> <ul> <li>List of similar anime with metadata</li> </ul> <code>Response</code> <ul> <li>Similarity scores</li> </ul> <code>Response</code> <ul> <li>Pagination information</li> </ul> RAISES DESCRIPTION <code>400</code> <p>If request validation fails</p> <code>500</code> <p>If internal processing error occurs</p> Source code in <code>src/api.py</code> <pre><code>@app.route(\"/anisearchmodel/anime\", methods=[\"POST\"])\n@limiter.limit(\"1 per second\")\ndef get_anime_similarities() -&gt; Response:\n    \"\"\"\n    API endpoint for finding similar anime based on a description.\n\n    This endpoint:\n\n    1. Validates the request payload\n\n    2. Processes the description using the specified model\n\n    3. Returns paginated results of similar anime\n\n    Expected JSON payload:\n    ```\n    {\n        \"model\": str,          # Name of the model to use\n        \"description\": str,    # Input description to find similarities for\n        \"page\": int,           # Optional: Page number (default: 1)\n        \"resultsPerPage\": int  # Optional: Results per page (default: 10)\n    }\n    ```\n\n    Returns:\n        JSON response containing:\n        - List of similar anime with metadata\n        - Similarity scores\n        - Pagination information\n\n    Raises:\n        400: If request validation fails\n        500: If internal processing error occurs\n    \"\"\"\n    try:\n        clear_memory()\n        data = request.json\n        if data is None:\n            raise ValueError(\"Request payload is missing or not in JSON format\")\n        validate_input(data)\n        model_name = data.get(\"model\")\n        if model_name == \"sentence-transformers/fine_tuned_sbert_anime_model\":\n            model_name = \"fine_tuned_sbert_model_anime\"\n        description = data.get(\"description\")\n        page = data.get(\"page\", 1)\n        results_per_page = data.get(\"resultsPerPage\", 10)\n\n        # Get the client's IP address\n        client_ip = request.headers.get(\"X-Forwarded-For\", request.remote_addr)\n        logging.info(\n            \"Received anime request from IP: %s with model: %s, \"\n            \"description: %s, page: %d, resultsPerPage: %d\",\n            client_ip,\n            model_name,\n            description,\n            page,\n            results_per_page,\n        )\n\n        results = get_similarities(\n            model_name, description, \"anime\", page, results_per_page\n        )\n        logging.info(\"Returning %d anime results\", len(results))\n        clear_memory()\n        return jsonify(results)\n\n    except ValueError as e:\n        logging.error(\"Validation error: %s\", e)\n        return make_response(jsonify({\"error\": \"Bad Request\"}), 400)\n    except Exception as e:  # pylint: disable=broad-exception-caught\n        logging.error(\"Internal server error: %s\", e)\n        return make_response(jsonify({\"error\": \"Internal server error\"}), 500)\n</code></pre>"},{"location":"API/#src.api.get_manga_similarities","title":"get_manga_similarities","text":"<pre><code>get_manga_similarities() -&gt; Response\n</code></pre> <p>API endpoint for finding similar manga based on a description.</p> <p>This endpoint:</p> <ol> <li> <p>Validates the request payload</p> </li> <li> <p>Processes the description using the specified model</p> </li> <li> <p>Returns paginated results of similar manga</p> </li> </ol> <p>Expected JSON payload: <pre><code>{\n    \"model\": str,          # Name of the model to use\n    \"description\": str,    # Input description to find similarities for\n    \"page\": int,           # Optional: Page number (default: 1)\n    \"resultsPerPage\": int  # Optional: Results per page (default: 10)\n}\n</code></pre></p> RETURNS DESCRIPTION <code>Response</code> <p>JSON response containing:</p> <code>Response</code> <ul> <li>List of similar manga with metadata</li> </ul> <code>Response</code> <ul> <li>Similarity scores</li> </ul> <code>Response</code> <ul> <li>Pagination information</li> </ul> RAISES DESCRIPTION <code>400</code> <p>If request validation fails</p> <code>500</code> <p>If internal processing error occurs</p> Source code in <code>src/api.py</code> <pre><code>@app.route(\"/anisearchmodel/manga\", methods=[\"POST\"])  # type: ignore\n@limiter.limit(\"1 per second\")\ndef get_manga_similarities() -&gt; Response:\n    \"\"\"\n    API endpoint for finding similar manga based on a description.\n\n    This endpoint:\n\n    1. Validates the request payload\n\n    2. Processes the description using the specified model\n\n    3. Returns paginated results of similar manga\n\n    Expected JSON payload:\n    ```\n    {\n        \"model\": str,          # Name of the model to use\n        \"description\": str,    # Input description to find similarities for\n        \"page\": int,           # Optional: Page number (default: 1)\n        \"resultsPerPage\": int  # Optional: Results per page (default: 10)\n    }\n    ```\n\n    Returns:\n        JSON response containing:\n        - List of similar manga with metadata\n        - Similarity scores\n        - Pagination information\n\n    Raises:\n        400: If request validation fails\n        500: If internal processing error occurs\n    \"\"\"\n    try:\n        clear_memory()\n        data = request.json\n        if data is None:\n            raise ValueError(\"Request payload is missing or not in JSON format\")\n        validate_input(data)\n        model_name = data.get(\"model\")\n        if model_name == \"sentence-transformers/fine_tuned_sbert_anime_model\":\n            model_name = \"fine_tuned_sbert_model_anime\"\n        description = data.get(\"description\")\n        page = data.get(\"page\", 1)\n        results_per_page = data.get(\"resultsPerPage\", 10)\n\n        # Get the client's IP address\n        client_ip = request.headers.get(\"X-Forwarded-For\", request.remote_addr)\n\n        logging.info(\n            \"Manga request - IP: %s, model: %s, desc: %s, page: %d, results/page: %d\",\n            client_ip,\n            model_name,\n            description,\n            page,\n            results_per_page,\n        )\n\n        results = get_similarities(\n            model_name, description, \"manga\", page, results_per_page\n        )\n        logging.info(\"Returning %d manga results\", len(results))\n        clear_memory()\n        return jsonify(results)\n\n    except HTTPException as e:\n        logging.error(\"HTTP error: %s\", e)\n        return make_response(jsonify({\"error\": e.description}), e.code)\n    except Exception as e:  # pylint: disable=broad-exception-caught\n        logging.error(\"Internal server error: %s\", e)\n        return make_response(jsonify({\"error\": \"Internal server error\"}), 500)\n</code></pre>"},{"location":"API/#src.api.get_similarities","title":"get_similarities","text":"<pre><code>get_similarities(model_name: str, description: str, dataset_type: str, page: int = 1, results_per_page: int = 10) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Finds the most similar descriptions in the specified dataset.</p> <p>This function:</p> <ol> <li> <p>Loads and validates the appropriate model</p> </li> <li> <p>Encodes the input description</p> </li> <li> <p>Calculates similarities with all stored descriptions</p> </li> <li> <p>Returns paginated results with metadata</p> </li> </ol> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the model to use</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Input description to find similarities for</p> <p> TYPE: <code>str</code> </p> <code>dataset_type</code> <p>Type of dataset ('anime' or 'manga')</p> <p> TYPE: <code>str</code> </p> <code>page</code> <p>Page number for pagination (default: 1)</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>results_per_page</code> <p>Number of results per page (default: 10)</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> RETURNS DESCRIPTION <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing similar items with metadata and similarity scores</p> RAISES DESCRIPTION <code>ValueError</code> <p>If model name is invalid or model loading fails</p> Source code in <code>src/api.py</code> <pre><code>def get_similarities(\n    model_name: str,\n    description: str,\n    dataset_type: str,\n    page: int = 1,\n    results_per_page: int = 10,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Finds the most similar descriptions in the specified dataset.\n\n    This function:\n\n    1. Loads and validates the appropriate model\n\n    2. Encodes the input description\n\n    3. Calculates similarities with all stored descriptions\n\n    4. Returns paginated results with metadata\n\n    Args:\n        model_name: Name of the model to use\n        description: Input description to find similarities for\n        dataset_type: Type of dataset ('anime' or 'manga')\n        page: Page number for pagination (default: 1)\n        results_per_page: Number of results per page (default: 10)\n\n    Returns:\n        List of dictionaries containing similar items with metadata and similarity scores\n\n    Raises:\n        ValueError: If model name is invalid or model loading fails\n    \"\"\"\n    update_last_request_time()\n\n    # Validate model name\n    if model_name not in allowed_models:\n        raise ValueError(\"Invalid model name\")\n\n    # Select the appropriate dataset and synopsis columns\n    if dataset_type == \"anime\":\n        df = anime_df\n        synopsis_columns = anime_synopsis_columns\n    else:\n        df = manga_df\n        synopsis_columns = manga_synopsis_columns\n\n    if (\n        model_name == \"fine_tuned_sbert_anime_model\"\n        or model_name == \"fine_tuned_sbert_model_anime\"\n    ):\n        load_model_name = script_dir.parent / f\"model/{model_name}\"\n    else:\n        load_model_name = model_name\n\n    # Load the complete SentenceTransformer model\n    try:\n        model = SentenceTransformer(load_model_name, device=device)\n    except Exception as e:\n        raise ValueError(f\"Failed to load model '{load_model_name}': {e}\") from e\n\n    processed_description = description.strip()\n    new_pooled_embedding = model.encode([processed_description])\n\n    cosine_similarities_dict = {\n        col: calculate_cosine_similarities(\n            model, model_name, new_pooled_embedding, col, dataset_type\n        )\n        for col in synopsis_columns\n    }\n\n    all_top_indices = find_top_similarities(\n        cosine_similarities_dict, num_similarities=page * results_per_page\n    )\n\n    seen_names = set()\n    results: List[Dict[str, Any]] = []\n\n    for idx, col in all_top_indices:\n        name = df.iloc[idx][\"title\"]\n        relevant_synopsis = df.iloc[idx][col]\n\n        # Check if the relevant synopsis is valid\n        if pd.isna(relevant_synopsis) or relevant_synopsis.strip() == \"\":\n            continue\n\n        if name not in seen_names:\n            row_data = df.iloc[idx].to_dict()  # Convert the entire row to a dictionary\n            # Keep only the relevant synopsis column\n            row_data = {\n                k: v\n                for k, v in row_data.items()\n                if k not in synopsis_columns or k == col\n            }\n            row_data.update(\n                {\n                    \"rank\": len(results) + 1,\n                    \"similarity\": float(cosine_similarities_dict[col][idx]),\n                    \"synopsis\": relevant_synopsis,  # Ensure the correct synopsis is included\n                }\n            )\n            results.append(row_data)\n            seen_names.add(name)\n            if len(results) &gt;= page * results_per_page:\n                break\n\n    # Clear memory\n    del model, new_pooled_embedding, cosine_similarities_dict\n    clear_memory()\n\n    # Calculate start and end indices for pagination\n    start_index = (page - 1) * results_per_page\n    end_index = start_index + results_per_page\n\n    return results[start_index:end_index]\n</code></pre>"},{"location":"API/#src.api.load_embeddings","title":"load_embeddings","text":"<pre><code>load_embeddings(model_name: str, col: str, dataset_type: str) -&gt; ndarray\n</code></pre> <p>Loads pre-computed embeddings for a specific model and dataset column.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the model used to generate the embeddings</p> <p> TYPE: <code>str</code> </p> <code>col</code> <p>Name of the synopsis column</p> <p> TYPE: <code>str</code> </p> <code>dataset_type</code> <p>Type of dataset ('anime' or 'manga')</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>NumPy array containing the pre-computed embeddings</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the embeddings file doesn't exist</p> Source code in <code>src/api.py</code> <pre><code>def load_embeddings(model_name: str, col: str, dataset_type: str) -&gt; np.ndarray:\n    \"\"\"\n    Loads pre-computed embeddings for a specific model and dataset column.\n\n    Args:\n        model_name: Name of the model used to generate the embeddings\n        col: Name of the synopsis column\n        dataset_type: Type of dataset ('anime' or 'manga')\n\n    Returns:\n        NumPy array containing the pre-computed embeddings\n\n    Raises:\n        FileNotFoundError: If the embeddings file doesn't exist\n    \"\"\"\n    embeddings_file = (\n        script_dir.parent\n        / f\"model/{dataset_type}/{model_name}/embeddings_{col.replace(' ', '_')}.npy\"\n    )\n    return np.load(embeddings_file)\n</code></pre>"},{"location":"API/#src.api.periodic_memory_clear","title":"periodic_memory_clear","text":"<pre><code>periodic_memory_clear() -&gt; None\n</code></pre> <p>Runs a background thread that periodically cleans up memory.</p> <p>The thread monitors the time since the last API request. If no requests have been made for over 300 seconds (5 minutes), it triggers memory cleanup to free resources.</p> <p>The function runs indefinitely until the application is shut down.</p> Source code in <code>src/api.py</code> <pre><code>def periodic_memory_clear() -&gt; None:\n    \"\"\"\n    Runs a background thread that periodically cleans up memory.\n\n    The thread monitors the time since the last API request. If no requests have been\n    made for over 300 seconds (5 minutes), it triggers memory cleanup to free resources.\n\n    The function runs indefinitely until the application is shut down.\n    \"\"\"\n    logging.info(\"Starting the periodic memory clear thread.\")\n    while True:\n        with last_request_time_lock:\n            current_time = time.time()\n            if current_time - last_request_time &gt; 300:\n                logging.debug(\"Clearing memory due to inactivity.\")\n                clear_memory()\n        time.sleep(300)\n</code></pre>"},{"location":"API/#src.api.update_last_request_time","title":"update_last_request_time","text":"<pre><code>update_last_request_time() -&gt; None\n</code></pre> <p>Updates the last request time to the current time in a thread-safe manner.</p> <p>This function is used to track when the last API request was made, which helps with memory management and cleanup of unused resources.</p> Source code in <code>src/api.py</code> <pre><code>def update_last_request_time() -&gt; None:\n    \"\"\"\n    Updates the last request time to the current time in a thread-safe manner.\n\n    This function is used to track when the last API request was made, which helps\n    with memory management and cleanup of unused resources.\n    \"\"\"\n    with last_request_time_lock:\n        global last_request_time\n        last_request_time = time.time()\n</code></pre>"},{"location":"API/#src.api.validate_input","title":"validate_input","text":"<pre><code>validate_input(data: Dict[str, Any]) -&gt; None\n</code></pre> <p>Validates the input data for API requests.</p> <p>This function checks that:</p> <ol> <li> <p>Both model name and description are provided</p> </li> <li> <p>The description length is within acceptable limits</p> </li> <li> <p>The specified model is in the list of allowed models</p> </li> </ol> PARAMETER DESCRIPTION <code>data</code> <p>Dictionary containing the request data with 'model' and 'description' keys</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RAISES DESCRIPTION <code>HTTPException</code> <p>If any validation check fails, with appropriate error message and status code</p> Source code in <code>src/api.py</code> <pre><code>def validate_input(data: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Validates the input data for API requests.\n\n    This function checks that:\n\n    1. Both model name and description are provided\n\n    2. The description length is within acceptable limits\n\n    3. The specified model is in the list of allowed models\n\n    Args:\n        data: Dictionary containing the request data with 'model' and 'description' keys\n\n    Raises:\n        HTTPException: If any validation check fails, with appropriate error message and status code\n    \"\"\"\n    model_name = data.get(\"model\")\n    description = data.get(\"description\")\n\n    if not model_name or not description:\n        logging.error(\"Model name or description missing in the request.\")\n        abort(400, description=\"Model name and description are required\")\n\n    if len(description) &gt; 2000:\n        logging.error(\"Description too long.\")\n        abort(400, description=\"Description is too long\")\n\n    if model_name not in allowed_models:\n        logging.error(\"Invalid model name.\")\n        abort(400, description=\"Invalid model name\")\n</code></pre>"},{"location":"Common/","title":"Common","text":"<p>This module provides utility functions for loading datasets, preprocessing text, and saving evaluation data for machine learning models.</p> FUNCTION DESCRIPTION <code>load_dataset</code> <p>Load and preprocess a dataset from a CSV file.</p> <code>preprocess_text</code> <p>Clean and normalize text data for ML processing.</p> <code>save_evaluation_data</code> <p>Save model evaluation results to JSON.</p>"},{"location":"Common/#src.common.lemmatizer","title":"lemmatizer  <code>module-attribute</code>","text":"<pre><code>lemmatizer = WordNetLemmatizer()\n</code></pre>"},{"location":"Common/#src.common.stop_words","title":"stop_words  <code>module-attribute</code>","text":"<pre><code>stop_words = set(words('english'))\n</code></pre>"},{"location":"Common/#src.common.load_dataset","title":"load_dataset","text":"<pre><code>load_dataset(file_path: str) -&gt; DataFrame\n</code></pre> <p>Load dataset from a CSV file and fill missing values in the 'Synopsis' column.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the CSV file containing the dataset.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: Loaded dataset with filled 'Synopsis' column.</p> Source code in <code>src/common.py</code> <pre><code>def load_dataset(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load dataset from a CSV file and fill missing values in the 'Synopsis' column.\n\n    Args:\n        file_path (str): Path to the CSV file containing the dataset.\n\n    Returns:\n        pd.DataFrame: Loaded dataset with filled 'Synopsis' column.\n    \"\"\"\n    df = pd.read_csv(file_path)\n    df[\"synopsis\"] = df[\"synopsis\"].fillna(\"\")\n    return df\n</code></pre>"},{"location":"Common/#src.common.preprocess_text","title":"preprocess_text","text":"<pre><code>preprocess_text(text: Any) -&gt; Any\n</code></pre> <p>Preprocess text data by applying various cleaning and normalization steps.</p> Steps include <ul> <li>Converting to lowercase</li> <li>Expanding contractions</li> <li>Removing accents</li> <li>Removing extra whitespace</li> <li>Removing URLs</li> <li>Removing source citations</li> <li>Removing stopwords</li> <li>Lemmatizing words</li> </ul> PARAMETER DESCRIPTION <code>text</code> <p>Input text to preprocess. Can be string or other type.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Preprocessed text if input was string, otherwise returns input unchanged.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/common.py</code> <pre><code>def preprocess_text(text: Any) -&gt; Any:\n    \"\"\"\n    Preprocess text data by applying various cleaning and normalization steps.\n\n    Steps include:\n        - Converting to lowercase\n        - Expanding contractions\n        - Removing accents\n        - Removing extra whitespace\n        - Removing URLs\n        - Removing source citations\n        - Removing stopwords\n        - Lemmatizing words\n\n    Args:\n        text (Any): Input text to preprocess. Can be string or other type.\n\n    Returns:\n        Any: Preprocessed text if input was string, otherwise returns input unchanged.\n    \"\"\"\n    if text is None:\n        return \"\"\n\n    try:\n        if isinstance(text, str):\n            text = text.strip()  # Strip whitespace\n            text = contractions.fix(text)  # Expand contractions\n            text = unidecode(text)  # Remove accents\n            text = re.sub(\n                r\"\\s+\", \" \", text\n            )  # Replace multiple spaces with a single space\n            # Remove wrapping quotes\n            if (text.startswith('\"') and text.endswith('\"')) or (\n                text.startswith(\"'\") and text.endswith(\"'\")\n            ):\n                text = text[1:-1]\n            text = re.sub(\n                r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE\n            )  # Remove URLs\n            # Remove specific patterns\n            text = re.sub(r\"\\[Written by .*?\\].*$\", \"\", text, flags=re.IGNORECASE)\n            text = re.sub(\n                r\"&lt;br&gt;&lt;br&gt;\\s*\\(source:.*?\\).*$\", \"\", text, flags=re.IGNORECASE\n            )\n            text = re.sub(r\"\\(source:.*?\\).*$\", \"\", text, flags=re.IGNORECASE)\n            # Tokenize and remove stopwords\n            words = text.split()\n            words = [word for word in words if word not in stop_words]\n            # Apply lemmatization\n            words = [lemmatizer.lemmatize(word) for word in words]\n            text = \" \".join(words)\n        else:\n            return text\n    except Exception:  # pylint: disable=broad-except\n        return text\n\n    return text\n</code></pre>"},{"location":"Common/#src.common.save_evaluation_data","title":"save_evaluation_data","text":"<pre><code>save_evaluation_data(model_name: str, batch_size: int, num_embeddings: int, additional_info: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Save model evaluation data to a JSON file with timestamp and parameters.</p> <p>Creates or appends to 'model/evaluation_results.json', storing evaluation metrics and model configuration details.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name/identifier of the model being evaluated.</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Batch size used for generating embeddings.</p> <p> TYPE: <code>int</code> </p> <code>num_embeddings</code> <p>Total number of embeddings generated.</p> <p> TYPE: <code>int</code> </p> <code>additional_info</code> <p>Additional evaluation metrics or parameters.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/common.py</code> <pre><code>def save_evaluation_data(\n    model_name: str,\n    batch_size: int,\n    num_embeddings: int,\n    additional_info: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Save model evaluation data to a JSON file with timestamp and parameters.\n\n    Creates or appends to 'model/evaluation_results.json', storing evaluation metrics\n    and model configuration details.\n\n    Args:\n        model_name (str): Name/identifier of the model being evaluated.\n        batch_size (int): Batch size used for generating embeddings.\n        num_embeddings (int): Total number of embeddings generated.\n        additional_info (Optional[Dict[str, Any]]): Additional evaluation metrics or parameters.\n    \"\"\"\n    evaluation_data = {\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"model_parameters\": {\n            \"model_name\": model_name,\n            \"batch_size\": batch_size,\n            \"num_embeddings\": num_embeddings,\n        },\n    }\n\n    if additional_info:\n        evaluation_data.update(additional_info)\n\n    # Path to the JSON file\n    file_path = \"model/evaluation_results.json\"\n\n    # Check if the file exists and is not empty\n    if os.path.exists(file_path) and os.path.getsize(file_path) &gt; 0:\n        # Read the existing data\n        with open(file_path, \"r+\", encoding=\"utf-8\") as f:\n            f.seek(0, os.SEEK_END)\n            f.seek(f.tell() - 1, os.SEEK_SET)\n            f.truncate()\n            f.write(\",\\n\")\n            json.dump(evaluation_data, f, indent=4)\n            f.write(\"\\n]\")\n    else:\n        # Create a new file with an array\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump([evaluation_data], f, indent=4)\n</code></pre>"},{"location":"CustomTransformer/","title":"CustomTransformer","text":"<p>This module defines a custom T5 Encoder model that replaces ReLU activation functions with GELU.</p> <p>The CustomT5EncoderModel class extends the Transformer model from the sentence_transformers library and modifies the activation functions in the feed-forward networks of the transformer blocks to use GELU instead of ReLU. This modification can help improve model performance since GELU has been shown to work well in transformer architectures.</p>"},{"location":"CustomTransformer/#src.custom_transformer.CustomT5EncoderModel","title":"CustomT5EncoderModel","text":"<pre><code>CustomT5EncoderModel(model_name_or_path: str, model_args: Optional[Dict] = None, max_seq_length: int = 256, do_lower_case: bool = False, dropout_rate: float = 0.2)\n</code></pre> <p>               Bases: <code>Transformer</code></p> <p>Custom T5 Encoder model that replaces ReLU activation functions with GELU.</p> <p>This class extends the Transformer model from the sentence_transformers library and modifies the activation functions in the feed-forward networks of the transformer blocks to use GELU instead of ReLU. GELU (Gaussian Error Linear Unit) is a smoother activation function that often performs better than ReLU in transformer architectures.</p> ATTRIBUTE DESCRIPTION <code>model_name_or_path</code> <p>Name or path of the pre-trained T5 model to load.</p> <p> TYPE: <code>str</code> </p> <code>model_args</code> <p>Additional arguments to pass to the T5 model constructor.</p> <p> TYPE: <code>Optional[Dict]</code> </p> <code>max_seq_length</code> <p>Maximum sequence length for input text. Longer sequences will be truncated.</p> <p> TYPE: <code>int</code> </p> <code>do_lower_case</code> <p>Whether to convert input text to lowercase before tokenization.</p> <p> TYPE: <code>bool</code> </p> <code>dropout_rate</code> <p>Dropout rate to apply to the feed-forward networks.</p> <p> TYPE: <code>float</code> </p> PARAMETER DESCRIPTION <code>model_name_or_path</code> <p>Name or path of the pre-trained T5 model to load.</p> <p> TYPE: <code>str</code> </p> <code>model_args</code> <p>Additional arguments to pass to the T5 model constructor. Defaults to an empty dict if None.</p> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>None</code> </p> <code>max_seq_length</code> <p>Maximum sequence length for input text. Default is 256.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> <code>do_lower_case</code> <p>Whether to convert input text to lowercase. Default is False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dropout_rate</code> <p>Dropout rate to apply to the feed-forward networks. Default is 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> Source code in <code>src/custom_transformer.py</code> <pre><code>def __init__(\n    self,\n    model_name_or_path: str,\n    model_args: Optional[Dict] = None,\n    max_seq_length: int = 256,\n    do_lower_case: bool = False,\n    dropout_rate: float = 0.2,\n):\n    \"\"\"\n    Initialize the CustomT5EncoderModel.\n\n    Args:\n        model_name_or_path (str): Name or path of the pre-trained T5 model to load.\n        model_args (Optional[Dict]): Additional arguments to pass to the T5 model constructor.\n            Defaults to an empty dict if None.\n        max_seq_length (int): Maximum sequence length for input text. Default is 256.\n        do_lower_case (bool): Whether to convert input text to lowercase. Default is False.\n        dropout_rate (float): Dropout rate to apply to the feed-forward networks. Default is 0.2.\n    \"\"\"\n    super().__init__(\n        model_name_or_path=model_name_or_path,\n        model_args=model_args if model_args is not None else {},\n        max_seq_length=max_seq_length,\n        do_lower_case=do_lower_case,\n    )\n    if not model_name_or_path.startswith(\"toobi/anime\"):\n        self.modify_activation(self.auto_model, dropout_rate)\n</code></pre>"},{"location":"CustomTransformer/#src.custom_transformer.CustomT5EncoderModel.modify_activation","title":"modify_activation","text":"<pre><code>modify_activation(model, dropout_rate)\n</code></pre> <p>Replace ReLU activation with GELU in all transformer blocks of the T5 encoder.</p> <p>This method iterates through all transformer blocks in the encoder and replaces the ReLU activation in each feed-forward network with GELU activation.</p> PARAMETER DESCRIPTION <code>model</code> <p>The underlying T5 transformer model whose activations will be modified.</p> <p> </p> <code>dropout_rate</code> <p>Dropout rate to apply to the feed-forward networks.</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/custom_transformer.py</code> <pre><code>def modify_activation(self, model, dropout_rate):\n    \"\"\"\n    Replace ReLU activation with GELU in all transformer blocks of the T5 encoder.\n\n    This method iterates through all transformer blocks in the encoder and replaces\n    the ReLU activation in each feed-forward network with GELU activation.\n\n    Args:\n        model: The underlying T5 transformer model whose activations will be modified.\n        dropout_rate (float): Dropout rate to apply to the feed-forward networks.\n    \"\"\"\n    if hasattr(model, \"encoder\") and hasattr(model.encoder, \"block\"):\n        model.encoder.dropout = nn.Dropout(p=dropout_rate, inplace=False)\n        for _, block in enumerate(model.encoder.block):\n            # Accessing the feed-forward network within each block\n            ff = block.layer[1].DenseReluDense\n            # Replace ReLU with GELU\n            ff.act = nn.GELU()\n            # Set dropout rates using float values instead of nn.Dropout instances\n            ff.dropout = nn.Dropout(p=dropout_rate, inplace=False)\n            block.layer[0].dropout = nn.Dropout(p=dropout_rate, inplace=False)\n            block.layer[1].dropout = nn.Dropout(p=dropout_rate, inplace=False)\n</code></pre>"},{"location":"MergeDatasets/","title":"MergeDatasets","text":"<p>Merges multiple anime or manga datasets into a single dataset.</p> <p>This script provides functionality to merge and preprocess multiple anime or manga datasets from various sources. It handles data cleaning, deduplication, and consolidation of information across datasets.</p> Key features <ul> <li>Loads datasets from CSV files and Hugging Face datasets library</li> <li>Cleans and preprocesses text fields like titles and synopses</li> <li>Merges datasets based on common identifiers while handling duplicates</li> <li>Consolidates information from multiple sources while preserving data quality</li> <li>Removes inappropriate content based on genres/demographics</li> <li>Saves the final merged dataset with progress tracking</li> </ul> <p>The script can be run from the command line with a required --type argument specifying either 'anime' or 'manga'.</p> <p>Example: <pre><code>python merge_datasets.py --type anime\n</code></pre></p> <p>The merged dataset will be saved to model/merged_[type]_dataset.csv</p>"},{"location":"MergeDatasets/#src.merge_datasets.CONSOLE_LOGGING_LEVEL","title":"CONSOLE_LOGGING_LEVEL  <code>module-attribute</code>","text":"<pre><code>CONSOLE_LOGGING_LEVEL = INFO\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.FILE_LOGGING_LEVEL","title":"FILE_LOGGING_LEVEL  <code>module-attribute</code>","text":"<pre><code>FILE_LOGGING_LEVEL = DEBUG\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.file_formatter","title":"file_formatter  <code>module-attribute</code>","text":"<pre><code>file_formatter = Formatter('%(asctime)s - %(levelname)s - %(message)s')\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.file_handler","title":"file_handler  <code>module-attribute</code>","text":"<pre><code>file_handler = RotatingFileHandler('./logs/merge_datasets.log', maxBytes=10 * 1024 * 1024, backupCount=10, encoding='utf-8')\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.stream_formatter","title":"stream_formatter  <code>module-attribute</code>","text":"<pre><code>stream_formatter = Formatter('%(asctime)s - %(levelname)s - %(message)s')\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.stream_handler","title":"stream_handler  <code>module-attribute</code>","text":"<pre><code>stream_handler = StreamHandler(stdout)\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.add_additional_info","title":"add_additional_info","text":"<pre><code>add_additional_info(merged: DataFrame, additional_df: DataFrame, description_col: str, name_columns: list[str], new_synopsis_col: str) -&gt; DataFrame\n</code></pre> <p>Add additional synopsis information from supplementary dataset.</p> PARAMETER DESCRIPTION <code>merged</code> <p>Main DataFrame to update with additional info</p> <p> TYPE: <code>DataFrame</code> </p> <code>additional_df</code> <p>DataFrame containing additional descriptions</p> <p> TYPE: <code>DataFrame</code> </p> <code>description_col</code> <p>Name of column containing descriptions</p> <p> TYPE: <code>str</code> </p> <code>name_columns</code> <p>List of columns to use for matching titles</p> <p> TYPE: <code>list[str]</code> </p> <code>new_synopsis_col</code> <p>Name for new column to store additional synopses</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: Updated DataFrame with additional synopsis information</p> <p>Matches entries between datasets and adds non-duplicate synopsis information. Uses tqdm for progress tracking during updates.</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def add_additional_info(\n    merged: pd.DataFrame,\n    additional_df: pd.DataFrame,\n    description_col: str,\n    name_columns: list[str],\n    new_synopsis_col: str,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add additional synopsis information from supplementary dataset.\n\n    Args:\n        merged: Main DataFrame to update with additional info\n        additional_df: DataFrame containing additional descriptions\n        description_col: Name of column containing descriptions\n        name_columns: List of columns to use for matching titles\n        new_synopsis_col: Name for new column to store additional synopses\n\n    Returns:\n        pd.DataFrame: Updated DataFrame with additional synopsis information\n\n    Matches entries between datasets and adds non-duplicate synopsis information.\n    Uses tqdm for progress tracking during updates.\n    \"\"\"\n    logging.info(\"Adding additional info to column: %s\", new_synopsis_col)\n    if new_synopsis_col not in merged.columns:\n        merged[new_synopsis_col] = pd.NA\n        logging.info(\"Initialized new synopsis column: %s\", new_synopsis_col)\n\n    for idx, row in tqdm(\n        merged.iterrows(),\n        total=merged.shape[0],\n        desc=f\"Adding additional info from '{new_synopsis_col}'\",\n    ):\n        if pd.isna(row[new_synopsis_col]):\n            info = find_additional_info(\n                row, additional_df, description_col, name_columns\n            )\n            if info:\n                merged.at[idx, new_synopsis_col] = info\n                logging.debug(\n                    \"Added info to row %d in column '%s'.\", idx, new_synopsis_col\n                )\n\n    added_count = merged[new_synopsis_col].notna().sum()\n    logging.info(\n        \"Added %d entries to column '%s'.\",\n        added_count,\n        new_synopsis_col,\n    )\n    return merged\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.clean_synopsis","title":"clean_synopsis","text":"<pre><code>clean_synopsis(df: DataFrame, synopsis_col: str, unwanted_phrases: list) -&gt; None\n</code></pre> <p>Clean synopsis text by removing entries containing unwanted phrases.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing the synopsis column</p> <p> TYPE: <code>DataFrame</code> </p> <code>synopsis_col</code> <p>Name of the column containing synopsis text</p> <p> TYPE: <code>str</code> </p> <code>unwanted_phrases</code> <p>List of phrases that indicate invalid synopsis content</p> <p> TYPE: <code>list</code> </p> <p>The function modifies the DataFrame in-place, setting matching synopses to empty strings.</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def clean_synopsis(df: pd.DataFrame, synopsis_col: str, unwanted_phrases: list) -&gt; None:\n    \"\"\"\n    Clean synopsis text by removing entries containing unwanted phrases.\n\n    Args:\n        df: DataFrame containing the synopsis column\n        synopsis_col: Name of the column containing synopsis text\n        unwanted_phrases: List of phrases that indicate invalid synopsis content\n\n    The function modifies the DataFrame in-place, setting matching synopses to empty strings.\n    \"\"\"\n    logging.info(\"Cleaning synopses in column: %s\", synopsis_col)\n    for index, row in df.iterrows():\n        if pd.notna(row[synopsis_col]):\n            for phrase in unwanted_phrases:\n                if phrase in row[synopsis_col]:\n                    df.at[index, synopsis_col] = \"\"\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.consolidate_titles","title":"consolidate_titles","text":"<pre><code>consolidate_titles(df: DataFrame, title_columns: list) -&gt; Series\n</code></pre> <p>Consolidate multiple title columns into a single title column.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing multiple title columns</p> <p> TYPE: <code>DataFrame</code> </p> <code>title_columns</code> <p>List of column names containing titles to consolidate</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pd.Series: Consolidated titles, using first non-null value found across columns</p> <p>The function prioritizes existing 'title' column if present, then fills missing values from other title columns in order. Empty strings and 'unknown title' are treated as null.</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def consolidate_titles(df: pd.DataFrame, title_columns: list) -&gt; pd.Series:\n    \"\"\"\n    Consolidate multiple title columns into a single title column.\n\n    Args:\n        df: DataFrame containing multiple title columns\n        title_columns: List of column names containing titles to consolidate\n\n    Returns:\n        pd.Series: Consolidated titles, using first non-null value found across columns\n\n    The function prioritizes existing 'title' column if present, then fills missing values\n    from other title columns in order. Empty strings and 'unknown title' are treated as null.\n    \"\"\"\n    logging.info(\"Consolidating titles into a single 'title' column.\")\n    if \"title\" in df.columns:\n        consolidated_title = df[\"title\"]\n        logging.info(\"Found existing 'title' column.\")\n    else:\n        consolidated_title = pd.Series([\"\"] * len(df), index=df.index)\n        logging.info(\"Initialized 'title' column as empty.\")\n\n    for col in title_columns:\n        if col in df.columns:\n            logging.info(\"Consolidating title from column: %s\", col)\n            consolidated_title = consolidated_title.where(\n                consolidated_title.notna(), df[col]\n            )\n        else:\n            logging.warning(\"Title column '%s' not found in DataFrame.\", col)\n\n    consolidated_title.replace([\"\", \"unknown title\"], pd.NA, inplace=True)\n    missing_titles = consolidated_title.isna().sum()\n    if missing_titles &gt; 0:\n        logging.warning(\n            \"Found %d entries with missing titles after consolidation.\", missing_titles\n        )\n    else:\n        logging.info(\"All titles consolidated successfully.\")\n    return consolidated_title\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.find_additional_info","title":"find_additional_info","text":"<pre><code>find_additional_info(row: Series, additional_df: DataFrame, description_col: str, name_columns: list) -&gt; Optional[str]\n</code></pre> <p>Find matching description information from additional dataset.</p> PARAMETER DESCRIPTION <code>row</code> <p>Series containing title information to match</p> <p> TYPE: <code>Series</code> </p> <code>additional_df</code> <p>DataFrame containing additional descriptions</p> <p> TYPE: <code>DataFrame</code> </p> <code>description_col</code> <p>Name of column containing descriptions</p> <p> TYPE: <code>str</code> </p> <code>name_columns</code> <p>List of column names to use for matching titles</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>Optional[str]</code> <p>str | None: Matching description if found, None otherwise</p> <p>Attempts to match titles across multiple name columns and returns first matching description.</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def find_additional_info(\n    row: pd.Series,\n    additional_df: pd.DataFrame,\n    description_col: str,\n    name_columns: list,\n) -&gt; Optional[str]:\n    \"\"\"\n    Find matching description information from additional dataset.\n\n    Args:\n        row: Series containing title information to match\n        additional_df: DataFrame containing additional descriptions\n        description_col: Name of column containing descriptions\n        name_columns: List of column names to use for matching titles\n\n    Returns:\n        str | None: Matching description if found, None otherwise\n\n    Attempts to match titles across multiple name columns and returns first matching description.\n    \"\"\"\n    for merged_name_col in [\"title\", \"title_english\", \"title_japanese\"]:\n        if pd.isna(row[merged_name_col]) or row[merged_name_col] == \"\":\n            continue\n        for additional_name_col in name_columns:\n            if row[merged_name_col] in additional_df[additional_name_col].values:\n                info = additional_df.loc[\n                    additional_df[additional_name_col] == row[merged_name_col],\n                    description_col,\n                ]\n                if isinstance(info, pd.Series):\n                    info = info.dropna().iloc[0] if not info.dropna().empty else None\n                    if info:\n                        logging.debug(\n                            \"Found additional info for '%s' from column '%s'.\",\n                            row[merged_name_col],\n                            description_col,\n                        )\n                        return info\n    logging.debug(\n        \"No additional info found for row with titles: %s, %s, %s.\",\n        row.get(\"title\", \"\"),\n        row.get(\"title_english\", \"\"),\n        row.get(\"title_japanese\", \"\"),\n    )\n    return None\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Main function to parse command-line arguments and merge datasets.</p> <p>This function serves as the entry point for the dataset merging process. It parses command-line arguments to determine whether to merge anime or manga datasets, then calls the appropriate merging function.</p> <p>The function expects a single command-line argument --type which must be either 'anime' or 'manga'. It will: 1. Parse the command-line arguments 2. Log the specified dataset type 3. Call merge_anime_datasets() or merge_manga_datasets() based on the type 4. Log an error if an invalid type is specified</p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"\n    Main function to parse command-line arguments and merge datasets.\n\n    This function serves as the entry point for the dataset merging process. It parses\n    command-line arguments to determine whether to merge anime or manga datasets, then\n    calls the appropriate merging function.\n\n    The function expects a single command-line argument --type which must be either\n    'anime' or 'manga'. It will:\n    1. Parse the command-line arguments\n    2. Log the specified dataset type\n    3. Call merge_anime_datasets() or merge_manga_datasets() based on the type\n    4. Log an error if an invalid type is specified\n\n    Returns:\n        None\n\n    Raises:\n        No exceptions are raised directly, but underlying merge functions may raise exceptions\n    \"\"\"\n    args = parse_args()\n    dataset_type: str = args.type\n    logging.info(\"Dataset type specified: '%s'.\", dataset_type)\n\n    if dataset_type == \"anime\":\n        merge_anime_datasets()\n    elif dataset_type == \"manga\":\n        merge_manga_datasets()\n    else:\n        logging.error(\"Invalid type specified. Use 'anime' or 'manga'.\")\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.merge_anime_datasets","title":"merge_anime_datasets","text":"<pre><code>merge_anime_datasets() -&gt; DataFrame\n</code></pre> <p>Merge multiple anime datasets into a single comprehensive dataset.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: Merged and cleaned anime dataset</p> Performs the following operations <ul> <li>Loads multiple anime datasets from files and Hugging Face</li> <li>Cleans and standardizes text fields</li> <li>Removes adult content and kids' content</li> <li>Merges datasets based on IDs and titles</li> <li>Consolidates synopsis information</li> <li>Removes duplicates</li> <li>Saves final dataset to CSV with progress tracking</li> </ul> RAISES DESCRIPTION <code>Exception</code> <p>If any error occurs during merging process</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def merge_anime_datasets() -&gt; pd.DataFrame:\n    \"\"\"\n    Merge multiple anime datasets into a single comprehensive dataset.\n\n    Returns:\n        pd.DataFrame: Merged and cleaned anime dataset\n\n    Performs the following operations:\n        - Loads multiple anime datasets from files and Hugging Face\n        - Cleans and standardizes text fields\n        - Removes adult content and kids' content\n        - Merges datasets based on IDs and titles\n        - Consolidates synopsis information\n        - Removes duplicates\n        - Saves final dataset to CSV with progress tracking\n\n    Raises:\n        Exception: If any error occurs during merging process\n    \"\"\"\n    logging.info(\"Starting to merge anime datasets.\")\n    try:\n        # Load datasets\n        logging.info(\"Loading anime datasets from CSV files.\")\n        myanimelist_dataset: pd.DataFrame = pd.read_csv(\"data/anime/anime.csv\")\n        anime_dataset_2023: pd.DataFrame = pd.read_csv(\n            \"data/anime/anime-dataset-2023.csv\"\n        )\n        animes: pd.DataFrame = pd.read_csv(\"data/anime/animes.csv\")\n        anime_4500: pd.DataFrame = pd.read_csv(\"data/anime/anime4500.csv\")\n        anime_2022: pd.DataFrame = pd.read_csv(\"data/anime/Anime-2022.csv\")\n        anime_data: pd.DataFrame = pd.read_csv(\"data/anime/Anime_data.csv\")\n        anime2: pd.DataFrame = pd.read_csv(\"data/anime/Anime2.csv\")\n        mal_anime: pd.DataFrame = pd.read_csv(\"data/anime/mal_anime.csv\")\n\n        # Load using the datasets library\n        logging.info(\"Loading 'anime_270' dataset from Hugging Face datasets.\")\n        anime_270 = load_dataset(\"johnidouglas/anime_270\", split=\"train\")\n        anime_270_df: pd.DataFrame = anime_270.to_pandas()  # type: ignore\n\n        logging.info(\"Loading 'wykonos/anime' dataset from Hugging Face datasets.\")\n        wykonos_dataset = load_dataset(\"wykonos/anime\", split=\"train\")\n        wykonos_dataset_df: pd.DataFrame = wykonos_dataset.to_pandas()  # type: ignore\n\n        # Drop specified columns from myanimelist_dataset\n        columns_to_drop: list[str] = [\n            \"scored_by\",\n            \"source\",\n            \"members\",\n            \"favorites\",\n            \"start_date\",\n            \"end_date\",\n            \"episode_duration\",\n            \"total_duration\",\n            \"rating\",\n            \"sfw\",\n            \"approved\",\n            \"created_at\",\n            \"updated_at\",\n            \"real_start_date\",\n            \"real_end_date\",\n            \"broadcast_day\",\n            \"broadcast_time\",\n            \"studios\",\n            \"producers\",\n            \"licensors\",\n        ]\n        logging.info(\"Dropping unnecessary columns from 'myanimelist_dataset'.\")\n        myanimelist_dataset.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n        # Remove row if 'type' is 'Music'\n        myanimelist_dataset = myanimelist_dataset[\n            myanimelist_dataset[\"type\"] != \"music\"\n        ]\n\n        # Remove row if 'demographics' contains 'Kids'\n        myanimelist_dataset = myanimelist_dataset[\n            ~myanimelist_dataset[\"demographics\"].apply(\n                lambda x: any(genre in [\"Kids\"] for genre in ast.literal_eval(x))\n            )\n        ]\n\n        # Check for duplicates in the keys and remove them\n        duplicate_checks: list[tuple[str, pd.DataFrame, str]] = [\n            (\"anime_id\", myanimelist_dataset, \"myanimelist_dataset\"),\n            (\"anime_id\", anime_dataset_2023, \"anime_dataset_2023\"),\n            (\"uid\", animes, \"animes\"),\n            (\"ID\", anime_2022, \"anime_2022\"),\n        ]\n\n        for key, df, name in duplicate_checks:\n            if df[key].duplicated().any():\n                logging.warning(\n                    \"Duplicate '%s' found in %s. Removing duplicates.\", key, name\n                )\n                df.drop_duplicates(subset=key, inplace=True)\n                df.to_csv(f\"data/anime/{name}.csv\", index=False)\n                logging.info(\"Duplicates removed and updated '%s.csv'.\", name)\n\n        # Preprocess names for matching\n        logging.info(\"Preprocessing names for matching.\")\n        preprocess_columns: dict[str, list[str]] = {\n            \"myanimelist_dataset\": [\"title\", \"title_english\", \"title_japanese\"],\n            \"anime_dataset_2023\": [\"Name\", \"English name\", \"Other name\"],\n            \"anime_4500\": [\"Title\"],\n            \"wykonos_dataset_df\": [\"Name\", \"Japanese_name\"],\n            \"anime_data\": [\"Name\"],\n            \"anime2\": [\"Name\"],\n            \"mal_anime\": [\"title\"],\n        }\n\n        for df_name, cols in preprocess_columns.items():\n            df = locals()[df_name]\n            for col in cols:\n                if col in df.columns:\n                    logging.info(\"Preprocessing column '%s' in '%s'.\", col, df_name)\n                    df[col] = df[col].apply(preprocess_name)\n\n        # Clean synopses in specific datasets\n        logging.info(\"Cleaning synopses in specific datasets.\")\n        unwanted_phrases = sorted(\n            [\n                \"A song\",\n                \"A music video\",\n                \"A new music video\",\n                \"A series animated music video\",\n                \"A short animation\",\n                \"A short film\",\n                \"A special music video\",\n                \"An animated music\",\n                \"An animated music video\",\n                \"An animation\",\n                \"An educational film\",\n                \"An independent music\",\n                \"An original song\",\n                \"Animated music video\",\n                \"Minna uta\",\n                \"Minna Uta\",\n                \"Music clip\",\n                \"Music video\",\n                \"No description available for this anime.\",\n                \"No synopsis has been added for this series yet.\",\n                \"No synopsis information has been added to this title.\",\n                \"No synopsis yet\",\n                \"Official music video\",\n                \"Short film\",\n                \"The animated film\",\n                \"The animated music video\",\n                \"The music video\",\n                \"The official music\",\n                \"This music video\",\n                \"Unknown\",\n            ]\n        )\n\n        clean_synopsis(anime_dataset_2023, \"Synopsis\", unwanted_phrases)\n        clean_synopsis(anime_2022, \"Synopsis\", unwanted_phrases)\n        clean_synopsis(wykonos_dataset_df, \"Description\", unwanted_phrases)\n        clean_synopsis(anime_data, \"Description\", unwanted_phrases)\n        clean_synopsis(anime2, \"Description\", unwanted_phrases)\n        clean_synopsis(mal_anime, \"synopsis\", unwanted_phrases)\n        clean_synopsis(animes, \"synopsis\", unwanted_phrases)\n        clean_synopsis(myanimelist_dataset, \"synopsis\", unwanted_phrases)\n\n        # Merge datasets on 'anime_id'\n        logging.info(\"Merging 'myanimelist_dataset' with 'anime_dataset_2023'.\")\n        final_merged_df: pd.DataFrame = pd.merge(\n            myanimelist_dataset,\n            anime_dataset_2023[[\"anime_id\", \"Synopsis\", \"Name\"]].rename(\n                columns={\"Name\": \"title_anime_dataset_2023\"}\n            ),\n            on=\"anime_id\",\n            how=\"outer\",\n        )\n        final_merged_df.rename(\n            columns={\"Synopsis\": \"Synopsis anime_dataset_2023\"}, inplace=True\n        )\n        logging.info(\"Dropped 'ID' and other unnecessary columns after first merge.\")\n        final_merged_df.drop(columns=[\"ID\"], inplace=True, errors=\"ignore\")\n\n        logging.info(\"Merging with 'animes' dataset on 'uid'.\")\n        final_merged_df = pd.merge(\n            final_merged_df,\n            animes[[\"uid\", \"synopsis\", \"title\"]].rename(\n                columns={\"title\": \"title_animes\"}\n            ),\n            left_on=\"anime_id\",\n            right_on=\"uid\",\n            how=\"outer\",\n            suffixes=(\"\", \"_animes\"),\n        )\n        final_merged_df.drop(columns=[\"uid\"], inplace=True, errors=\"ignore\")\n        final_merged_df.rename(\n            columns={\"synopsis_animes\": \"Synopsis animes dataset\"}, inplace=True\n        )\n\n        logging.info(\"Merging with 'anime_270_df' dataset on 'MAL_ID'.\")\n        final_merged_df = pd.merge(\n            final_merged_df,\n            anime_270_df[[\"MAL_ID\", \"sypnopsis\", \"Name\"]].rename(\n                columns={\"Name\": \"title_anime_270\"}\n            ),\n            left_on=\"anime_id\",\n            right_on=\"MAL_ID\",\n            how=\"outer\",\n        )\n        final_merged_df.rename(\n            columns={\"sypnopsis\": \"Synopsis anime_270 Dataset\"}, inplace=True\n        )\n        final_merged_df.drop(columns=[\"MAL_ID\"], inplace=True, errors=\"ignore\")\n\n        logging.info(\"Merging with 'anime_2022' dataset on 'ID'.\")\n        final_merged_df = pd.merge(\n            final_merged_df,\n            anime_2022[[\"ID\", \"Synopsis\", \"Title\"]].rename(\n                columns={\"Title\": \"title_anime_2022\"}\n            ),\n            left_on=\"anime_id\",\n            right_on=\"ID\",\n            how=\"outer\",\n        )\n        final_merged_df.rename(\n            columns={\"Synopsis\": \"Synopsis Anime-2022 Dataset\"}, inplace=True\n        )\n        final_merged_df.drop(columns=[\"ID\"], inplace=True, errors=\"ignore\")\n\n        # Consolidate all title columns into a single 'title' column\n        logging.info(\"Consolidating all title columns into a single 'title' column.\")\n        title_columns: list[str] = [\n            \"title_anime_dataset_2023\",\n            \"title_animes\",\n            \"title_anime_270\",\n            \"title_anime_2022\",\n        ]\n        final_merged_df[\"title\"] = consolidate_titles(final_merged_df, title_columns)\n\n        # Drop redundant title columns\n        logging.info(\"Dropping redundant title columns: %s\", title_columns)\n        final_merged_df.drop(columns=title_columns, inplace=True, errors=\"ignore\")\n\n        # Update the merged dataset with additional synopses from various sources\n        logging.info(\"Adding additional synopses from various sources.\")\n        final_merged_df = add_additional_info(\n            final_merged_df,\n            anime_4500,\n            \"Description\",\n            [\"Title\"],\n            \"Synopsis anime4500 Dataset\",\n        )\n        final_merged_df = add_additional_info(\n            final_merged_df,\n            wykonos_dataset_df,\n            \"Description\",\n            [\"Name\", \"Japanese_name\"],\n            \"Synopsis wykonos Dataset\",\n        )\n        final_merged_df = add_additional_info(\n            final_merged_df,\n            anime_data,\n            \"Description\",\n            [\"Name\"],\n            \"Synopsis Anime_data Dataset\",\n        )\n        final_merged_df = add_additional_info(\n            final_merged_df,\n            anime2,\n            \"Description\",\n            [\"Name\", \"Japanese_name\"],\n            \"Synopsis anime2 Dataset\",\n        )\n        final_merged_df = add_additional_info(\n            final_merged_df,\n            mal_anime,\n            \"synopsis\",\n            [\"title\"],\n            \"Synopsis mal_anime Dataset\",\n        )\n\n        synopsis_cols: list[str] = [\n            \"synopsis\",\n            \"Synopsis anime_dataset_2023\",\n            \"Synopsis animes dataset\",\n            \"Synopsis anime_270 Dataset\",\n            \"Synopsis Anime-2022 Dataset\",\n            \"Synopsis anime4500 Dataset\",\n            \"Synopsis wykonos Dataset\",\n            \"Synopsis Anime_data Dataset\",\n            \"Synopsis anime2 Dataset\",\n            \"Synopsis mal_anime Dataset\",\n        ]\n        preprocess_synopsis_columns(final_merged_df, synopsis_cols)\n\n        logging.info(\"Removing duplicate synopses across columns: %s\", synopsis_cols)\n        final_merged_df = remove_duplicate_infos(final_merged_df, synopsis_cols)\n\n        # Remove duplicates based on 'anime_id'\n        logging.info(\"Removing duplicates based on 'anime_id'.\")\n        final_merged_df.drop_duplicates(subset=[\"anime_id\"], inplace=True)\n\n        # Remove rows with all empty or NaN synopsis columns\n        logging.info(\"Removing rows with all empty or NaN synopsis columns.\")\n        initial_row_count = len(final_merged_df)\n        final_merged_df = final_merged_df[\n            final_merged_df[synopsis_cols].apply(\n                lambda x: x.str.strip().replace(\"\", pd.NA).notna().any(), axis=1\n            )\n        ]\n        removed_rows = initial_row_count - len(final_merged_df)\n        logging.info(\n            \"Removed %d rows with all empty or NaN synopsis columns.\", removed_rows\n        )\n\n        # Save the updated merged dataset with a progress bar\n        logging.info(\n            \"Saving the merged anime dataset to 'model/merged_anime_dataset.csv'.\"\n        )\n        chunk_size: int = 1000\n        total_chunks: int = (len(final_merged_df) // chunk_size) + 1\n\n        with open(\n            \"model/merged_anime_dataset.csv\", \"w\", newline=\"\", encoding=\"utf-8\"\n        ) as f:\n            # Write the header\n            final_merged_df.iloc[:0].to_csv(f, index=False)\n            for i in tqdm(range(total_chunks), desc=\"Saving to CSV\"):\n                start: int = i * chunk_size\n                end: int = start + chunk_size\n                final_merged_df.iloc[start:end].to_csv(f, header=False, index=False)\n\n        logging.info(\n            \"Anime datasets merged and saved to 'model/merged_anime_dataset.csv'.\"\n        )\n        return final_merged_df\n    except Exception as e:\n        logging.error(\n            \"An error occurred while merging anime datasets: %s\", e, exc_info=True\n        )\n        raise\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.merge_manga_datasets","title":"merge_manga_datasets","text":"<pre><code>merge_manga_datasets() -&gt; DataFrame\n</code></pre> <p>Merge multiple manga datasets into a single comprehensive dataset.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: Merged and cleaned manga dataset</p> Performs the following operations <ul> <li>Loads manga datasets from multiple CSV files</li> <li>Removes adult content</li> <li>Cleans and standardizes text fields</li> <li>Merges datasets based on IDs and titles</li> <li>Consolidates synopsis information</li> <li>Removes duplicates</li> <li>Saves final dataset to CSV with progress tracking</li> </ul> RAISES DESCRIPTION <code>Exception</code> <p>If any error occurs during merging process</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def merge_manga_datasets() -&gt; pd.DataFrame:\n    \"\"\"\n    Merge multiple manga datasets into a single comprehensive dataset.\n\n    Returns:\n        pd.DataFrame: Merged and cleaned manga dataset\n\n    Performs the following operations:\n        - Loads manga datasets from multiple CSV files\n        - Removes adult content\n        - Cleans and standardizes text fields\n        - Merges datasets based on IDs and titles\n        - Consolidates synopsis information\n        - Removes duplicates\n        - Saves final dataset to CSV with progress tracking\n\n    Raises:\n        Exception: If any error occurs during merging process\n    \"\"\"\n    logging.info(\"Starting to merge manga datasets.\")\n    try:\n        # Load datasets\n        logging.info(\"Loading manga datasets from CSV files.\")\n        manga_main: pd.DataFrame = pd.read_csv(\"data/manga/manga.csv\")  # Base dataset\n        jikan: pd.DataFrame = pd.read_csv(\"data/manga/jikan.csv\")\n        data: pd.DataFrame = pd.read_csv(\"data/manga/data.csv\")\n\n        # Drop specified columns from manga_main if necessary\n        columns_to_drop: list[str] = [\n            \"scored_by\",\n            \"members\",\n            \"favorites\",\n            \"end_date\",\n            \"sfw\",\n            \"approved\",\n            \"created_at\",\n            \"updated_at\",\n            \"real_start_date\",\n            \"real_end_date\",\n            \"authors\",\n            \"serializations\",\n        ]\n        logging.info(\"Dropping unnecessary columns from 'manga_main' dataset.\")\n        manga_main.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n        # Identify and remove rows with 'genres' containing 'Hentai' or 'Boys Love'\n        logging.info(\"Identifying IDs with 'Hentai' or 'Boys Love' genres.\")\n        removed_ids = set(\n            manga_main[\n                manga_main[\"genres\"].apply(\n                    lambda x: any(\n                        genre in [\"Hentai\", \"Boys Love\"]\n                        for genre in ast.literal_eval(x)\n                    )\n                )\n            ][\"manga_id\"]\n        )\n        logging.info(\"Removing rows with 'Hentai' or 'Boys Love' genres.\")\n        manga_main = manga_main[~manga_main[\"manga_id\"].isin(removed_ids)]\n\n        # Check for duplicates in the keys and remove them\n        duplicate_checks: list[tuple[str, pd.DataFrame, str]] = [\n            (\"manga_id\", manga_main, \"manga_main\"),\n            (\"mal_id\", jikan, \"jikan\"),\n            (\"title\", data, \"data\"),\n        ]\n\n        for key, df, name in duplicate_checks:\n            if df[key].duplicated().any():\n                logging.warning(\n                    \"Duplicate '%s' found in %s. Removing duplicates.\", key, name\n                )\n                df.drop_duplicates(subset=key, inplace=True)\n                df.to_csv(f\"data/manga/{name}.csv\", index=False)\n                logging.info(\"Duplicates removed and updated '%s.csv'.\", name)\n\n        # Preprocess names for matching\n        logging.info(\"Preprocessing names for matching.\")\n        preprocess_columns: dict[str, list[str]] = {\n            \"manga_main\": [\"title\", \"title_english\", \"title_japanese\"],\n            \"jikan\": [\"title\"],\n            \"data\": [\"title\"],\n        }\n\n        for df_name, cols in preprocess_columns.items():\n            df = locals()[df_name]\n            for col in cols:\n                if col in df.columns:\n                    logging.info(\"Preprocessing column '%s' in '%s'.\", col, df_name)\n                    df[col] = df[col].apply(preprocess_name)\n\n        # Clean synopses in specific datasets\n        logging.info(\"Cleaning synopses in specific datasets.\")\n        clean_synopsis(manga_main, \"synopsis\", [\"No synopsis\"])\n        clean_synopsis(\n            data, \"description\", [\"This entry currently doesn't have a synopsis.\"]\n        )\n        clean_synopsis(jikan, \"synopsis\", [\"Looking for information on the\"])\n        clean_synopsis(jikan, \"synopsis\", [\"No synopsis\"])\n\n        # Merge main dataset with jikan on 'manga_id' and 'mal_id'\n        logging.info(\n            \"Merging 'manga_main' with 'jikan' dataset on 'manga_id' and 'mal_id'.\"\n        )\n        merged_df: pd.DataFrame = pd.merge(\n            manga_main,\n            jikan[~jikan[\"mal_id\"].isin(removed_ids)][\n                [\"mal_id\", \"synopsis\", \"title\"]\n            ].rename(columns={\"title\": \"title_jikan\"}),\n            left_on=\"manga_id\",\n            right_on=\"mal_id\",\n            how=\"outer\",\n            suffixes=(\"\", \"_jikan\"),\n        )\n        merged_df.rename(\n            columns={\"synopsis_jikan\": \"Synopsis jikan Dataset\"}, inplace=True\n        )\n        merged_df.drop(columns=[\"mal_id\", \"title_jikan\"], inplace=True, errors=\"ignore\")\n        logging.info(\"Dropped 'mal_id' and 'title_jikan' after first merge.\")\n\n        # Merge with data on title\n        logging.info(\"Merging with 'data' dataset on 'title'.\")\n        merged_df = add_additional_info(\n            merged_df,\n            data[~data[\"title\"].isin(removed_ids)],\n            \"description\",\n            [\"title\"],\n            \"Synopsis data Dataset\",\n        )\n\n        info_cols: list[str] = [\n            \"synopsis\",\n            \"Synopsis jikan Dataset\",\n            \"Synopsis data Dataset\",\n        ]\n        preprocess_synopsis_columns(merged_df, info_cols)\n\n        remove_numbered_list_synopsis(merged_df, info_cols)\n\n        logging.info(\"Removing duplicate synopses and descriptions.\")\n        merged_df = remove_duplicate_infos(merged_df, info_cols)\n\n        # Remove duplicates based on 'manga_id'\n        logging.info(\"Removing duplicates based on 'manga_id'.\")\n        merged_df.drop_duplicates(subset=[\"manga_id\"], inplace=True)\n\n        # Remove rows with all empty or NaN synopsis columns\n        logging.info(\"Removing rows with all empty or NaN synopsis columns.\")\n        initial_row_count = len(merged_df)\n        merged_df = merged_df[\n            merged_df[info_cols].apply(\n                lambda x: x.str.strip().replace(\"\", pd.NA).notna().any(), axis=1\n            )\n        ]\n        removed_rows = initial_row_count - len(merged_df)\n        logging.info(\n            \"Removed %d rows with all empty or NaN synopsis columns.\", removed_rows\n        )\n\n        # Save the updated merged dataset with a progress bar\n        logging.info(\n            \"Saving the merged manga dataset to 'model/merged_manga_dataset.csv'.\"\n        )\n        chunk_size: int = 1000\n        total_chunks: int = (len(merged_df) // chunk_size) + 1\n\n        with open(\n            \"model/merged_manga_dataset.csv\", \"w\", newline=\"\", encoding=\"utf-8\"\n        ) as f:\n            # Write the header\n            merged_df.iloc[:0].to_csv(f, index=False)\n            logging.info(\"Writing data in chunks of %d.\", chunk_size)\n            for i in tqdm(range(total_chunks), desc=\"Saving to CSV\"):\n                start: int = i * chunk_size\n                end: int = start + chunk_size\n                merged_df.iloc[start:end].to_csv(f, header=False, index=False)\n\n        logging.info(\n            \"Manga datasets merged and saved to 'model/merged_manga_dataset.csv'.\"\n        )\n        return merged_df\n    except Exception as e:\n        logging.error(\n            \"An error occurred while merging manga datasets: %s\", e, exc_info=True\n        )\n        raise\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.parse_args","title":"parse_args","text":"<pre><code>parse_args() -&gt; Namespace\n</code></pre> <p>Parse command line arguments for dataset type selection.</p> RETURNS DESCRIPTION <code>Namespace</code> <p>argparse.Namespace: Parsed arguments containing: type (str): Either 'anime' or 'manga' to specify dataset type to merge</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def parse_args() -&gt; argparse.Namespace:\n    \"\"\"\n    Parse command line arguments for dataset type selection.\n\n    Returns:\n        argparse.Namespace: Parsed arguments containing:\n            type (str): Either 'anime' or 'manga' to specify dataset type to merge\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Merge anime or manga datasets into a single dataset.\"\n    )\n    parser.add_argument(\n        \"--type\",\n        type=str,\n        choices=[\"anime\", \"manga\"],\n        required=True,\n        help=\"Type of dataset to generate: 'anime' or 'manga'.\",\n    )\n    return parser.parse_args()\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.preprocess_name","title":"preprocess_name","text":"<pre><code>preprocess_name(name: Any) -&gt; str\n</code></pre> <p>Preprocess a name string for consistent matching.</p> PARAMETER DESCRIPTION <code>name</code> <p>Input name value of any type</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Preprocessed name in lowercase with whitespace stripped Returns empty string if input is null/NaN</p> <p> TYPE: <code>str</code> </p> <p>Used to standardize names across datasets before matching/merging.</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def preprocess_name(name: Any) -&gt; str:\n    \"\"\"\n    Preprocess a name string for consistent matching.\n\n    Args:\n        name: Input name value of any type\n\n    Returns:\n        str: Preprocessed name in lowercase with whitespace stripped\n            Returns empty string if input is null/NaN\n\n    Used to standardize names across datasets before matching/merging.\n    \"\"\"\n    if pd.isna(name):\n        return \"\"\n    return str(name).strip().lower()\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.preprocess_synopsis_columns","title":"preprocess_synopsis_columns","text":"<pre><code>preprocess_synopsis_columns(df: DataFrame, synopsis_cols: list[str]) -&gt; None\n</code></pre> <p>Preprocess text in synopsis columns for consistency.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing synopsis columns</p> <p> TYPE: <code>DataFrame</code> </p> <code>synopsis_cols</code> <p>List of column names containing synopsis text</p> <p> TYPE: <code>list[str]</code> </p> <p>Applies common text preprocessing to each synopsis column in-place. Uses common.preprocess_text() for standardization. Logs warning if specified column not found.</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def preprocess_synopsis_columns(df: pd.DataFrame, synopsis_cols: list[str]) -&gt; None:\n    \"\"\"\n    Preprocess text in synopsis columns for consistency.\n\n    Args:\n        df: DataFrame containing synopsis columns\n        synopsis_cols: List of column names containing synopsis text\n\n    Applies common text preprocessing to each synopsis column in-place.\n    Uses common.preprocess_text() for standardization.\n    Logs warning if specified column not found.\n    \"\"\"\n    logging.info(\"Preprocessing synopsis columns: %s\", synopsis_cols)\n    for col in synopsis_cols:\n        if col in df.columns:\n            logging.info(\"Preprocessing column: %s\", col)\n            df[col] = df[col].apply(common.preprocess_text)\n        else:\n            logging.warning(\"Synopsis column '%s' not found in DataFrame.\", col)\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.remove_duplicate_infos","title":"remove_duplicate_infos","text":"<pre><code>remove_duplicate_infos(df: DataFrame, info_cols: list[str]) -&gt; DataFrame\n</code></pre> <p>Remove duplicate synopsis/description entries across columns.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing synopsis columns</p> <p> TYPE: <code>DataFrame</code> </p> <code>info_cols</code> <p>List of column names containing synopsis information</p> <p> TYPE: <code>list[str]</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: DataFrame with duplicate synopses removed</p> <p>Keeps first occurrence of each unique synopsis and sets duplicates to NA. Processes row-by-row to maintain data integrity.</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def remove_duplicate_infos(df: pd.DataFrame, info_cols: list[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove duplicate synopsis/description entries across columns.\n\n    Args:\n        df: DataFrame containing synopsis columns\n        info_cols: List of column names containing synopsis information\n\n    Returns:\n        pd.DataFrame: DataFrame with duplicate synopses removed\n\n    Keeps first occurrence of each unique synopsis and sets duplicates to NA.\n    Processes row-by-row to maintain data integrity.\n    \"\"\"\n    for index, row in df.iterrows():\n        unique_infos = set()\n        for col in info_cols:\n            if pd.notna(row[col]) and row[col] not in unique_infos:\n                unique_infos.add(row[col])\n            else:\n                df.at[index, col] = pd.NA\n                logging.debug(\n                    \"Removed duplicate info for row %d in column '%s'.\", index, col\n                )\n    logging.info(\"Duplicate removal completed.\")\n    return df\n</code></pre>"},{"location":"MergeDatasets/#src.merge_datasets.remove_numbered_list_synopsis","title":"remove_numbered_list_synopsis","text":"<pre><code>remove_numbered_list_synopsis(df: DataFrame, synopsis_cols: list[str]) -&gt; None\n</code></pre> <p>Remove synopsis entries that are formatted as numbered lists.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing the synopsis columns</p> <p> TYPE: <code>DataFrame</code> </p> <code>synopsis_cols</code> <p>List of column names containing synopsis text</p> <p> TYPE: <code>list[str]</code> </p> <p>The function modifies the DataFrame in-place, setting numbered list synopses to empty strings. Uses regex pattern matching to identify numbered list formats.</p> Source code in <code>src/merge_datasets.py</code> <pre><code>def remove_numbered_list_synopsis(df: pd.DataFrame, synopsis_cols: list[str]) -&gt; None:\n    \"\"\"\n    Remove synopsis entries that are formatted as numbered lists.\n\n    Args:\n        df: DataFrame containing the synopsis columns\n        synopsis_cols: List of column names containing synopsis text\n\n    The function modifies the DataFrame in-place, setting numbered list synopses to empty strings.\n    Uses regex pattern matching to identify numbered list formats.\n    \"\"\"\n    logging.info(\"Removing numbered list synopses in columns: %s\", synopsis_cols)\n    numbered_list_pattern = re.compile(\n        r\"(?s)^.*?(\\d+[-\\d]*[.)]\\s+.+?)(?:\\n|$)\", re.MULTILINE\n    )\n\n    for col in synopsis_cols:\n        logging.info(\"Removing numbered list synopses in column: %s\", col)\n        df[col] = df[col].apply(\n            lambda x: \"\" if pd.notna(x) and numbered_list_pattern.match(x) else x\n        )\n</code></pre>"},{"location":"RunServer/","title":"RunServer","text":"<p>Starts the Flask application using an appropriate server based on the operating system.</p> The module accepts optional command line arguments <ul> <li>First argument: Device type ('cuda' or 'cpu', defaults to 'cpu')</li> <li>Second argument: Number of workers/threads (positive integer, defaults to 4)</li> </ul> Server selection <ul> <li>Linux: Uses Gunicorn with specified number of worker processes</li> <li>Windows: Uses Waitress with specified number of threads</li> <li>Other OS: Uses Flask's built-in development server</li> </ul> <p>The server runs on port 21493 and binds to all network interfaces (0.0.0.0).</p>"},{"location":"RunServer/#src.run_server.run_server","title":"run_server","text":"<pre><code>run_server() -&gt; None\n</code></pre> <p>Start the Flask application using an OS-appropriate server with configurable settings.</p> <p>Command line arguments:</p> <pre><code>argv[1]: Device type ('cuda' or 'cpu', defaults to 'cpu')\n\nargv[2]: Number of workers/threads (positive integer, defaults to 4)\n</code></pre> <p>Environment variables set:</p> <pre><code>DEVICE: Set to the specified device type ('cuda' or 'cpu')\n</code></pre> <p>Server configuration:</p> Gunicorn <ul> <li>Workers: Specified by argv[2]</li> <li>Logs: ./logs/gunicorn_access.log and gunicorn_error.log</li> <li>Binds to: 0.0.0.0:21493</li> </ul> Waitress <ul> <li>Threads: Specified by argv[2]</li> <li>Port: 21493</li> </ul> <p>Other OS: Flask development server</p> Source code in <code>src/run_server.py</code> <pre><code>def run_server() -&gt; None:\n    \"\"\"\n    Start the Flask application using an OS-appropriate server with configurable settings.\n\n    Command line arguments:\n\n        argv[1]: Device type ('cuda' or 'cpu', defaults to 'cpu')\n\n        argv[2]: Number of workers/threads (positive integer, defaults to 4)\n\n    Environment variables set:\n\n        DEVICE: Set to the specified device type ('cuda' or 'cpu')\n\n    Server configuration:\n\n    Linux: Gunicorn\n        - Workers: Specified by argv[2]\n        - Logs: ./logs/gunicorn_access.log and gunicorn_error.log\n        - Binds to: 0.0.0.0:21493\n\n    Windows: Waitress\n        - Threads: Specified by argv[2]\n        - Port: 21493\n\n        Other OS: Flask development server\n    \"\"\"\n    os_type: str = platform.system()\n\n    # Check for device argument\n    if len(sys.argv) &gt; 1:\n        device = sys.argv[1].lower()\n        if device not in [\"cuda\", \"cpu\"]:\n            print(\"Invalid device argument. Use 'cuda' or 'cpu'.\")\n            sys.exit(1)\n    else:\n        device = \"cpu\"  # Default to CPU if no argument is provided\n\n    # Check for threads argument\n    if len(sys.argv) &gt; 2:\n        try:\n            threads = int(sys.argv[2])\n            if threads &lt; 1:\n                raise ValueError\n        except ValueError:\n            print(\"Invalid threads argument. Use a positive integer.\")\n            sys.exit(1)\n    else:\n        threads = 4  # Default to 4 threads if no argument is provided\n\n    # Set the device as an environment variable\n    os.environ[\"DEVICE\"] = device\n\n    if os_type == \"Linux\":\n        # Use Gunicorn on Linux\n        print(f\"Running on Linux. Starting Gunicorn server with {threads} workers.\")\n        subprocess.run(\n            [\n                \"gunicorn\",\n                \"-w\",\n                str(threads),\n                \"-b\",\n                \"0.0.0.0:21493\",\n                \"--access-logfile\",\n                \"./logs/gunicorn_access.log\",\n                \"--error-logfile\",\n                \"./logs/gunicorn_error.log\",\n                \"src.api:app\",\n            ],\n            check=True,\n        )\n    elif os_type == \"Windows\":\n        # Use Waitress on Windows\n        print(f\"Running on Windows. Starting Waitress server with {threads} threads.\")\n        subprocess.run(\n            [\"waitress-serve\", \"--port=21493\", f\"--threads={threads}\", \"src.api:app\"],\n            check=True,\n        )\n    else:\n        print(f\"Running on {os_type}. Using Flask's built-in server.\")\n        subprocess.run([\"python\", \"src.api:app\"], check=True)\n</code></pre>"},{"location":"Sbert/","title":"Sbert","text":"<p>Generate Sentence-BERT (SBERT) embeddings for anime or manga datasets.</p> <p>This script loads a pre-trained SBERT model and generates embeddings for text data from anime or manga datasets. It handles batched processing, supports multiple synopsis/description columns, and saves the generated embeddings to disk.</p> Key Features <ul> <li>Configurable model selection via command line arguments</li> <li>Automatic device selection (CPU/CUDA) with optimized batch sizes</li> <li>Preprocessing of text data before embedding generation</li> <li>Batched processing for memory efficiency</li> <li>Comprehensive evaluation data recording</li> <li>Support for both pre-trained and fine-tuned models</li> </ul> <p>The embeddings are saved in separate directories based on the dataset type and model used. Performance metrics and model information are also recorded for evaluation purposes.</p>"},{"location":"Sbert/#src.sbert.get_sbert_embeddings","title":"get_sbert_embeddings","text":"<pre><code>get_sbert_embeddings(dataframe: DataFrame, sbert_model: SentenceTransformer, batch_size: int, column_name: str, model_name: str, device: str) -&gt; ndarray\n</code></pre> <p>Generate SBERT embeddings for text data using batched processing.</p> <p>Processes text data in batches to generate embeddings efficiently while managing memory usage. Supports mixed precision for specific models on CUDA devices.</p> PARAMETER DESCRIPTION <code>dataframe</code> <p>DataFrame containing the text data</p> <p> TYPE: <code>DataFrame</code> </p> <code>sbert_model</code> <p>Initialized SBERT model instance</p> <p> TYPE: <code>SentenceTransformer</code> </p> <code>batch_size</code> <p>Number of texts to process per batch</p> <p> TYPE: <code>int</code> </p> <code>column_name</code> <p>Name of column containing text data</p> <p> TYPE: <code>str</code> </p> <code>model_name</code> <p>Name/identifier of the SBERT model</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Computation device ('cpu' or 'cuda')</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>numpy.ndarray: Matrix of embeddings where each row corresponds to a text input</p> Source code in <code>src/sbert.py</code> <pre><code>def get_sbert_embeddings(\n    dataframe: pd.DataFrame,\n    sbert_model: SentenceTransformer,\n    batch_size: int,\n    column_name: str,\n    model_name: str,\n    device: str,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate SBERT embeddings for text data using batched processing.\n\n    Processes text data in batches to generate embeddings efficiently while managing memory usage.\n    Supports mixed precision for specific models on CUDA devices.\n\n    Args:\n        dataframe: DataFrame containing the text data\n        sbert_model: Initialized SBERT model instance\n        batch_size: Number of texts to process per batch\n        column_name: Name of column containing text data\n        model_name: Name/identifier of the SBERT model\n        device: Computation device ('cpu' or 'cuda')\n\n    Returns:\n        numpy.ndarray: Matrix of embeddings where each row corresponds to a text input\n    \"\"\"\n    embeddings_list = []\n    for i in tqdm(\n        range(0, len(dataframe), batch_size),\n        desc=f\"Generating Embeddings for {column_name}\",\n    ):\n        batch_texts = dataframe[column_name].iloc[i : i + batch_size].tolist()\n        if batch_texts:\n            if (\n                model_name == \"sentence-transformers/sentence-t5-xxl\"\n                and device == \"cuda\"\n            ):\n                # Use mixed precision for this specific model\n                with torch.no_grad():\n                    with torch.amp.autocast(\"cuda\"):  # type: ignore\n                        batch_embeddings = sbert_model.encode(\n                            batch_texts, convert_to_numpy=True, show_progress_bar=False\n                        )\n            else:\n                # Standard encoding for other models\n                with torch.no_grad():\n                    batch_embeddings = sbert_model.encode(\n                        batch_texts, convert_to_numpy=True, show_progress_bar=False\n                    )\n            embeddings_list.append(batch_embeddings)\n    torch.cuda.empty_cache()\n    if embeddings_list:\n        return np.vstack(embeddings_list)\n    return np.array([])\n</code></pre>"},{"location":"Sbert/#src.sbert.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Execute the SBERT embedding generation pipeline.</p> <p>Workflow:</p> <ol> <li> <p>Parse command line arguments and determine device</p> </li> <li> <p>Load and preprocess dataset based on type (anime/manga)</p> </li> <li> <p>Initialize SBERT model with appropriate configuration</p> </li> <li> <p>Generate embeddings for each text column in batches</p> </li> <li> <p>Save embeddings and evaluation data to disk</p> </li> </ol> <p>The function handles device selection, batch size optimization, and memory management based on the model and available hardware.</p> Source code in <code>src/sbert.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"\n    Execute the SBERT embedding generation pipeline.\n\n    Workflow:\n\n    1. Parse command line arguments and determine device\n\n    2. Load and preprocess dataset based on type (anime/manga)\n\n    3. Initialize SBERT model with appropriate configuration\n\n    4. Generate embeddings for each text column in batches\n\n    5. Save embeddings and evaluation data to disk\n\n    The function handles device selection, batch size optimization, and memory management\n    based on the model and available hardware.\n    \"\"\"\n    args = parse_args()\n\n    # Determine device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Device: {device}\")\n\n    # Parameters\n    model_name = args.model\n    dataset_type = args.type\n\n    # Load the merged dataset based on type\n    if dataset_type == \"anime\":\n        dataset_path = \"model/merged_anime_dataset.csv\"\n        synopsis_columns = [\n            \"synopsis\",\n            \"Synopsis anime_dataset_2023\",\n            \"Synopsis animes dataset\",\n            \"Synopsis anime_270 Dataset\",\n            \"Synopsis Anime-2022 Dataset\",\n            \"Synopsis anime4500 Dataset\",\n            \"Synopsis wykonos Dataset\",\n            \"Synopsis Anime_data Dataset\",\n            \"Synopsis anime2 Dataset\",\n            \"Synopsis mal_anime Dataset\",\n        ]\n        embeddings_save_dir = f\"model/anime/{model_name.split('/')[-1]}\"\n    elif dataset_type == \"manga\":\n        dataset_path = \"model/merged_manga_dataset.csv\"\n        synopsis_columns = [\n            \"synopsis\",\n            \"Synopsis jikan Dataset\",\n            \"Synopsis data Dataset\",\n        ]\n        embeddings_save_dir = f\"model/manga/{model_name.split('/')[-1]}\"\n    else:\n        raise ValueError(\"Invalid dataset type specified. Use 'anime' or 'manga'.\")\n\n    # Load the merged dataset\n    df = common.load_dataset(dataset_path)\n\n    # Preprocess each synopsis or description column\n    for col in synopsis_columns:\n        df[f\"Processed_{col}\"] = df[col].fillna(\"\").apply(common.preprocess_text)\n\n    if device == \"cuda\":\n        batch_size = 448\n        if model_name in [\n            \"sentence-transformers/gtr-t5-xl\",\n            \"sentence-transformers/sentence-t5-xl\",\n            \"sentence-transformers/sentence-t5-xxl\",\n        ]:\n            batch_size = 8\n            # Limited by GPU memory, must not go past Dedicated GPU memory (Will Freeze/Slow Down).\n            # Change as needed.\n            if model_name == \"sentence-transformers/sentence-t5-xxl\":\n                batch_size = 1\n                device = \"cpu\"\n    else:\n        batch_size = 128\n        if model_name == \"sentence-transformers/sentence-t5-xxl\":\n            batch_size = 1\n            device = \"cpu\"\n\n    # Create directory for model-specific embeddings\n    os.makedirs(embeddings_save_dir, exist_ok=True)\n\n    # Load the underlying Hugging Face model to access config\n    if (\n        model_name == \"fine_tuned_sbert_model_anime\"\n        or model_name == \"fine_tuned_sbert_model_manga\"\n    ):\n        model_name = f\"model/{model_name}\"\n    hf_model = AutoModel.from_pretrained(model_name)\n\n    # Check if the model is a path to a fine-tuned model\n    if os.path.exists(model_name):\n        # Load the fine-tuned model from the specified directory\n        model = SentenceTransformer(model_name, device=device)\n    else:\n        # Load a pre-trained model from Hugging Face\n        if not model_name.startswith(\"sentence-transformers/\"):\n            if model_name != \"toobi/anime\":\n                model_name = f\"sentence-transformers/{model_name}\"\n\n    # Define the maximum token counts for each model for both anime and manga\n    max_token_counts = {\n        \"toobi/anime\": {\"anime\": 733, \"manga\": 673},\n        \"sentence-transformers/all-distilroberta-v1\": {\"anime\": 704, \"manga\": 654},\n        \"sentence-transformers/all-MiniLM-L6-v1\": {\"anime\": 733, \"manga\": 673},\n        \"sentence-transformers/all-MiniLM-L12-v1\": {\"anime\": 733, \"manga\": 673},\n        \"sentence-transformers/all-MiniLM-L6-v2\": {\"anime\": 733, \"manga\": 673},\n        \"sentence-transformers/all-MiniLM-L12-v2\": {\"anime\": 733, \"manga\": 673},\n        \"sentence-transformers/all-mpnet-base-v1\": {\"anime\": 733, \"manga\": 673},\n        \"sentence-transformers/all-mpnet-base-v2\": {\"anime\": 733, \"manga\": 673},\n        \"sentence-transformers/all-roberta-large-v1\": {\"anime\": 704, \"manga\": 654},\n        \"sentence-transformers/gtr-t5-base\": {\"anime\": 843, \"manga\": 765},\n        \"sentence-transformers/gtr-t5-large\": {\"anime\": 843, \"manga\": 765},\n        \"sentence-transformers/gtr-t5-xl\": {\"anime\": 843, \"manga\": 765},\n        \"sentence-transformers/multi-qa-distilbert-dot-v1\": {\n            \"anime\": 733,\n            \"manga\": 673,\n        },\n        \"sentence-transformers/multi-qa-mpnet-base-cos-v1\": {\n            \"anime\": 733,\n            \"manga\": 673,\n        },\n        \"sentence-transformers/multi-qa-mpnet-base-dot-v1\": {\n            \"anime\": 733,\n            \"manga\": 673,\n        },\n        \"sentence-transformers/paraphrase-distilroberta-base-v2\": {\n            \"anime\": 704,\n            \"manga\": 654,\n        },\n        \"sentence-transformers/paraphrase-mpnet-base-v2\": {\n            \"anime\": 733,\n            \"manga\": 673,\n        },\n        \"sentence-transformers/sentence-t5-base\": {\"anime\": 843, \"manga\": 765},\n        \"sentence-transformers/sentence-t5-large\": {\"anime\": 843, \"manga\": 765},\n        \"sentence-transformers/sentence-t5-xl\": {\"anime\": 843, \"manga\": 765},\n        \"sentence-transformers/sentence-t5-xxl\": {\"anime\": 843, \"manga\": 765},\n        \"model/fine_tuned_sbert_model_anime\": {\"anime\": 843, \"manga\": 765},\n        \"model/fine_tuned_sbert_model_manga\": {\"anime\": 843, \"manga\": 765},\n    }\n\n    # Retrieve max_position_embeddings from the model's config\n    max_position_embeddings = (\n        hf_model.config.max_position_embeddings - 2\n        if hasattr(hf_model.config, \"max_position_embeddings\")\n        else max_token_counts.get(model_name, {}).get(dataset_type, 512)\n    )\n    print(f\"Model's max_position_embeddings: {max_position_embeddings}\")\n\n    # Initialize SBERT components with dynamic max_seq_length\n    word_embedding_model = models.Transformer(model_name)\n    word_embedding_model.max_seq_length = min(\n        max_token_counts.get(model_name, {}).get(dataset_type, max_position_embeddings),\n        max_position_embeddings,\n    )\n\n    pooling_model = models.Pooling(\n        word_embedding_model.get_word_embedding_dimension(),\n    )\n\n    # Load pre-trained SBERT model\n    model = SentenceTransformer(\n        model_name,\n        device=device,\n        modules=[word_embedding_model, pooling_model],\n    )\n\n    # Ensure the model's max_seq_length does not exceed max_position_embeddings\n    model[0].max_seq_length = word_embedding_model.max_seq_length  # type: ignore\n    model[\n        1\n    ].word_embedding_dimension = word_embedding_model.get_word_embedding_dimension()  # type: ignore\n\n    print(model)\n\n    # Measure the time taken to generate embeddings for each column\n    start_time = time.time()\n    total_num_embeddings = 0\n    for col in synopsis_columns:\n        processed_col = f\"Processed_{col}\"\n        embeddings = get_sbert_embeddings(\n            df, model, batch_size, processed_col, model_name, device\n        )\n\n        # Save the embeddings for the current column\n        if embeddings.size &gt; 0:\n            save_path = os.path.join(\n                embeddings_save_dir, f\"embeddings_{col.replace(' ', '_')}.npy\"\n            )\n            np.save(save_path, embeddings)\n            total_num_embeddings += embeddings.shape[0]\n\n            # Clear memory\n            del embeddings\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        else:\n            print(f\"No embeddings generated for column: {col}\")\n\n    end_time = time.time()\n    embedding_generation_time = end_time - start_time\n\n    # Prepare evaluation data\n    additional_info: Dict[str, Any] = {\n        \"dataset_info\": {\n            \"num_samples\": len(df),\n            \"preprocessing\": \"text normalization\",\n            \"source\": [dataset_path],\n        },\n        \"model_info\": {\n            \"num_layers\": hf_model.config.num_hidden_layers,\n            \"hidden_size\": hf_model.config.hidden_size,\n            \"max_seq_length\": (\n                word_embedding_model.max_seq_length\n                if hasattr(word_embedding_model, \"max_seq_length\")\n                else None\n            ),\n        },\n        \"timing\": {\"embedding_generation_time\": embedding_generation_time},\n        \"type\": dataset_type,\n        \"device\": device,\n    }\n\n    # Save evaluation data\n    common.save_evaluation_data(\n        model_name=model_name,\n        batch_size=batch_size,\n        num_embeddings=total_num_embeddings,\n        additional_info=additional_info,\n    )\n</code></pre>"},{"location":"Sbert/#src.sbert.parse_args","title":"parse_args","text":"<pre><code>parse_args() -&gt; Namespace\n</code></pre> <p>Parse command-line arguments for SBERT embedding generation.</p> RETURNS DESCRIPTION <code>Namespace</code> <p>argparse.Namespace: Parsed arguments containing: model (str): Name or path of SBERT model to use type (str): Dataset type ('anime' or 'manga')</p> Source code in <code>src/sbert.py</code> <pre><code>def parse_args() -&gt; argparse.Namespace:\n    \"\"\"\n    Parse command-line arguments for SBERT embedding generation.\n\n    Returns:\n        argparse.Namespace: Parsed arguments containing:\n            model (str): Name or path of SBERT model to use\n            type (str): Dataset type ('anime' or 'manga')\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Generate SBERT embeddings for anime or manga dataset.\"\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        required=True,\n        help=\"The model name to use (e.g., 'all-mpnet-base-v1' or path to fine-tuned model).\",\n    )\n    parser.add_argument(\n        \"--type\",\n        type=str,\n        choices=[\"anime\", \"manga\"],\n        required=True,\n        help=\"Type of dataset to generate embeddings for: 'anime' or 'manga'.\",\n    )\n    return parser.parse_args()\n</code></pre>"},{"location":"Test/","title":"Test","text":"<p>Provides functionality for semantic similarity search in anime and manga datasets.</p> <p>This module handles loading pre-trained models and embeddings, calculating semantic similarities between descriptions, and saving evaluation results. It supports both anime and manga datasets and uses sentence transformers for embedding generation.</p> Key Features <ul> <li>Model and embedding loading with automatic device selection</li> <li>Batched similarity calculation using cosine similarity</li> <li>Deduplication of results based on titles</li> <li>Comprehensive evaluation result logging</li> <li>Support for multiple synopsis/description columns</li> </ul> <p>The module is designed to work with pre-computed embeddings stored in numpy arrays and uses efficient tensor operations for similarity calculations.</p> FUNCTION DESCRIPTION <code>load_model_and_embeddings</code> <p>Loads model, dataset and embeddings for similarity search</p> <code>calculate_similarities</code> <p>Computes semantic similarities between descriptions</p> <code>save_evaluation_results</code> <p>Logs evaluation results with timestamps and metadata</p>"},{"location":"Test/#src.test.calculate_similarities","title":"calculate_similarities","text":"<pre><code>calculate_similarities(model: SentenceTransformer, df: DataFrame, synopsis_columns: List[str], embeddings_save_dir: str, new_description: str, top_n: int = 10) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Find semantically similar titles by comparing embeddings.</p> <p>Calculates cosine similarities between a new description's embedding and pre-computed embeddings from the dataset. Returns the top-N most similar titles, removing duplicates across different synopsis columns.</p> PARAMETER DESCRIPTION <code>model</code> <p>Model to encode the new description</p> <p> TYPE: <code>SentenceTransformer</code> </p> <code>df</code> <p>Dataset containing titles and synopses</p> <p> TYPE: <code>DataFrame</code> </p> <code>synopsis_columns</code> <p>Columns containing synopsis text</p> <p> TYPE: <code>List[str]</code> </p> <code>embeddings_save_dir</code> <p>Directory containing pre-computed embeddings</p> <p> TYPE: <code>str</code> </p> <code>new_description</code> <p>Description to find similar titles for</p> <p> TYPE: <code>str</code> </p> <code>top_n</code> <p>Number of similar titles to return. Defaults to 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> RETURNS DESCRIPTION <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: Top similar titles, each containing: - rank: Position in results (1-based) - title: Title of the anime/manga - synopsis: Plot description/synopsis - similarity: Cosine similarity score - source_column: Column the synopsis came from</p> RAISES DESCRIPTION <code>ValueError</code> <p>If no valid embeddings are found in embeddings_save_dir</p> Source code in <code>src/test.py</code> <pre><code>def calculate_similarities(\n    model: SentenceTransformer,\n    df: pd.DataFrame,\n    synopsis_columns: List[str],\n    embeddings_save_dir: str,\n    new_description: str,\n    top_n: int = 10,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Find semantically similar titles by comparing embeddings.\n\n    Calculates cosine similarities between a new description's embedding and\n    pre-computed embeddings from the dataset. Returns the top-N most similar\n    titles, removing duplicates across different synopsis columns.\n\n    Args:\n        model (SentenceTransformer): Model to encode the new description\n        df (pd.DataFrame): Dataset containing titles and synopses\n        synopsis_columns (List[str]): Columns containing synopsis text\n        embeddings_save_dir (str): Directory containing pre-computed embeddings\n        new_description (str): Description to find similar titles for\n        top_n (int, optional): Number of similar titles to return. Defaults to 10.\n\n    Returns:\n        List[Dict[str, Any]]: Top similar titles, each containing:\n            - rank: Position in results (1-based)\n            - title: Title of the anime/manga\n            - synopsis: Plot description/synopsis\n            - similarity: Cosine similarity score\n            - source_column: Column the synopsis came from\n\n    Raises:\n        ValueError: If no valid embeddings are found in embeddings_save_dir\n    \"\"\"\n    processed_description = common.preprocess_text(new_description)\n    new_pooled_embedding = model.encode(\n        [processed_description], convert_to_tensor=True, device=\"cpu\"\n    )\n\n    cosine_similarities_dict = {}\n    for col in synopsis_columns:\n        embeddings_file = os.path.join(\n            embeddings_save_dir, f\"embeddings_{col.replace(' ', '_')}.npy\"\n        )\n        if not os.path.exists(embeddings_file):\n            print(f\"Embeddings file not found for column '{col}': {embeddings_file}\")\n            continue\n\n        existing_embeddings = np.load(embeddings_file)\n        existing_embeddings_tensor = torch.tensor(existing_embeddings).to(\"cpu\")\n\n        with torch.no_grad():\n            cosine_similarities = (\n                util.pytorch_cos_sim(new_pooled_embedding, existing_embeddings_tensor)\n                .squeeze(0)\n                .cpu()\n                .numpy()\n            )\n\n        cosine_similarities_dict[col] = cosine_similarities\n\n    if not cosine_similarities_dict:\n        raise ValueError(\n            \"No valid embeddings were loaded. Please check your embeddings directory and files.\"\n        )\n\n    all_top_indices = []\n    for col, cosine_similarities in cosine_similarities_dict.items():\n        top_indices_unsorted = np.argsort(cosine_similarities)[-top_n:]\n        top_indices = top_indices_unsorted[\n            np.argsort(cosine_similarities[top_indices_unsorted])[::-1]\n        ]\n        all_top_indices.extend([(idx, col) for idx in top_indices])\n\n    all_top_indices.sort(\n        key=lambda x: cosine_similarities_dict[x[1]][x[0]], reverse=True\n    )\n\n    seen_names = set()\n    top_results: List[Dict[str, Any]] = []\n    for idx, col in all_top_indices:\n        if len(top_results) &gt;= top_n:\n            break\n        name = df.iloc[idx][\"title\"]\n        if name in seen_names:\n            continue\n        synopsis = df.iloc[idx][col]\n        similarity = cosine_similarities_dict[col][idx]\n        top_results.append(\n            {\n                \"rank\": len(top_results) + 1,\n                \"title\": name,\n                \"synopsis\": synopsis,\n                \"similarity\": float(similarity),\n                \"source_column\": col,\n            }\n        )\n        seen_names.add(name)\n\n    return top_results\n</code></pre>"},{"location":"Test/#src.test.load_model_and_embeddings","title":"load_model_and_embeddings","text":"<pre><code>load_model_and_embeddings(model_name: str, dataset_type: str) -&gt; Tuple[SentenceTransformer, DataFrame, List[str], str]\n</code></pre> <p>Load the model, dataset and pre-computed embeddings for similarity search.</p> <p>Handles loading of the appropriate sentence transformer model, dataset and pre-computed embeddings based on the specified dataset type. Supports both anime and manga datasets with their respective synopsis columns.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the sentence transformer model to load. Will prepend 'sentence-transformers/' if not already present.</p> <p> TYPE: <code>str</code> </p> <code>dataset_type</code> <p>Type of dataset to load ('anime' or 'manga'). Determines which dataset and embeddings to load.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple</code> <ul> <li>SentenceTransformer: Loaded model instance</li> <li>pd.DataFrame: Dataset containing titles and synopses</li> <li>List[str]: Names of synopsis columns in the dataset</li> <li>str: Directory path containing pre-computed embeddings</li> </ul> <p> TYPE: <code>Tuple[SentenceTransformer, DataFrame, List[str], str]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If dataset_type is not 'anime' or 'manga'</p> Source code in <code>src/test.py</code> <pre><code>def load_model_and_embeddings(\n    model_name: str, dataset_type: str\n) -&gt; Tuple[SentenceTransformer, pd.DataFrame, List[str], str]:\n    \"\"\"\n    Load the model, dataset and pre-computed embeddings for similarity search.\n\n    Handles loading of the appropriate sentence transformer model, dataset and\n    pre-computed embeddings based on the specified dataset type. Supports both\n    anime and manga datasets with their respective synopsis columns.\n\n    Args:\n        model_name (str): Name of the sentence transformer model to load.\n            Will prepend 'sentence-transformers/' if not already present.\n        dataset_type (str): Type of dataset to load ('anime' or 'manga').\n            Determines which dataset and embeddings to load.\n\n    Returns:\n        tuple:\n            - SentenceTransformer: Loaded model instance\n            - pd.DataFrame: Dataset containing titles and synopses\n            - List[str]: Names of synopsis columns in the dataset\n            - str: Directory path containing pre-computed embeddings\n\n    Raises:\n        ValueError: If dataset_type is not 'anime' or 'manga'\n    \"\"\"\n    if not model_name.startswith(\"sentence-transformers/\"):\n        model_name = f\"sentence-transformers/{model_name}\"\n\n    if dataset_type == \"anime\":\n        dataset_path = \"model/merged_anime_dataset.csv\"\n        synopsis_columns = [\n            \"synopsis\",\n            \"Synopsis anime_dataset_2023\",\n            \"Synopsis animes dataset\",\n            \"Synopsis anime_270 Dataset\",\n            \"Synopsis Anime-2022 Dataset\",\n            \"Synopsis anime4500 Dataset\",\n            \"Synopsis wykonos Dataset\",\n            \"Synopsis Anime_data Dataset\",\n            \"Synopsis anime2 Dataset\",\n            \"Synopsis mal_anime Dataset\",\n        ]\n        embeddings_save_dir = f\"model/anime/{model_name.split('/')[-1]}\"\n    elif dataset_type == \"manga\":\n        dataset_path = \"model/merged_manga_dataset.csv\"\n        synopsis_columns = [\n            \"synopsis\",\n            \"Synopsis jikan Dataset\",\n            \"Synopsis data Dataset\",\n        ]\n        embeddings_save_dir = f\"model/manga/{model_name.split('/')[-1]}\"\n    else:\n        raise ValueError(\"Invalid dataset type specified. Use 'anime' or 'manga'.\")\n\n    df = common.load_dataset(dataset_path)\n    model = SentenceTransformer(model_name, device=\"cpu\")\n    return model, df, synopsis_columns, embeddings_save_dir\n</code></pre>"},{"location":"Test/#src.test.save_evaluation_results","title":"save_evaluation_results","text":"<pre><code>save_evaluation_results(evaluation_file: str, model_name: str, dataset_type: str, new_description: str, top_results: List[Dict[str, Any]]) -&gt; str\n</code></pre> <p>Save similarity search results with metadata for evaluation.</p> <p>Appends the search results and metadata to a JSON file for later analysis. Creates a new file if it doesn't exist. Each entry includes a timestamp, model information, dataset type, query description, and similarity results.</p> PARAMETER DESCRIPTION <code>evaluation_file</code> <p>Path to save/append results</p> <p> TYPE: <code>str</code> </p> <code>model_name</code> <p>Name of model used for embeddings</p> <p> TYPE: <code>str</code> </p> <code>dataset_type</code> <p>Type of dataset searched ('anime' or 'manga')</p> <p> TYPE: <code>str</code> </p> <code>new_description</code> <p>Query description used for search</p> <p> TYPE: <code>str</code> </p> <code>top_results</code> <p>Similarity search results</p> <p> TYPE: <code>List[Dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Path to the evaluation file</p> <p> TYPE: <code>str</code> </p> The saved JSON structure includes <ul> <li>timestamp: When the search was performed</li> <li>model_name: Model used for embeddings</li> <li>dataset_type: Type of dataset searched</li> <li>new_description: Query description</li> <li>top_similarities: List of similar titles and their scores</li> </ul> Source code in <code>src/test.py</code> <pre><code>def save_evaluation_results(\n    evaluation_file: str,\n    model_name: str,\n    dataset_type: str,\n    new_description: str,\n    top_results: List[Dict[str, Any]],\n) -&gt; str:\n    \"\"\"\n    Save similarity search results with metadata for evaluation.\n\n    Appends the search results and metadata to a JSON file for later analysis.\n    Creates a new file if it doesn't exist. Each entry includes a timestamp,\n    model information, dataset type, query description, and similarity results.\n\n    Args:\n        evaluation_file (str): Path to save/append results\n        model_name (str): Name of model used for embeddings\n        dataset_type (str): Type of dataset searched ('anime' or 'manga')\n        new_description (str): Query description used for search\n        top_results (List[Dict[str, Any]]): Similarity search results\n\n    Returns:\n        str: Path to the evaluation file\n\n    The saved JSON structure includes:\n        - timestamp: When the search was performed\n        - model_name: Model used for embeddings\n        - dataset_type: Type of dataset searched\n        - new_description: Query description\n        - top_similarities: List of similar titles and their scores\n    \"\"\"\n    if os.path.exists(evaluation_file):\n        with open(evaluation_file, \"r\", encoding=\"utf-8\") as f:\n            try:\n                evaluation_data = json.load(f)\n            except json.JSONDecodeError:\n                evaluation_data = []\n    else:\n        evaluation_data = []\n\n    test_result = {\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"model_name\": model_name,\n        \"dataset_type\": dataset_type,\n        \"new_description\": new_description,\n        \"top_similarities\": top_results,\n    }\n\n    evaluation_data.append(test_result)\n\n    with open(evaluation_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(evaluation_data, f, indent=4)\n\n    return evaluation_file\n</code></pre>"},{"location":"Train/","title":"Train","text":"<p>Training module for fine-tuning SentenceTransformer models on anime/manga synopsis data.</p> <p>This module provides functionality for training sentence transformer models to understand semantic similarities between anime/manga synopses. It handles the complete training pipeline, including:</p> <ol> <li> <p>Data Processing:</p> <ul> <li>Loading anime/manga datasets</li> <li>Managing genres and themes</li> <li>Generating embeddings for categories</li> </ul> </li> <li> <p>Pair Generation:</p> <ul> <li>Positive pairs: Same-entry synopses with high similarity</li> <li>Partial positive pairs: Different-entry synopses with moderate similarity</li> <li>Negative pairs: Different-entry synopses with low similarity</li> </ul> </li> <li> <p>Model Training:</p> <ul> <li>Fine-tuning pre-trained sentence transformers</li> <li>Custom loss function support (cosine, cosent, angle)</li> <li>Validation and evaluation during training</li> <li>Checkpoint saving and model persistence</li> </ul> </li> <li> <p>Resource Management:</p> <ul> <li>GPU memory management with garbage collection</li> <li>Multiprocessing for pair generation</li> <li>Efficient data loading with DataLoader</li> </ul> </li> </ol> <p>Usage: <pre><code>python train.py [arguments]\n</code></pre></p> <p>For full list of arguments, use: python train.py --help</p> Notes <ul> <li>Supports resuming training from saved pair files</li> <li>Uses cosine similarity for evaluation</li> <li>Handles both anime and manga datasets with specific genre/theme sets</li> <li>Custom transformer option available for modified architectures</li> </ul>"},{"location":"Train/#src.train.create_pairs","title":"create_pairs","text":"<pre><code>create_pairs(df: DataFrame, max_negative_per_row: int, max_partial_positive_per_row: int, category_to_embedding: Dict[str, NDArray[float64]], partial_threshold: float = 0.5, positive_pairs_file: Optional[str] = None, partial_positive_pairs_file: Optional[str] = None, negative_pairs_file: Optional[str] = None, use_saved_pairs: bool = False, num_workers: int = cpu_count() // 4) -&gt; Tuple[List[InputExample], List[InputExample], List[InputExample]]\n</code></pre> <p>Create positive, partial positive, and negative pairs from the dataframe.</p> <p>This function handles the generation of three types of synopsis pairs:</p> <ol> <li> <p>Positive pairs: From same entries with high similarity</p> </li> <li> <p>Partial positive pairs: From different entries with moderate similarity</p> </li> <li> <p>Negative pairs: From different entries with low similarity</p> </li> </ol> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing anime/manga data with synopses</p> <p> TYPE: <code>DataFrame</code> </p> <code>max_negative_per_row</code> <p>Maximum negative pairs to generate per entry</p> <p> TYPE: <code>int</code> </p> <code>max_partial_positive_per_row</code> <p>Maximum partial positive pairs per entry</p> <p> TYPE: <code>int</code> </p> <code>category_to_embedding</code> <p>Dictionary mapping categories to their vector embeddings</p> <p> TYPE: <code>Dict[str, NDArray[float64]]</code> </p> <code>partial_threshold</code> <p>Similarity threshold for partial positive pairs (default: 0.5)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>positive_pairs_file</code> <p>Optional path to save/load positive pairs</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>partial_positive_pairs_file</code> <p>Optional path to save/load partial positive pairs</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>negative_pairs_file</code> <p>Optional path to save/load negative pairs</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>use_saved_pairs</code> <p>Whether to load existing pairs if available</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_workers</code> <p>Number of parallel workers for pair generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>cpu_count() // 4</code> </p> RETURNS DESCRIPTION <code>List[InputExample]</code> <p>Tuple containing:</p> <code>List[InputExample]</code> <ul> <li>List[InputExample]: Positive pairs</li> </ul> <code>List[InputExample]</code> <ul> <li>List[InputExample]: Partial positive pairs</li> </ul> <code>Tuple[List[InputExample], List[InputExample], List[InputExample]]</code> <ul> <li>List[InputExample]: Negative pairs</li> </ul> Notes <ul> <li>Uses sentence-t5-xl model for encoding during pair generation</li> <li>Performs garbage collection after each pair type generation</li> <li>Saves generated pairs to files if paths are provided</li> </ul> Source code in <code>src/train.py</code> <pre><code>def create_pairs(\n    df: pd.DataFrame,\n    max_negative_per_row: int,\n    max_partial_positive_per_row: int,\n    category_to_embedding: Dict[str, NDArray[np.float64]],\n    partial_threshold: float = 0.5,\n    positive_pairs_file: Optional[str] = None,\n    partial_positive_pairs_file: Optional[str] = None,\n    negative_pairs_file: Optional[str] = None,\n    use_saved_pairs: bool = False,\n    num_workers: int = cpu_count() // 4,\n) -&gt; Tuple[List[InputExample], List[InputExample], List[InputExample]]:\n    \"\"\"\n    Create positive, partial positive, and negative pairs from the dataframe.\n\n    This function handles the generation of three types of synopsis pairs:\n\n    1. Positive pairs: From same entries with high similarity\n\n    2. Partial positive pairs: From different entries with moderate similarity\n\n    3. Negative pairs: From different entries with low similarity\n\n    Args:\n        df: DataFrame containing anime/manga data with synopses\n        max_negative_per_row: Maximum negative pairs to generate per entry\n        max_partial_positive_per_row: Maximum partial positive pairs per entry\n        category_to_embedding: Dictionary mapping categories to their vector embeddings\n        partial_threshold: Similarity threshold for partial positive pairs (default: 0.5)\n        positive_pairs_file: Optional path to save/load positive pairs\n        partial_positive_pairs_file: Optional path to save/load partial positive pairs\n        negative_pairs_file: Optional path to save/load negative pairs\n        use_saved_pairs: Whether to load existing pairs if available\n        num_workers: Number of parallel workers for pair generation\n\n    Returns:\n        Tuple containing:\n        - List[InputExample]: Positive pairs\n        - List[InputExample]: Partial positive pairs\n        - List[InputExample]: Negative pairs\n\n    Notes:\n        - Uses sentence-t5-xl model for encoding during pair generation\n        - Performs garbage collection after each pair type generation\n        - Saves generated pairs to files if paths are provided\n    \"\"\"\n    synopses_columns: List[str] = [\n        col for col in df.columns if \"synopsis\" in col.lower()\n    ]\n\n    # Load a pre-trained Sentence Transformer model for encoding\n    encoder_model: SentenceTransformer = SentenceTransformer(\"sentence-t5-xl\")\n    logger.debug(\"Loaded encoder model: %s\", encoder_model)  # type: ignore\n\n    positive_pairs: List[InputExample] = []\n    if (\n        positive_pairs_file is None\n        or not os.path.exists(positive_pairs_file)\n        or not use_saved_pairs\n    ):\n        logger.info(\"Creating positive pairs.\")  # type: ignore\n        positive_pairs = create_positive_pairs(\n            df, synopses_columns, encoder_model, positive_pairs_file\n        )\n        logger.debug(\"Generated %d positive pairs.\", len(positive_pairs))  # type: ignore\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    partial_positive_pairs: List[InputExample] = []\n    if (\n        partial_positive_pairs_file is None\n        or not os.path.exists(partial_positive_pairs_file)\n        or not use_saved_pairs\n    ):\n        logger.info(\"Creating partial positive pairs.\")  # type: ignore\n        partial_positive_pairs = create_partial_positive_pairs(\n            df,\n            synopses_columns,\n            partial_threshold,\n            max_partial_positive_per_row,\n            partial_positive_pairs_file,\n            num_workers,\n            category_to_embedding,\n        )\n        logger.debug(\n            \"Generated %d partial positive pairs.\",\n            len(partial_positive_pairs),  # type: ignore\n        )\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    negative_pairs: List[InputExample] = []\n    if (\n        negative_pairs_file is None\n        or not os.path.exists(negative_pairs_file)\n        or not use_saved_pairs\n    ):\n        logger.info(\"Creating negative pairs.\")  # type: ignore\n        negative_pairs = create_negative_pairs(\n            df,\n            synopses_columns,\n            partial_threshold,\n            max_negative_per_row,\n            negative_pairs_file,\n            num_workers,\n            category_to_embedding,\n        )\n        logger.debug(\"Generated %d negative pairs.\", len(negative_pairs))  # type: ignore\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    return positive_pairs, partial_positive_pairs, negative_pairs\n</code></pre>"},{"location":"Train/#src.train.get_pairs","title":"get_pairs","text":"<pre><code>get_pairs(df: DataFrame, use_saved_pairs: bool, saved_pairs_directory: str, max_negative_per_row: int, max_partial_positive_per_row: int, num_workers: int, data_type: str, category_to_embedding: Dict[str, NDArray[float64]]) -&gt; List[InputExample]\n</code></pre> <p>Retrieve or generate all training pairs for model training.</p> <p>This function handles loading existing pairs from files or generating new ones as needed. It manages three types of pairs: positive, partial positive, and negative pairs.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing the anime/manga data</p> <p> TYPE: <code>DataFrame</code> </p> <code>use_saved_pairs</code> <p>Whether to attempt loading existing pairs</p> <p> TYPE: <code>bool</code> </p> <code>saved_pairs_directory</code> <p>Base directory for saved pair files</p> <p> TYPE: <code>str</code> </p> <code>max_negative_per_row</code> <p>Maximum negative pairs per entry</p> <p> TYPE: <code>int</code> </p> <code>max_partial_positive_per_row</code> <p>Maximum partial positive pairs per entry</p> <p> TYPE: <code>int</code> </p> <code>num_workers</code> <p>Number of parallel workers for generation</p> <p> TYPE: <code>int</code> </p> <code>data_type</code> <p>Type of data ('anime' or 'manga')</p> <p> TYPE: <code>str</code> </p> <code>category_to_embedding</code> <p>Dictionary mapping categories to embeddings</p> <p> TYPE: <code>Dict[str, NDArray[float64]]</code> </p> RETURNS DESCRIPTION <code>List[InputExample]</code> <p>List[InputExample]: Combined list of all pair types for training</p> Notes <ul> <li>Automatically creates directory structure for pair files</li> <li>Falls back to generation if loading fails or files missing</li> <li>Combines all pair types into a single training set</li> <li>Maintains consistent file naming based on data_type</li> </ul> Source code in <code>src/train.py</code> <pre><code>def get_pairs(\n    df: pd.DataFrame,\n    use_saved_pairs: bool,\n    saved_pairs_directory: str,\n    max_negative_per_row: int,\n    max_partial_positive_per_row: int,\n    num_workers: int,\n    data_type: str,\n    category_to_embedding: Dict[str, NDArray[np.float64]],\n) -&gt; List[InputExample]:\n    \"\"\"\n    Retrieve or generate all training pairs for model training.\n\n    This function handles loading existing pairs from files or generating new ones\n    as needed. It manages three types of pairs: positive, partial positive, and\n    negative pairs.\n\n    Args:\n        df: DataFrame containing the anime/manga data\n        use_saved_pairs: Whether to attempt loading existing pairs\n        saved_pairs_directory: Base directory for saved pair files\n        max_negative_per_row: Maximum negative pairs per entry\n        max_partial_positive_per_row: Maximum partial positive pairs per entry\n        num_workers: Number of parallel workers for generation\n        data_type: Type of data ('anime' or 'manga')\n        category_to_embedding: Dictionary mapping categories to embeddings\n\n    Returns:\n        List[InputExample]: Combined list of all pair types for training\n\n    Notes:\n        - Automatically creates directory structure for pair files\n        - Falls back to generation if loading fails or files missing\n        - Combines all pair types into a single training set\n        - Maintains consistent file naming based on data_type\n    \"\"\"\n    positive_pairs: List[InputExample] = []\n    partial_positive_pairs: List[InputExample] = []\n    negative_pairs: List[InputExample] = []\n\n    # Define file paths\n    positive_pairs_file: str = os.path.join(\n        saved_pairs_directory, \"pairs\", data_type, \"positive_pairs.csv\"\n    )\n    partial_positive_pairs_file: str = os.path.join(\n        saved_pairs_directory, \"pairs\", data_type, \"partial_positive_pairs.csv\"\n    )\n    negative_pairs_file: str = os.path.join(\n        saved_pairs_directory, \"pairs\", data_type, \"negative_pairs.csv\"\n    )\n\n    if use_saved_pairs:\n        if os.path.exists(positive_pairs_file):\n            logger.info(\"Loading positive pairs from %s\", positive_pairs_file)  # type: ignore\n            positive_pairs_df: pd.DataFrame = pd.read_csv(positive_pairs_file)\n            positive_pairs = [\n                InputExample(texts=[row[\"text_a\"], row[\"text_b\"]], label=row[\"label\"])\n                for _, row in positive_pairs_df.iterrows()\n            ]\n            logger.debug(\"Loaded %d positive pairs from file.\", len(positive_pairs))  # type: ignore\n\n        if os.path.exists(partial_positive_pairs_file):\n            logger.info(\n                \"Loading partial positive pairs from %s\", partial_positive_pairs_file\n            )  # type: ignore\n            partial_positive_pairs_df: pd.DataFrame = pd.read_csv(\n                partial_positive_pairs_file\n            )\n            partial_positive_pairs = [\n                InputExample(texts=[row[\"text_a\"], row[\"text_b\"]], label=row[\"label\"])\n                for _, row in partial_positive_pairs_df.iterrows()\n            ]\n            logger.debug(\n                \"Loaded %d partial positive pairs from file.\",\n                len(partial_positive_pairs),\n            )  # type: ignore\n\n        if os.path.exists(negative_pairs_file):\n            logger.info(\"Loading negative pairs from %s\", negative_pairs_file)  # type: ignore\n            negative_pairs_df: pd.DataFrame = pd.read_csv(negative_pairs_file)\n            negative_pairs = [\n                InputExample(texts=[row[\"text_a\"], row[\"text_b\"]], label=row[\"label\"])\n                for _, row in negative_pairs_df.iterrows()\n            ]\n            logger.debug(\"Loaded %d negative pairs from file.\", len(negative_pairs))  # type: ignore\n\n    if (\n        not use_saved_pairs\n        or not positive_pairs\n        or not partial_positive_pairs\n        or not negative_pairs\n    ):\n        logger.info(\"Generating pairs as some or all pair types are missing.\")  # type: ignore\n        (\n            generated_positive_pairs,\n            generated_partial_positive_pairs,\n            generated_negative_pairs,\n        ) = create_pairs(\n            df,\n            max_negative_per_row=max_negative_per_row,\n            max_partial_positive_per_row=max_partial_positive_per_row,\n            category_to_embedding=category_to_embedding,\n            partial_threshold=0.5,\n            positive_pairs_file=positive_pairs_file,\n            partial_positive_pairs_file=partial_positive_pairs_file,\n            negative_pairs_file=negative_pairs_file,\n            use_saved_pairs=use_saved_pairs,\n            num_workers=num_workers,\n        )\n\n        if not positive_pairs:\n            positive_pairs = generated_positive_pairs\n            logger.debug(\"Assigned generated positive pairs.\")  # type: ignore\n\n        if not partial_positive_pairs:\n            partial_positive_pairs = generated_partial_positive_pairs\n            logger.debug(\"Assigned generated partial positive pairs.\")  # type: ignore\n\n        if not negative_pairs:\n            negative_pairs = generated_negative_pairs\n            logger.debug(\"Assigned generated negative pairs.\")  # type: ignore\n\n    total_pairs = (\n        len(positive_pairs) + len(partial_positive_pairs) + len(negative_pairs)\n    )\n    logger.info(\"Total pairs prepared for training: %d\", total_pairs)  # type: ignore\n    return positive_pairs + partial_positive_pairs + negative_pairs\n</code></pre>"},{"location":"Train/#src.train.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Main training function for fine-tuning SentenceTransformer models.</p> <p>This function: 1. Parses command line arguments for training configuration 2. Sets up model paths and data loading 3. Generates or loads training pairs 4. Initializes and configures the model 5. Sets up training parameters and loss functions 6. Executes the training loop with early stopping 7. Saves the final model</p> <p>Command line arguments control all aspects of training including: - Model selection and architecture - Training hyperparameters - Data processing settings - Resource allocation - Input/output paths</p> <p>The function handles the complete training pipeline from data preparation through model saving, with appropriate logging and error handling.</p> Source code in <code>src/train.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"\n    Main training function for fine-tuning SentenceTransformer models.\n\n    This function:\n    1. Parses command line arguments for training configuration\n    2. Sets up model paths and data loading\n    3. Generates or loads training pairs\n    4. Initializes and configures the model\n    5. Sets up training parameters and loss functions\n    6. Executes the training loop with early stopping\n    7. Saves the final model\n\n    Command line arguments control all aspects of training including:\n    - Model selection and architecture\n    - Training hyperparameters\n    - Data processing settings\n    - Resource allocation\n    - Input/output paths\n\n    The function handles the complete training pipeline from data preparation\n    through model saving, with appropriate logging and error handling.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Train a SentenceTransformer model.\")\n    parser.add_argument(\n        \"--model_name\",\n        type=str,\n        default=\"sentence-t5-base\",\n        help=\"Name of the model to train. Default is 'sentence-t5-base'.\",\n    )\n    parser.add_argument(\n        \"--use_saved_pairs\",\n        action=\"store_true\",\n        help=\"Whether to use saved pairs. Default is False.\",\n    )\n    parser.add_argument(\n        \"--saved_pairs_directory\",\n        type=str,\n        default=\"model\",\n        help=\"Directory to save/load pairs. Default is 'model'.\",\n    )\n    parser.add_argument(\n        \"--max_negative_per_row\",\n        type=int,\n        default=5,\n        help=\"Maximum number of negative pairs to generate per row. Default is 5.\",\n    )\n    parser.add_argument(\n        \"--max_partial_positive_per_row\",\n        type=int,\n        default=5,\n        help=\"Maximum number of partial positive pairs to generate per row. Default is 5.\",\n    )\n    parser.add_argument(\n        \"--output_model_path\",\n        type=str,\n        default=\"model/fine_tuned_sbert_model\",\n        help=\"Path to save the fine-tuned model. Default is 'model/fine_tuned_sbert_model'.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=2e-5,\n        help=\"Learning rate for the optimizer. Default is 2e-5 (0.00002).\",\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        default=3,\n        help=\"Batch size for training. Default is 3.\",\n    )\n    parser.add_argument(\n        \"--evaluations_per_epoch\",\n        type=int,\n        default=20,\n        help=\"Number of evaluations per epoch. Default is 20.\",\n    )\n    parser.add_argument(\n        \"--epochs\",\n        type=int,\n        default=2,\n        help=\"Number of epochs for training. Default is 2.\",\n    )\n    parser.add_argument(\n        \"--loss_function\",\n        type=str,\n        choices=[\"cosine\", \"cosent\", \"angle\"],\n        default=\"cosine\",\n        help=\"Loss function to use: 'cosine', 'cosent', or 'angle'. Default is 'cosine'.\",\n    )\n    parser.add_argument(\n        \"--num_workers\",\n        type=int,\n        default=cpu_count() // 4,\n        help=\"Number of workers for multiprocessing. Default is (cpu_count() // 4).\",\n    )\n    parser.add_argument(\n        \"--data_type\",\n        type=str,\n        choices=[\"anime\", \"manga\"],\n        default=\"anime\",\n        help=\"Type of data to train on: 'anime' or 'manga'. Default is 'anime'.\",\n    )\n    parser.add_argument(\n        \"--use_custom_transformer\",\n        action=\"store_true\",\n        help=\"Whether to use the custom transformer with GELU activation.\",\n    )\n    parser.add_argument(\n        \"--early_stopping_patience\",\n        type=int,\n        default=3,\n        help=(\n            \"Number of evaluations with no improvement after which training will be \"\n            \"stopped. Default is 3.\"\n        ),\n    )\n    parser.add_argument(\n        \"--early_stopping_min_delta\",\n        type=float,\n        default=0.0,\n        help=\"Minimum change in the monitored metric to qualify as an improvement. Default is 0.0.\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        help=\"Device to use for training: 'cuda', 'cpu', or specific GPU indices (e.g., 'cuda:0').\",\n    )\n    parser.add_argument(\n        \"--num_gpus\",\n        type=int,\n        default=1,\n        help=\"Number of GPUs to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--logging_level\",\n        type=str,\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Logging level for the training script. Default is 'INFO'.\",\n    )\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=500,\n        help=\"Number of steps between saving model checkpoints. Default is 500.\",\n    )\n    parser.add_argument(\n        \"--checkpoint_dir\",\n        type=str,\n        default=\"checkpoints\",\n        help=\"Directory to save model checkpoints. Default is 'checkpoints'.\",\n    )\n    parser.add_argument(\n        \"--scheduler_type\",\n        type=str,\n        default=\"warmuplinear\",\n        choices=[\n            \"constant\",\n            \"warmupconstant\",\n            \"warmuplinear\",\n            \"warmupcosine\",\n            \"warmupcosinewithhardrestarts\",\n        ],\n        help=\"Type of learning rate scheduler to use. Default is 'warmuplinear'.\",\n    )\n    parser.add_argument(\n        \"--warmup_ratio\",\n        type=float,\n        default=0.1,\n        help=\"Proportion of total training steps to use for warmup. Default is 0.1 (10%).\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=42,\n        help=\"Random seed for reproducibility. Default is 42.\",\n    )\n    parser.add_argument(\n        \"--overwrite_output_dir\",\n        action=\"store_true\",\n        help=\"Overwrite the content of the output directory if it exists. Default is False.\",\n    )\n    parser.add_argument(\n        \"--checkpoint_save_total_limit\",\n        type=int,\n        default=5,\n        help=\"Maximum number of checkpoints to save. Default is 5.\",\n    )\n    parser.add_argument(\n        \"--weight_decay\",\n        type=float,\n        default=0.01,\n        help=\"Weight decay for optimizer. Default is 0.01.\",\n    )\n\n    args = parser.parse_args()\n\n    global logger  # pylint: disable=global-statement global-variable-undefined\n    logger = logging.getLogger(\"train\")\n    logger.setLevel(getattr(logging, args.logging_level.upper(), logging.INFO))\n\n    # Create handler (StreamHandler for console output)\n    handler = logging.StreamHandler()\n    handler.setLevel(getattr(logging, args.logging_level.upper(), logging.INFO))\n\n    # Create formatter and add it to the handler\n    formatter = logging.Formatter(\n        fmt=\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n    )\n    handler.setFormatter(formatter)\n\n    # Add handler to the logger\n    if not logger.handlers:\n        logger.addHandler(handler)\n\n    logger.debug(\"Argument parser initialized with arguments: %s\", args)\n\n    # Handle output_model_path based on data_type\n    output_model_path: str = args.output_model_path\n    if \"anime\" in args.output_model_path and args.data_type == \"manga\":\n        output_model_path = args.output_model_path.replace(\"anime\", \"manga\")\n    elif \"manga\" in args.output_model_path and args.data_type == \"anime\":\n        output_model_path = args.output_model_path.replace(\"manga\", \"anime\")\n    elif args.data_type not in args.output_model_path.lower():\n        output_model_path = f\"{output_model_path}_{args.data_type}\"\n\n    args.output_model_path = output_model_path\n    logger.debug(\"Final output_model_path set to: %s\", args.output_model_path)\n\n    # Ensure checkpoint directory exists\n    os.makedirs(args.checkpoint_dir, exist_ok=True)\n    logger.debug(\"Checkpoint directory ensured at: %s\", args.checkpoint_dir)\n\n    # Set random seed for reproducibility\n    set_seed(args.seed)\n\n    # Set up device\n    device = args.device\n    if isinstance(device, str):\n        device = torch.device(device)\n    logger.info(\"Using device: %s\", device)\n\n    # Initialize Early Stopping Callback\n    early_stopping = EarlyStoppingCallback(\n        patience=args.early_stopping_patience, min_delta=args.early_stopping_min_delta\n    )\n    logger.debug(\n        \"EarlyStoppingCallback initialized with patience=%d, min_delta=%.2f\",\n        args.early_stopping_patience,\n        args.early_stopping_min_delta,\n    )\n\n    # Load genres and themes\n    logger.info(\"Loading genres and themes for data_type: %s\", args.data_type)\n    all_genres, all_themes = get_genres_and_themes(args.data_type)\n    logger.debug(\"Loaded %d genres and %d themes.\", len(all_genres), len(all_themes))\n\n    # Load the SBERT model\n    model_path = args.model_name\n    if not model_path.startswith(\"toobi/anime\"):\n        model_path = \"sentence-transformers/\" + model_path\n    logger.info(\"Creating model from path: %s\", model_path)\n    model: SentenceTransformer | torch.nn.DataParallel = create_model(\n        model_path,\n        use_custom_transformer=True if args.use_custom_transformer else False,\n        max_seq_length=843,\n    )\n    logger.debug(\"Model created: %s\", model)\n\n    # Access the Transformer model\n    transformer = model._first_module().auto_model  # pylint: disable=protected-access\n    logger.debug(\"Accessed Transformer encoder: %s\", transformer.encoder)\n\n    model.to(device)\n    logger.info(\"Model moved to device: %s\", device)\n\n    # Multi-GPU Support\n    if torch.cuda.device_count() &gt; 1 and args.num_gpus &gt; 1:\n        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpus)))\n        logger.info(\"Using %d GPUs for training.\", args.num_gpus)\n    else:\n        logger.info(\"Using a single GPU or CPU for training.\")\n\n    # Prepare category embeddings\n    all_categories: list = list(all_genres) + list(all_themes)\n    logger.info(\"Encoding categories for embeddings.\")\n    category_embeddings = model.encode(all_categories, convert_to_tensor=False)\n    category_to_embedding = {\n        category: embedding\n        for category, embedding in zip(all_categories, category_embeddings)\n    }\n    logger.debug(\"Category embeddings created for %d categories.\", len(all_categories))\n\n    # Load dataset\n    dataset_path: str = f\"model/merged_{args.data_type}_dataset.csv\"\n    logger.info(\"Loading dataset from %s\", dataset_path)\n    if not os.path.exists(dataset_path):\n        logger.error(\"Dataset file does not exist at path: %s\", dataset_path)\n        raise FileNotFoundError(f\"Dataset file not found: {dataset_path}\")\n    df: pd.DataFrame = pd.read_csv(dataset_path)\n    logger.debug(\"Dataset loaded with %d records.\", len(df))\n\n    # Generate or load training pairs\n    logger.info(\"Preparing training pairs.\")\n    pairs: list = get_pairs(\n        df,\n        use_saved_pairs=args.use_saved_pairs,\n        saved_pairs_directory=args.saved_pairs_directory,\n        max_negative_per_row=args.max_negative_per_row,\n        max_partial_positive_per_row=args.max_partial_positive_per_row,\n        num_workers=args.num_workers,\n        data_type=args.data_type,\n        category_to_embedding=category_to_embedding,\n    )\n    logger.debug(\"Total training pairs prepared: %d\", len(pairs))\n\n    # Split the pairs into training and validation sets\n    logger.info(\"Splitting data into training and validation sets.\")\n    train_pairs, val_pairs = train_test_split(\n        pairs, test_size=0.1, random_state=args.seed\n    )\n    logger.debug(\n        \"Training pairs: %d, Validation pairs: %d\", len(train_pairs), len(val_pairs)\n    )\n\n    # Create the evaluator\n    logger.info(\"Creating evaluator for validation.\")\n    evaluator = create_evaluator(val_pairs)\n\n    # Create DataLoader\n    logger.info(\"Creating DataLoader for training.\")\n    train_dataloader: DataLoader = DataLoader(\n        train_pairs, shuffle=True, batch_size=args.batch_size\n    )\n    logger.debug(\"DataLoader created with batch size: %d\", args.batch_size)\n\n    # Calculate the number of batches per epoch\n    num_batches_per_epoch: int = len(train_dataloader)\n    logger.debug(\"Number of batches per epoch: %d\", num_batches_per_epoch)\n\n    # Calculate total training steps\n    total_steps = args.epochs * num_batches_per_epoch\n    logger.debug(\"Total training steps: %d\", total_steps)\n\n    # Calculate warmup steps\n    warmup_steps = int((args.warmup_ratio * total_steps) // args.epochs)\n    logger.debug(\"Warmup steps: %d\", warmup_steps)\n\n    # Get the loss function\n    train_loss = get_loss_function(args.loss_function, model)  # type: ignore\n    logger.debug(\"Loss function set to: %s\", args.loss_function)\n\n    # Start Training Loop\n    logger.info(\"Starting fine-tuning of the model.\")\n    for epoch in range(args.epochs):\n        logger.info(\"Epoch %d/%d\", epoch + 1, args.epochs)\n        model.fit(\n            train_objectives=[(train_dataloader, train_loss)],\n            evaluator=evaluator,\n            epochs=1,\n            evaluation_steps=max(\n                1, num_batches_per_epoch // args.evaluations_per_epoch\n            ),\n            output_path=args.output_model_path,\n            warmup_steps=warmup_steps,\n            scheduler=args.scheduler_type,\n            optimizer_params={\"lr\": args.learning_rate},\n            weight_decay=args.weight_decay,\n            checkpoint_save_steps=args.save_steps,\n            checkpoint_path=args.checkpoint_dir,\n            checkpoint_save_total_limit=args.checkpoint_save_total_limit,\n        )\n        logger.debug(\"Model.fit() completed for epoch %d.\", epoch + 1)\n\n        # Evaluate after each epoch\n        evaluation = evaluator(model)  # type: ignore\n        current_score = evaluation.get(\"eval_pearson_cosine\", 0)\n\n        # Early Stopping\n        early_stopping.on_evaluate(current_score, epoch, steps=num_batches_per_epoch)\n        if early_stopping.stop_training:\n            logger.info(\"Early stopping triggered. Stopping training.\")\n            break\n\n    # Save the final model\n    model.save(args.output_model_path)\n    logger.info(\"Final model saved at %s\", args.output_model_path)\n</code></pre>"},{"location":"Train/#src.train.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int)\n</code></pre> <p>Set the random seed for reproducibility.</p> Source code in <code>src/train.py</code> <pre><code>def set_seed(seed: int):\n    \"\"\"\n    Set the random seed for reproducibility.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    logger.debug(\"Random seed set to %d for reproducibility.\", seed)  # type: ignore\n</code></pre>"},{"location":"Misc/MaxTokens/","title":"MaxTokens","text":"<p>This module calculates the maximum token count for different transformer models across anime and manga datasets.</p> <p>The module processes multiple synopsis columns from anime and manga datasets, tokenizing the text using various transformer models to determine the maximum token length needed for each model. This information is useful for setting appropriate maximum sequence lengths when training or using these models.</p>"},{"location":"Misc/MaxTokens/#src.misc.max_tokens.anime_max_tokens","title":"anime_max_tokens  <code>module-attribute</code>","text":"<pre><code>anime_max_tokens = calculate_max_tokens('model/merged_anime_dataset.csv', anime_synopsis_columns, model_list)\n</code></pre>"},{"location":"Misc/MaxTokens/#src.misc.max_tokens.anime_synopsis_columns","title":"anime_synopsis_columns  <code>module-attribute</code>","text":"<pre><code>anime_synopsis_columns = ['synopsis', 'Synopsis anime_dataset_2023', 'Synopsis animes dataset', 'Synopsis anime_270 Dataset', 'Synopsis Anime-2022 Dataset', 'Synopsis anime4500 Dataset', 'Synopsis wykonos Dataset', 'Synopsis Anime_data Dataset', 'Synopsis anime2 Dataset', 'Synopsis mal_anime Dataset']\n</code></pre>"},{"location":"Misc/MaxTokens/#src.misc.max_tokens.manga_max_tokens","title":"manga_max_tokens  <code>module-attribute</code>","text":"<pre><code>manga_max_tokens = calculate_max_tokens('model/merged_manga_dataset.csv', manga_synopsis_columns, model_list)\n</code></pre>"},{"location":"Misc/MaxTokens/#src.misc.max_tokens.manga_synopsis_columns","title":"manga_synopsis_columns  <code>module-attribute</code>","text":"<pre><code>manga_synopsis_columns = ['synopsis', 'Synopsis jikan Dataset', 'Synopsis data Dataset']\n</code></pre>"},{"location":"Misc/MaxTokens/#src.misc.max_tokens.max_tokens_anime","title":"max_tokens_anime  <code>module-attribute</code>","text":"<pre><code>max_tokens_anime = max(values())\n</code></pre>"},{"location":"Misc/MaxTokens/#src.misc.max_tokens.max_tokens_manga","title":"max_tokens_manga  <code>module-attribute</code>","text":"<pre><code>max_tokens_manga = max(values())\n</code></pre>"},{"location":"Misc/MaxTokens/#src.misc.max_tokens.model_list","title":"model_list  <code>module-attribute</code>","text":"<pre><code>model_list = ['toobi/anime', 'sentence-transformers/all-distilroberta-v1', 'sentence-transformers/all-MiniLM-L6-v1', 'sentence-transformers/all-MiniLM-L12-v1', 'sentence-transformers/all-MiniLM-L6-v2', 'sentence-transformers/all-MiniLM-L12-v2', 'sentence-transformers/all-mpnet-base-v1', 'sentence-transformers/all-mpnet-base-v2', 'sentence-transformers/all-roberta-large-v1', 'sentence-transformers/gtr-t5-base', 'sentence-transformers/gtr-t5-large', 'sentence-transformers/gtr-t5-xl', 'sentence-transformers/multi-qa-distilbert-dot-v1', 'sentence-transformers/multi-qa-mpnet-base-cos-v1', 'sentence-transformers/multi-qa-mpnet-base-dot-v1', 'sentence-transformers/paraphrase-distilroberta-base-v2', 'sentence-transformers/paraphrase-mpnet-base-v2', 'sentence-transformers/sentence-t5-base', 'sentence-transformers/sentence-t5-large', 'sentence-transformers/sentence-t5-xl', 'sentence-transformers/sentence-t5-xxl']\n</code></pre>"},{"location":"Misc/MaxTokens/#src.misc.max_tokens.calculate_max_tokens","title":"calculate_max_tokens","text":"<pre><code>calculate_max_tokens(dataset_path: str, synopsis_columns: List[str], model_names: List[str], batch_size: int = 64) -&gt; Dict[str, int]\n</code></pre> <p>Calculate the maximum token count for each model across specified synopsis columns in a dataset.</p> PARAMETER DESCRIPTION <code>dataset_path</code> <p>Path to the CSV dataset file.</p> <p> TYPE: <code>str</code> </p> <code>synopsis_columns</code> <p>List of column names containing synopsis text to analyze.</p> <p> TYPE: <code>list</code> </p> <code>model_names</code> <p>List of model names/paths to test for tokenization.</p> <p> TYPE: <code>list</code> </p> <code>batch_size</code> <p>Batch size for processing. Defaults to 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary mapping model names to their maximum token counts. Example: {'model-name': max_token_count}</p> <p> TYPE: <code>Dict[str, int]</code> </p> Source code in <code>src/misc/max_tokens.py</code> <pre><code>def calculate_max_tokens(\n    dataset_path: str,\n    synopsis_columns: List[str],\n    model_names: List[str],\n    batch_size: int = 64,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Calculate the maximum token count for each model across specified synopsis columns in a dataset.\n\n    Args:\n        dataset_path (str): Path to the CSV dataset file.\n        synopsis_columns (list): List of column names containing synopsis text to analyze.\n        model_names (list): List of model names/paths to test for tokenization.\n        batch_size (int, optional): Batch size for processing. Defaults to 64.\n\n    Returns:\n        dict: Dictionary mapping model names to their maximum token counts.\n            Example: {'model-name': max_token_count}\n    \"\"\"\n    # Load the dataset\n    df = pd.read_csv(dataset_path)\n\n    # Dictionary to store the highest token count for each model\n    model_max_token_counts = {}\n\n    for model_name in model_names:\n        # Initialize the tokenizer for the current model\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name, model_max_length=100000, clean_up_tokenization_spaces=True\n        )\n\n        # Variable to store the maximum token count for the current model\n        current_max_tokens = 0\n\n        for column in synopsis_columns:\n            if column not in df.columns:\n                print(f\"Column '{column}' not found in dataset. Skipping...\")\n                continue\n\n            # Drop NaN values and convert to list\n            synopses = df[column].dropna().tolist()\n\n            # Process in batches\n            for i in tqdm(\n                range(0, len(synopses), batch_size),\n                desc=f\"Processing {model_name} - {column}\",\n            ):\n                batch = synopses[i : i + batch_size]\n                # Tokenize each text individually to avoid iteration issues\n                for text in batch:\n                    tokens = tokenizer(\n                        text, add_special_tokens=True, max_length=100000\n                    )[\"input_ids\"]\n                    tokens_count = len(tokens)  # type: ignore\n                    # Update current_max_tokens if the current tokens_count is higher\n                    if tokens_count &gt; current_max_tokens:\n                        current_max_tokens = tokens_count\n\n        # Store the maximum token count for the current model\n        model_max_token_counts[model_name] = current_max_tokens\n\n    return model_max_token_counts\n</code></pre>"},{"location":"Tests/Conftest/","title":"Conftest","text":"<p>This module configures pytest options and fixtures for testing.</p> <p>It includes a command line option to specify the model name for tests and a fixture to retrieve the model name from the command line options. The model name is used for loading pre-trained sentence transformer models during testing.</p> <p>The default model is 'sentence-transformers/all-MiniLM-L6-v1', which provides a good balance between performance and resource usage for testing purposes.</p>"},{"location":"Tests/Conftest/#tests.conftest.model_name","title":"model_name","text":"<pre><code>model_name(request: FixtureRequest) -&gt; str\n</code></pre> <p>Fixture to retrieve the model name from the command line options.</p> <p>This fixture provides access to the model name specified via the --model command line option. If no model is specified, it returns the default 'sentence-transformers/all-MiniLM-L6-v1'.</p> PARAMETER DESCRIPTION <code>request</code> <p>The request object providing access to the test context and command line options.</p> <p> TYPE: <code>FixtureRequest</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The model name specified in the command line options or the default value. The model name is typically in the format 'sentence-transformers/model-name'.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tests/conftest.py</code> <pre><code>@pytest.fixture\ndef model_name(request: FixtureRequest) -&gt; str:\n    \"\"\"\n    Fixture to retrieve the model name from the command line options.\n\n    This fixture provides access to the model name specified via the --model\n    command line option. If no model is specified, it returns the default\n    'sentence-transformers/all-MiniLM-L6-v1'.\n\n    Args:\n        request (FixtureRequest): The request object providing access to the test context\n            and command line options.\n\n    Returns:\n        str: The model name specified in the command line options or the default value.\n            The model name is typically in the format 'sentence-transformers/model-name'.\n    \"\"\"\n    return str(request.config.getoption(\"--model\"))\n</code></pre>"},{"location":"Tests/Conftest/#tests.conftest.pytest_addoption","title":"pytest_addoption","text":"<pre><code>pytest_addoption(parser: Parser) -&gt; None\n</code></pre> <p>Add a command line option to specify the model name for tests.</p> <p>This function adds the '--model' option to pytest's command line interface, allowing users to specify which sentence transformer model to use during testing.</p> PARAMETER DESCRIPTION <code>parser</code> <p>The pytest command line argument parser.</p> <p> TYPE: <code>Parser</code> </p> Example <p>pytest --model \"sentence-transformers/all-mpnet-base-v2\"</p> Source code in <code>tests/conftest.py</code> <pre><code>def pytest_addoption(parser: Parser) -&gt; None:\n    \"\"\"\n    Add a command line option to specify the model name for tests.\n\n    This function adds the '--model' option to pytest's command line interface,\n    allowing users to specify which sentence transformer model to use during testing.\n\n    Args:\n        parser (Parser): The pytest command line argument parser.\n\n    Example:\n        pytest --model \"sentence-transformers/all-mpnet-base-v2\"\n    \"\"\"\n    parser.addoption(\n        \"--model\",\n        action=\"store\",\n        default=\"sentence-transformers/all-MiniLM-L6-v1\",\n        help=\"Model name to use for tests\",\n    )\n</code></pre>"},{"location":"Tests/TestAPI/","title":"TestAPI","text":"<p>This module contains tests for the Flask API endpoints in the src.api module.</p> <p>The tests verify the functionality of the /anisearchmodel/manga endpoint, ensuring it handles valid inputs, missing fields, and internal server errors correctly. The tests use a mock for the get_similarities function to simulate different scenarios.</p> The test suite includes <ul> <li>Testing successful manga similarity search with valid inputs</li> <li>Testing error handling for invalid inputs (missing fields, invalid model names)</li> <li>Testing internal server error handling</li> <li>Parameterized tests for different invalid input scenarios</li> </ul> <p>The tests use pytest fixtures for the Flask test client and model name configuration.</p>"},{"location":"Tests/TestAPI/#tests.test_api.client","title":"client","text":"<pre><code>client() -&gt; Generator[FlaskClient, None, None]\n</code></pre> <p>Fixture to create a test client for the Flask application.</p> RETURNS DESCRIPTION <code>None</code> <p>Generator[FlaskClient, None, None]: A Flask test client instance that can be used to make requests to the application endpoints.</p> Source code in <code>tests/test_api.py</code> <pre><code>@pytest.fixture\ndef client() -&gt; Generator[FlaskClient, None, None]:\n    \"\"\"\n    Fixture to create a test client for the Flask application.\n\n    Returns:\n        Generator[FlaskClient, None, None]: A Flask test client instance that can be used\n            to make requests to the application endpoints.\n    \"\"\"\n    app.config[\"TESTING\"] = True\n    with app.test_client() as client:  # pylint: disable=W0621\n        yield client\n</code></pre>"},{"location":"Tests/TestAPI/#tests.test_api.test_get_manga_similarities_internal_error","title":"test_get_manga_similarities_internal_error","text":"<pre><code>test_get_manga_similarities_internal_error(client: FlaskClient, model_name: str) -&gt; None\n</code></pre> <p>Test the /anisearchmodel/manga endpoint for internal server errors.</p> <p>Verifies that the endpoint returns a 500 status code and an error message when an exception occurs during processing.</p> PARAMETER DESCRIPTION <code>client</code> <p>Flask test client fixture</p> <p> TYPE: <code>FlaskClient</code> </p> <code>model_name</code> <p>Model name fixture from command line options</p> <p> TYPE: <code>str</code> </p> The test <ol> <li>Creates a valid payload</li> <li>Mocks get_similarities to raise an exception</li> <li>Verifies the 500 status code and error message</li> </ol> Source code in <code>tests/test_api.py</code> <pre><code>@pytest.mark.order(15)\ndef test_get_manga_similarities_internal_error(\n    client: FlaskClient,  # pylint: disable=W0621\n    model_name: str,\n) -&gt; None:\n    \"\"\"\n    Test the /anisearchmodel/manga endpoint for internal server errors.\n\n    Verifies that the endpoint returns a 500 status code and an error message\n    when an exception occurs during processing.\n\n    Args:\n        client (FlaskClient): Flask test client fixture\n        model_name (str): Model name fixture from command line options\n\n    The test:\n        1. Creates a valid payload\n        2. Mocks get_similarities to raise an exception\n        3. Verifies the 500 status code and error message\n    \"\"\"\n    payload = {\n        \"model\": model_name,\n        \"description\": \"A hero reincarnated as a slime.\",\n    }\n\n    with patch(\n        \"src.api.get_similarities\", side_effect=Exception(\"Database connection failed\")\n    ):\n        response = client.post(\"/anisearchmodel/manga\", json=payload)\n        assert response.status_code == 500\n        data = response.get_json()\n        assert \"error\" in data\n        assert data[\"error\"] == \"Internal server error\"\n    time.sleep(1)\n</code></pre>"},{"location":"Tests/TestAPI/#tests.test_api.test_get_manga_similarities_invalid_input","title":"test_get_manga_similarities_invalid_input","text":"<pre><code>test_get_manga_similarities_invalid_input(client: FlaskClient, payload: dict, expected_error: str) -&gt; None\n</code></pre> <p>Test the /anisearchmodel/manga endpoint with invalid inputs.</p> <p>Verifies that the endpoint returns a 400 status code and an error message when the input is invalid.</p> PARAMETER DESCRIPTION <code>client</code> <p>Flask test client fixture</p> <p> TYPE: <code>FlaskClient</code> </p> <code>payload</code> <p>Test payload with invalid input combinations</p> <p> TYPE: <code>dict</code> </p> <code>expected_error</code> <p>Expected error message for the given invalid input</p> <p> TYPE: <code>str</code> </p> The test cases verify <ol> <li>Missing model name</li> <li>Invalid model name</li> <li>Missing description</li> </ol> Source code in <code>tests/test_api.py</code> <pre><code>@pytest.mark.parametrize(\n    \"payload, expected_error\",\n    [\n        (\n            {\"description\": \"A hero reincarnated as a slime.\"},\n            \"Model name and description are required\",\n        ),\n        (\n            {\n                \"model\": \"invalid_model\",\n                \"description\": \"A hero reincarnated as a slime.\",\n            },\n            \"Invalid model name\",\n        ),\n        ({\"model\": \"valid_model\"}, \"Model name and description are required\"),\n    ],\n)\n@pytest.mark.order(14)\ndef test_get_manga_similarities_invalid_input(\n    client: FlaskClient,  # pylint: disable=W0621\n    payload: dict,\n    expected_error: str,\n) -&gt; None:\n    \"\"\"\n    Test the /anisearchmodel/manga endpoint with invalid inputs.\n\n    Verifies that the endpoint returns a 400 status code and an error message\n    when the input is invalid.\n\n    Args:\n        client (FlaskClient): Flask test client fixture\n        payload (dict): Test payload with invalid input combinations\n        expected_error (str): Expected error message for the given invalid input\n\n    The test cases verify:\n        1. Missing model name\n        2. Invalid model name\n        3. Missing description\n    \"\"\"\n    response = client.post(\"/anisearchmodel/manga\", json=payload)\n    assert response.status_code == 400\n    data = response.get_json()\n    assert \"error\" in data\n    assert data[\"error\"] == expected_error\n    time.sleep(1)\n</code></pre>"},{"location":"Tests/TestAPI/#tests.test_api.test_get_manga_similarities_success","title":"test_get_manga_similarities_success","text":"<pre><code>test_get_manga_similarities_success(client: FlaskClient, model_name: str) -&gt; None\n</code></pre> <p>Test the /anisearchmodel/manga endpoint with valid input.</p> <p>Verifies that the endpoint returns a 200 status code and the expected list of similarities when provided with a valid model and description.</p> PARAMETER DESCRIPTION <code>client</code> <p>Flask test client fixture</p> <p> TYPE: <code>FlaskClient</code> </p> <code>model_name</code> <p>Model name fixture from command line options</p> <p> TYPE: <code>str</code> </p> The test <ol> <li>Creates a payload with valid model name and description</li> <li>Mocks the get_similarities function to return predefined results</li> <li>Verifies the response status code and structure of returned data</li> </ol> Source code in <code>tests/test_api.py</code> <pre><code>@pytest.mark.order(13)\ndef test_get_manga_similarities_success(client: FlaskClient, model_name: str) -&gt; None:  # pylint: disable=W0621\n    \"\"\"\n    Test the /anisearchmodel/manga endpoint with valid input.\n\n    Verifies that the endpoint returns a 200 status code and the expected\n    list of similarities when provided with a valid model and description.\n\n    Args:\n        client (FlaskClient): Flask test client fixture\n        model_name (str): Model name fixture from command line options\n\n    The test:\n        1. Creates a payload with valid model name and description\n        2. Mocks the get_similarities function to return predefined results\n        3. Verifies the response status code and structure of returned data\n    \"\"\"\n    # Sample payload\n    payload = {\n        \"model\": model_name,\n        \"description\": \"A hero reincarnated as a slime.\",\n    }\n\n    # Mock the get_similarities function\n    with patch(\"src.api.get_similarities\") as mock_get_similarities:\n        mock_get_similarities.return_value = [\n            {\"name\": \"A slime with unique powers.\", \"similarity\": 0.95},\n            {\"name\": \"Reincarnation in a fantasy world.\", \"similarity\": 0.90},\n        ]\n\n        response = client.post(\"/anisearchmodel/manga\", json=payload)\n        assert response.status_code == 200\n        data = response.get_json()\n        assert isinstance(data, list)\n        assert len(data) == 2\n        assert data[0][\"similarity\"] == 0.95\n    time.sleep(1)\n</code></pre>"},{"location":"Tests/TestMergeDatasets/","title":"TestMergeDatasets","text":"<p>This module contains unit tests for the functions in the src.merge_datasets module.</p> The tests cover <ul> <li>Preprocessing of names to ensure correct formatting (test_preprocess_name)</li> <li>Cleaning of synopsis data by removing unwanted phrases (test_clean_synopsis)</li> <li>Consolidation of multiple title columns into a single title column (test_consolidate_titles)</li> <li>Removal of duplicate synopses or descriptions (test_remove_duplicate_infos)</li> <li>Addition of additional synopsis information to the merged DataFrame (test_add_additional_info)</li> <li>Handling of missing matches when adding additional info (test_add_additional_info_no_match)</li> <li>Processing of partial title information (test_add_additional_info_partial_titles)</li> <li>Handling of missing title data (test_add_additional_info_all_titles_na)</li> <li>Handling of whitespace and case variations (test_add_additional_info_whitespace_case)</li> </ul>"},{"location":"Tests/TestMergeDatasets/#tests.test_merge_datasets.test_add_additional_info","title":"test_add_additional_info","text":"<pre><code>test_add_additional_info(mock_find_additional_info: patch) -&gt; None\n</code></pre> <p>Test the add_additional_info function for basic functionality.</p> Tests <ul> <li>Adding additional synopsis information when titles match</li> <li>Creating new synopsis column in output DataFrame</li> <li>Correctly using mock find_additional_info function</li> <li>Handling English and Japanese titles</li> </ul> Source code in <code>tests/test_merge_datasets.py</code> <pre><code>@pytest.mark.order(5)\n@patch(\"src.merge_datasets.find_additional_info\")\ndef test_add_additional_info(mock_find_additional_info: patch) -&gt; None:  # type: ignore\n    \"\"\"\n    Test the add_additional_info function for basic functionality.\n\n    Tests:\n        - Adding additional synopsis information when titles match\n        - Creating new synopsis column in output DataFrame\n        - Correctly using mock find_additional_info function\n        - Handling English and Japanese titles\n    \"\"\"\n    # Create a merged DataFrame with multiple title columns\n    merged = pd.DataFrame(\n        {\n            \"anime_id\": [1, 2],\n            \"title_english\": [\"Naruto\", pd.NA],\n            \"title_japanese\": [pd.NA, \"\u30ca\u30eb\u30c8\"],\n            \"Synopsis\": [\"Hero's journey.\", \"Slime adventures.\"],\n        }\n    )\n\n    # Additional DataFrame with synopses based on titles\n    additional_df = pd.DataFrame(\n        {\n            \"title_english\": [\"naruto\", \"bleach\"],\n            \"title_japanese\": [pd.NA, \"\u30d6\u30ea\u30fc\u30c1\"],\n            \"additional_synopsis\": [\n                \"An epic hero's journey.\",\n                \"Bleach story synopsis.\",\n            ],\n        }\n    )\n\n    # Define a mock function to simulate find_additional_info behavior\n    def mock_find_info(\n        row: pd.Series,\n        additional_df: pd.DataFrame,  # pylint: disable=W0613\n        description_col: str,  # pylint: disable=W0613\n        name_columns: list,  # pylint: disable=W0613\n    ) -&gt; Union[str, None]:\n        if (\n            pd.notna(row[\"title_english\"])\n            and row[\"title_english\"].strip().lower() == \"naruto\"\n        ):\n            return \"An epic hero's journey.\"\n        elif (\n            pd.notna(row[\"title_english\"])\n            and row[\"title_english\"].strip().lower() == \"bleach\"\n        ):\n            return \"Bleach story synopsis.\"\n        return None\n\n    # Assign the side effect to the mock\n    mock_find_additional_info.side_effect = mock_find_info\n\n    # Call the function under test\n    updated = add_additional_info(\n        merged=merged,\n        additional_df=additional_df,\n        description_col=\"additional_synopsis\",\n        name_columns=[\"title_english\", \"title_japanese\"],\n        new_synopsis_col=\"Synopsis additional Dataset\",\n    )\n\n    # Assertions\n    assert \"Synopsis additional Dataset\" in updated.columns\n    assert updated.loc[0, \"Synopsis additional Dataset\"] == \"An epic hero's journey.\"\n    assert pd.isna(updated.loc[1, \"Synopsis additional Dataset\"])\n</code></pre>"},{"location":"Tests/TestMergeDatasets/#tests.test_merge_datasets.test_add_additional_info_all_titles_na","title":"test_add_additional_info_all_titles_na","text":"<pre><code>test_add_additional_info_all_titles_na(mock_find_additional_info: patch) -&gt; None\n</code></pre> <p>Test the add_additional_info function with completely missing title information.</p> Tests <ul> <li>Handling rows where all title columns are NA</li> <li>Skipping processing for rows with no valid titles</li> <li>Maintaining data integrity for rows with all NA titles</li> <li>Correctly processing mixed rows (some with all NA titles, some with valid titles)</li> </ul> Source code in <code>tests/test_merge_datasets.py</code> <pre><code>@pytest.mark.order(8)\n@patch(\"src.merge_datasets.find_additional_info\")\ndef test_add_additional_info_all_titles_na(\n    mock_find_additional_info: patch,  # type: ignore\n) -&gt; None:\n    \"\"\"\n    Test the add_additional_info function with completely missing title information.\n\n    Tests:\n        - Handling rows where all title columns are NA\n        - Skipping processing for rows with no valid titles\n        - Maintaining data integrity for rows with all NA titles\n        - Correctly processing mixed rows (some with all NA titles, some with valid titles)\n    \"\"\"\n    # Create a merged DataFrame with all titles as NA for one row\n    merged = pd.DataFrame(\n        {\n            \"anime_id\": [1, 2],\n            \"title_english\": [pd.NA, \"Bleach\"],\n            \"title_japanese\": [pd.NA, pd.NA],\n            \"Synopsis\": [\"Unknown.\", \"Soul reapers.\"],\n        }\n    )\n\n    # Additional DataFrame with synopses based on titles\n    additional_df = pd.DataFrame(\n        {\n            \"title_english\": [\"bleach\"],\n            \"title_japanese\": [pd.NA],\n            \"additional_synopsis\": [\"Bleach story synopsis.\"],\n        }\n    )\n\n    # Define a mock function to simulate find_additional_info behavior\n    def mock_find_info(\n        row: pd.Series,\n        additional_df: pd.DataFrame,  # pylint: disable=W0613\n        description_col: str,  # pylint: disable=W0613\n        name_columns: list,  # pylint: disable=W0613\n    ) -&gt; Union[str, None]:\n        if (\n            pd.notna(row[\"title_english\"])\n            and row[\"title_english\"].strip().lower() == \"bleach\"\n        ):\n            return \"Bleach story synopsis.\"\n        return None\n\n    # Assign the side effect to the mock\n    mock_find_additional_info.side_effect = mock_find_info\n\n    # Call the function under test\n    updated = add_additional_info(\n        merged=merged,\n        additional_df=additional_df,\n        description_col=\"additional_synopsis\",\n        name_columns=[\"title_english\", \"title_japanese\"],\n        new_synopsis_col=\"Synopsis additional Dataset\",\n    )\n\n    # Assertions\n    assert \"Synopsis additional Dataset\" in updated.columns\n    assert pd.isna(updated.loc[0, \"Synopsis additional Dataset\"])  # All titles NA\n    assert updated.loc[1, \"Synopsis additional Dataset\"] == \"Bleach story synopsis.\"\n</code></pre>"},{"location":"Tests/TestMergeDatasets/#tests.test_merge_datasets.test_add_additional_info_no_match","title":"test_add_additional_info_no_match","text":"<pre><code>test_add_additional_info_no_match(mock_find_additional_info: patch) -&gt; None\n</code></pre> <p>Test the add_additional_info function when no matches are found.</p> Tests <ul> <li>Handling cases where no matching additional info exists</li> <li>Proper handling of NA values for non-matches</li> <li>Processing multiple rows with varying match conditions</li> <li>Maintaining data integrity for non-matching rows</li> </ul> Source code in <code>tests/test_merge_datasets.py</code> <pre><code>@pytest.mark.order(6)\n@patch(\"src.merge_datasets.find_additional_info\")\ndef test_add_additional_info_no_match(mock_find_additional_info: patch) -&gt; None:  # type: ignore\n    \"\"\"\n    Test the add_additional_info function when no matches are found.\n\n    Tests:\n        - Handling cases where no matching additional info exists\n        - Proper handling of NA values for non-matches\n        - Processing multiple rows with varying match conditions\n        - Maintaining data integrity for non-matching rows\n    \"\"\"\n    # Create a merged DataFrame with multiple title columns\n    merged = pd.DataFrame(\n        {\n            \"anime_id\": [1, 2, 3],\n            \"title_english\": [\"Naruto\", \"Bleach\", \"One Piece\"],\n            \"title_japanese\": [pd.NA, \"\u30d6\u30ea\u30fc\u30c1\", \"\u30ef\u30f3\u30d4\u30fc\u30b9\"],\n            \"Synopsis\": [\"Hero's journey.\", \"Slime adventures.\", \"Pirate adventures.\"],\n        }\n    )\n\n    # Additional DataFrame with synopses based on titles\n    additional_df = pd.DataFrame(\n        {\n            \"title_english\": [\"naruto\", \"bleach\"],\n            \"title_japanese\": [pd.NA, \"\u30d6\u30ea\u30fc\u30c1\"],\n            \"additional_synopsis\": [\n                \"An epic hero's journey.\",\n                \"Bleach story synopsis.\",\n            ],\n        }\n    )\n\n    # Define a mock function to simulate find_additional_info behavior\n    def mock_find_info(\n        row: pd.Series,\n        additional_df: pd.DataFrame,  # pylint: disable=W0613\n        description_col: str,  # pylint: disable=W0613\n        name_columns: list,  # pylint: disable=W0613\n    ) -&gt; Union[str, None]:\n        if (\n            pd.notna(row[\"title_english\"])\n            and row[\"title_english\"].strip().lower() == \"naruto\"\n        ):\n            return \"An epic hero's journey.\"\n        elif (\n            pd.notna(row[\"title_english\"])\n            and row[\"title_english\"].strip().lower() == \"bleach\"\n        ):\n            return \"Bleach story synopsis.\"\n        elif (\n            pd.notna(row[\"title_english\"])\n            and row[\"title_english\"].strip().lower() == \"one piece\"\n        ):\n            return None  # No matching info\n        return None\n\n    # Assign the side effect to the mock\n    mock_find_additional_info.side_effect = mock_find_info\n\n    # Call the function under test\n    updated = add_additional_info(\n        merged=merged,\n        additional_df=additional_df,\n        description_col=\"additional_synopsis\",\n        name_columns=[\"title_english\", \"title_japanese\"],\n        new_synopsis_col=\"Synopsis additional Dataset\",\n    )\n\n    # Assertions\n    assert \"Synopsis additional Dataset\" in updated.columns\n    assert updated.loc[0, \"Synopsis additional Dataset\"] == \"An epic hero's journey.\"\n    assert updated.loc[1, \"Synopsis additional Dataset\"] == \"Bleach story synopsis.\"\n    assert pd.isna(updated.loc[2, \"Synopsis additional Dataset\"])\n</code></pre>"},{"location":"Tests/TestMergeDatasets/#tests.test_merge_datasets.test_add_additional_info_partial_titles","title":"test_add_additional_info_partial_titles","text":"<pre><code>test_add_additional_info_partial_titles(mock_find_additional_info: patch) -&gt; None\n</code></pre> <p>Test the add_additional_info function with partial title information.</p> Tests <ul> <li>Processing rows with some NA title columns but at least one valid title</li> <li>Matching based on available title information</li> <li>Handling mixed NA and non-NA title columns</li> <li>Correct synopsis assignment when matching on partial information</li> </ul> Source code in <code>tests/test_merge_datasets.py</code> <pre><code>@pytest.mark.order(7)\n@patch(\"src.merge_datasets.find_additional_info\")\ndef test_add_additional_info_partial_titles(\n    mock_find_additional_info: patch,  # type: ignore\n) -&gt; None:\n    \"\"\"\n    Test the add_additional_info function with partial title information.\n\n    Tests:\n        - Processing rows with some NA title columns but at least one valid title\n        - Matching based on available title information\n        - Handling mixed NA and non-NA title columns\n        - Correct synopsis assignment when matching on partial information\n    \"\"\"\n    # Create a merged DataFrame with partial NA titles\n    merged = pd.DataFrame(\n        {\n            \"anime_id\": [1, 2, 3],\n            \"title_english\": [\"Naruto\", \"Bleach\", pd.NA],\n            \"title_japanese\": [pd.NA, \"\u30d6\u30ea\u30fc\u30c1\", \"\u30ef\u30f3\u30d4\u30fc\u30b9\"],\n            \"Synopsis\": [\"Unknown.\", \"Unknown.\", \"Pirate adventures.\"],\n        }\n    )\n\n    # Additional DataFrame with synopses based on titles\n    additional_df = pd.DataFrame(\n        {\n            \"title_english\": [\"naruto\", \"bleach\", \"one piece\"],\n            \"title_japanese\": [pd.NA, \"\u30d6\u30ea\u30fc\u30c1\", \"\u30ef\u30f3\u30d4\u30fc\u30b9\"],\n            \"additional_synopsis\": [\n                \"An epic hero's journey.\",\n                \"Bleach story synopsis.\",\n                \"The adventures of pirates seeking the ultimate treasure.\",\n            ],\n        }\n    )\n\n    # Define a mock function to simulate find_additional_info behavior\n    def mock_find_info(\n        row: pd.Series,\n        additional_df: pd.DataFrame,  # pylint: disable=W0613\n        description_col: str,  # pylint: disable=W0613\n        name_columns: list,  # pylint: disable=W0613\n    ) -&gt; Union[str, None]:\n        if (\n            pd.notna(row[\"title_english\"])\n            and row[\"title_english\"].strip().lower() == \"naruto\"\n        ):\n            return \"An epic hero's journey.\"\n        elif (\n            pd.notna(row[\"title_japanese\"])\n            and row[\"title_japanese\"].strip().lower() == \"\u30d6\u30ea\u30fc\u30c1\"\n        ):\n            return \"Bleach story synopsis.\"\n        elif (\n            pd.notna(row[\"title_japanese\"])\n            and row[\"title_japanese\"].strip().lower() == \"\u30ef\u30f3\u30d4\u30fc\u30b9\"\n        ):\n            return \"The adventures of pirates seeking the ultimate treasure.\"\n        return None\n\n    # Assign the side effect to the mock\n    mock_find_additional_info.side_effect = mock_find_info\n\n    # Call the function under test\n    updated = add_additional_info(\n        merged=merged,\n        additional_df=additional_df,\n        description_col=\"additional_synopsis\",\n        name_columns=[\"title_english\", \"title_japanese\"],\n        new_synopsis_col=\"Synopsis additional Dataset\",\n    )\n\n    # Assertions\n    assert \"Synopsis additional Dataset\" in updated.columns\n    assert updated.loc[0, \"Synopsis additional Dataset\"] == \"An epic hero's journey.\"\n    assert updated.loc[1, \"Synopsis additional Dataset\"] == \"Bleach story synopsis.\"\n    assert (\n        updated.loc[2, \"Synopsis additional Dataset\"]\n        == \"The adventures of pirates seeking the ultimate treasure.\"\n    )\n</code></pre>"},{"location":"Tests/TestMergeDatasets/#tests.test_merge_datasets.test_add_additional_info_whitespace_case","title":"test_add_additional_info_whitespace_case","text":"<pre><code>test_add_additional_info_whitespace_case(mock_find_additional_info: patch) -&gt; None\n</code></pre> <p>Test the add_additional_info function's handling of whitespace and case variations.</p> Tests <ul> <li>Processing titles with leading/trailing whitespace</li> <li>Handling different case variations (uppercase, lowercase, mixed)</li> <li>Correct matching despite whitespace/case differences</li> <li>Maintaining original data while normalizing for comparison</li> </ul> Source code in <code>tests/test_merge_datasets.py</code> <pre><code>@pytest.mark.order(9)\n@patch(\"src.merge_datasets.find_additional_info\")\ndef test_add_additional_info_whitespace_case(\n    mock_find_additional_info: patch,  # type: ignore\n) -&gt; None:\n    \"\"\"\n    Test the add_additional_info function's handling of whitespace and case variations.\n\n    Tests:\n        - Processing titles with leading/trailing whitespace\n        - Handling different case variations (uppercase, lowercase, mixed)\n        - Correct matching despite whitespace/case differences\n        - Maintaining original data while normalizing for comparison\n    \"\"\"\n    # Create a merged DataFrame with varied title formats\n    merged = pd.DataFrame(\n        {\n            \"anime_id\": [1, 2],\n            \"title_english\": [\"  Naruto  \", \"BLEACH\"],\n            \"title_japanese\": [\"\u30ca\u30eb\u30c8\", \"\u30d6\u30ea\u30fc\u30c1\"],\n            \"Synopsis\": [\"Unknown.\", \"Unknown.\"],\n        }\n    )\n\n    # Additional DataFrame with synopses based on titles\n    additional_df = pd.DataFrame(\n        {\n            \"title_english\": [\"naruto\", \"bleach\"],\n            \"title_japanese\": [\"\u30ca\u30eb\u30c8\", \"\u30d6\u30ea\u30fc\u30c1\"],\n            \"additional_synopsis\": [\n                \"An epic hero's journey.\",\n                \"Bleach story synopsis.\",\n            ],\n        }\n    )\n\n    # Define a mock function to simulate find_additional_info behavior\n    def mock_find_info(\n        row: pd.Series,\n        additional_df: pd.DataFrame,  # pylint: disable=W0613\n        description_col: str,  # pylint: disable=W0613\n        name_columns: list,  # pylint: disable=W0613\n    ) -&gt; Union[str, None]:\n        if (\n            pd.notna(row[\"title_english\"])\n            and row[\"title_english\"].strip().lower() == \"naruto\"\n        ):\n            return \"An epic hero's journey.\"\n        elif (\n            pd.notna(row[\"title_english\"])\n            and row[\"title_english\"].strip().lower() == \"bleach\"\n        ):\n            return \"Bleach story synopsis.\"\n        return None\n\n    # Assign the side effect to the mock\n    mock_find_additional_info.side_effect = mock_find_info\n\n    # Call the function under test\n    updated = add_additional_info(\n        merged=merged,\n        additional_df=additional_df,\n        description_col=\"additional_synopsis\",\n        name_columns=[\"title_english\", \"title_japanese\"],\n        new_synopsis_col=\"Synopsis additional Dataset\",\n    )\n\n    # Assertions\n    assert \"Synopsis additional Dataset\" in updated.columns\n    assert updated.loc[0, \"Synopsis additional Dataset\"] == \"An epic hero's journey.\"\n    assert updated.loc[1, \"Synopsis additional Dataset\"] == \"Bleach story synopsis.\"\n</code></pre>"},{"location":"Tests/TestMergeDatasets/#tests.test_merge_datasets.test_clean_synopsis","title":"test_clean_synopsis","text":"<pre><code>test_clean_synopsis() -&gt; None\n</code></pre> <p>Test the clean_synopsis function to ensure it correctly cleans the synopsis column.</p> Tests <ul> <li>Preserving valid synopses</li> <li>Removing specified unwanted phrases</li> <li>Replacing unwanted phrases with empty strings</li> <li>Handling multiple occurrences of unwanted phrases</li> </ul> Source code in <code>tests/test_merge_datasets.py</code> <pre><code>@pytest.mark.order(2)\ndef test_clean_synopsis() -&gt; None:\n    \"\"\"\n    Test the clean_synopsis function to ensure it correctly cleans the synopsis column.\n\n    Tests:\n        - Preserving valid synopses\n        - Removing specified unwanted phrases\n        - Replacing unwanted phrases with empty strings\n        - Handling multiple occurrences of unwanted phrases\n    \"\"\"\n    data = {\n        \"Synopsis\": [\n            \"This is a valid synopsis.\",\n            \"No description available for this anime.\",\n            \"Another valid synopsis.\",\n            \"No description available for this anime.\",\n        ]\n    }\n    df = pd.DataFrame(data)\n    clean_synopsis(df, \"Synopsis\", [\"No description available for this anime.\"])\n    assert df.loc[0, \"Synopsis\"] == \"This is a valid synopsis.\"\n    assert df.loc[1, \"Synopsis\"] == \"\"\n    assert df.loc[2, \"Synopsis\"] == \"Another valid synopsis.\"\n    assert df.loc[3, \"Synopsis\"] == \"\"\n</code></pre>"},{"location":"Tests/TestMergeDatasets/#tests.test_merge_datasets.test_consolidate_titles","title":"test_consolidate_titles","text":"<pre><code>test_consolidate_titles() -&gt; None\n</code></pre> <p>Test the consolidate_titles function to ensure it correctly consolidates multiple title columns.</p> Tests <ul> <li>Prioritizing the main 'title' column values</li> <li>Filling missing values from alternate title columns in order</li> <li>Handling multiple NA values across columns</li> <li>Preserving existing valid titles</li> </ul> Source code in <code>tests/test_merge_datasets.py</code> <pre><code>@pytest.mark.order(3)\ndef test_consolidate_titles() -&gt; None:\n    \"\"\"\n    Test the consolidate_titles function to ensure it correctly consolidates multiple title columns.\n\n    Tests:\n        - Prioritizing the main 'title' column values\n        - Filling missing values from alternate title columns in order\n        - Handling multiple NA values across columns\n        - Preserving existing valid titles\n    \"\"\"\n    data = {\n        \"title\": [\"naruto\", pd.NA, \"one piece\", pd.NA],\n        \"title_english\": [pd.NA, \"bleach\", pd.NA, \"fullmetal alchemist\"],\n        \"title_japanese\": [pd.NA, pd.NA, \"wotakoi\", \"fm alchemist\"],\n    }\n    df = pd.DataFrame(data)\n    title_columns = [\"title_english\", \"title_japanese\"]\n\n    consolidated = consolidate_titles(df, title_columns)\n    expected = pd.Series(\n        [\"naruto\", \"bleach\", \"one piece\", \"fullmetal alchemist\"], name=\"title\"\n    )\n\n    pd.testing.assert_series_equal(consolidated, expected)\n</code></pre>"},{"location":"Tests/TestMergeDatasets/#tests.test_merge_datasets.test_preprocess_name","title":"test_preprocess_name","text":"<pre><code>test_preprocess_name() -&gt; None\n</code></pre> <p>Test the preprocess_name function to ensure it correctly preprocesses names.</p> Tests <ul> <li>Converting strings to lowercase</li> <li>Stripping leading/trailing whitespace</li> <li>Handling None values (returns empty string)</li> <li>Handling numeric values (converts to string)</li> </ul> Source code in <code>tests/test_merge_datasets.py</code> <pre><code>@pytest.mark.order(1)\ndef test_preprocess_name() -&gt; None:\n    \"\"\"\n    Test the preprocess_name function to ensure it correctly preprocesses names.\n\n    Tests:\n        - Converting strings to lowercase\n        - Stripping leading/trailing whitespace\n        - Handling None values (returns empty string)\n        - Handling numeric values (converts to string)\n    \"\"\"\n    assert preprocess_name(\"  Naruto  \") == \"naruto\"\n    assert preprocess_name(\"One Piece\") == \"one piece\"\n    assert preprocess_name(None) == \"\"\n    assert preprocess_name(123) == \"123\"\n</code></pre>"},{"location":"Tests/TestMergeDatasets/#tests.test_merge_datasets.test_remove_duplicate_infos","title":"test_remove_duplicate_infos","text":"<pre><code>test_remove_duplicate_infos() -&gt; None\n</code></pre> <p>Test the remove_duplicate_infos function to ensure it correctly handles duplicate synopses.</p> Tests <ul> <li>Identifying and removing duplicate synopses across columns</li> <li>Preserving unique synopses</li> <li>Handling NA values</li> <li>Maintaining original data structure and column order</li> </ul> Source code in <code>tests/test_merge_datasets.py</code> <pre><code>@pytest.mark.order(4)\ndef test_remove_duplicate_infos() -&gt; None:\n    \"\"\"\n    Test the remove_duplicate_infos function to ensure it correctly handles duplicate synopses.\n\n    Tests:\n        - Identifying and removing duplicate synopses across columns\n        - Preserving unique synopses\n        - Handling NA values\n        - Maintaining original data structure and column order\n    \"\"\"\n    data = {\n        \"anime_id\": [1, 2, 3],\n        \"Synopsis anime_dataset_2023\": [\"A story\", \"A story\", pd.NA],\n        \"Synopsis animes dataset\": [\"A story\", \"Different story\", pd.NA],\n        \"Synopsis anime_270 Dataset\": [pd.NA, \"A story\", \"Another story\"],\n    }\n    df = pd.DataFrame(data)\n    synopsis_cols = [\n        \"Synopsis anime_dataset_2023\",\n        \"Synopsis animes dataset\",\n        \"Synopsis anime_270 Dataset\",\n    ]\n\n    cleaned_df = remove_duplicate_infos(df, synopsis_cols)\n\n    expected = {\n        \"anime_id\": [1, 2, 3],\n        \"Synopsis anime_dataset_2023\": [\"A story\", \"A story\", pd.NA],\n        \"Synopsis animes dataset\": [pd.NA, \"Different story\", pd.NA],\n        \"Synopsis anime_270 Dataset\": [pd.NA, pd.NA, \"Another story\"],\n    }\n    expected_df = pd.DataFrame(expected)\n\n    # Ensure both DataFrames use pd.NA for missing values\n    cleaned_df = cleaned_df.fillna(pd.NA)\n    expected_df = expected_df.fillna(pd.NA)\n\n    pd.testing.assert_frame_equal(cleaned_df, expected_df)\n</code></pre>"},{"location":"Tests/TestModel/","title":"TestModel","text":"<p>This module contains tests for evaluating the performance of models on anime and manga datasets. It includes fixtures and test functions to ensure that the models are correctly loaded, similarities are calculated, and evaluation results are saved properly.</p> The tests verify <ul> <li>Model and embedding loading functionality</li> <li>Similarity calculation between new descriptions and existing content</li> <li>Proper saving and structure of evaluation results</li> <li>Consistent behavior across both anime and manga datasets</li> </ul>"},{"location":"Tests/TestModel/#tests.test_model.new_description","title":"new_description","text":"<pre><code>new_description() -&gt; str\n</code></pre> <p>Fixture that provides a new description for testing similarity calculations.</p> <p>The description represents a common isekai anime/manga plot to test against the datasets.</p> RETURNS DESCRIPTION <code>str</code> <p>A test description about a character being reborn in another world as a slime.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tests/test_model.py</code> <pre><code>@pytest.fixture\ndef new_description() -&gt; str:\n    \"\"\"\n    Fixture that provides a new description for testing similarity calculations.\n\n    The description represents a common isekai anime/manga plot to test against the datasets.\n\n    Returns:\n        str: A test description about a character being reborn in another world as a slime.\n    \"\"\"\n    return (\n        \"The main character is a 37 year old man who is stabbed and dies, \"\n        \"but is reborn as a slime in a different world.\"\n    )\n</code></pre>"},{"location":"Tests/TestModel/#tests.test_model.test_anime_model","title":"test_anime_model","text":"<pre><code>test_anime_model(new_description: str, model_name: str) -&gt; None\n</code></pre> <p>Test the anime model's ability to find similar content based on description.</p> This test verifies <ol> <li>Proper loading of the model and anime embeddings</li> <li>Accurate calculation of similarities between new description and existing anime</li> <li>Correct structure and saving of evaluation results</li> <li>Expected number and format of top similar results</li> </ol> PARAMETER DESCRIPTION <code>new_description</code> <p>A test description to compare against the anime database.</p> <p> TYPE: <code>str</code> </p> <code>model_name</code> <p>The identifier of the model being tested.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>If any of the test conditions fail, including file existence,            data structure, or expected result format.</p> Source code in <code>tests/test_model.py</code> <pre><code>@pytest.mark.order(11)\ndef test_anime_model(new_description: str, model_name: str) -&gt; None:  # pylint: disable=redefined-outer-name\n    \"\"\"\n    Test the anime model's ability to find similar content based on description.\n\n    This test verifies:\n        1. Proper loading of the model and anime embeddings\n        2. Accurate calculation of similarities between new description and existing anime\n        3. Correct structure and saving of evaluation results\n        4. Expected number and format of top similar results\n\n    Args:\n        new_description (str): A test description to compare against the anime database.\n        model_name (str): The identifier of the model being tested.\n\n    Raises:\n        AssertionError: If any of the test conditions fail, including file existence,\n                       data structure, or expected result format.\n    \"\"\"\n    dataset_type = \"anime\"\n    top_n = 5\n\n    model, df, synopsis_columns, embeddings_save_dir = load_model_and_embeddings(\n        model_name, dataset_type\n    )\n    top_results: List[Dict[str, float]] = calculate_similarities(\n        model, df, synopsis_columns, embeddings_save_dir, new_description, top_n\n    )\n\n    assert len(top_results) == top_n\n    for result in top_results:\n        assert \"title\" in result\n        assert \"synopsis\" in result\n        assert \"similarity\" in result\n\n    evaluation_results = save_evaluation_results(\n        \"./model/evaluation_results_anime.json\",\n        model_name,\n        dataset_type,\n        new_description,\n        top_results,\n    )\n    assert os.path.exists(evaluation_results)\n    with open(evaluation_results, \"r\", encoding=\"utf-8\") as f:\n        evaluation_data = json.load(f)\n    assert len(evaluation_data) &gt; 0\n    assert isinstance(evaluation_data, list), \"evaluation_results should be a list\"\n    assert isinstance(\n        evaluation_data[-1], dict\n    ), \"Last item in evaluation_results should be a dictionary\"\n    assert \"model_name\" in evaluation_data[-1]\n    assert \"dataset_type\" in evaluation_data[-1]\n    assert \"new_description\" in evaluation_data[-1]\n    assert len(evaluation_data[-1][\"top_similarities\"]) == top_n\n</code></pre>"},{"location":"Tests/TestModel/#tests.test_model.test_manga_model","title":"test_manga_model","text":"<pre><code>test_manga_model(new_description: str, model_name: str) -&gt; None\n</code></pre> <p>Test the manga model's ability to find similar content based on description.</p> This test verifies <ol> <li>Proper loading of the model and manga embeddings</li> <li>Accurate calculation of similarities between new description and existing manga</li> <li>Correct structure and saving of evaluation results</li> <li>Expected number and format of top similar results</li> </ol> PARAMETER DESCRIPTION <code>new_description</code> <p>A test description to compare against the manga database.</p> <p> TYPE: <code>str</code> </p> <code>model_name</code> <p>The identifier of the model being tested.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>If any of the test conditions fail, including file existence,            data structure, or expected result format.</p> Source code in <code>tests/test_model.py</code> <pre><code>@pytest.mark.order(12)\ndef test_manga_model(new_description: str, model_name: str) -&gt; None:  # pylint: disable=redefined-outer-name\n    \"\"\"\n    Test the manga model's ability to find similar content based on description.\n\n    This test verifies:\n        1. Proper loading of the model and manga embeddings\n        2. Accurate calculation of similarities between new description and existing manga\n        3. Correct structure and saving of evaluation results\n        4. Expected number and format of top similar results\n\n    Args:\n        new_description (str): A test description to compare against the manga database.\n        model_name (str): The identifier of the model being tested.\n\n    Raises:\n        AssertionError: If any of the test conditions fail, including file existence,\n                       data structure, or expected result format.\n    \"\"\"\n    dataset_type = \"manga\"\n    top_n = 5\n\n    model, df, synopsis_columns, embeddings_save_dir = load_model_and_embeddings(\n        model_name, dataset_type\n    )\n    top_results: List[Dict[str, float]] = calculate_similarities(\n        model, df, synopsis_columns, embeddings_save_dir, new_description, top_n\n    )\n\n    assert len(top_results) == top_n\n    for result in top_results:\n        assert \"title\" in result\n        assert \"synopsis\" in result\n        assert \"similarity\" in result\n\n    evaluation_results = save_evaluation_results(\n        \"./model/evaluation_results_manga.json\",\n        model_name,\n        dataset_type,\n        new_description,\n        top_results,\n    )\n    assert os.path.exists(evaluation_results)\n    with open(evaluation_results, \"r\", encoding=\"utf-8\") as f:\n        evaluation_data = json.load(f)\n    assert len(evaluation_data) &gt; 0\n    assert isinstance(evaluation_data, list), \"evaluation_results should be a list\"\n    assert isinstance(\n        evaluation_data[-1], dict\n    ), \"Last item in evaluation_results should be a dictionary\"\n    assert \"model_name\" in evaluation_data[-1]\n    assert \"dataset_type\" in evaluation_data[-1]\n    assert \"new_description\" in evaluation_data[-1]\n    assert len(evaluation_data[-1][\"top_similarities\"]) == top_n\n</code></pre>"},{"location":"Tests/TestSbert/","title":"TestSbert","text":"<p>This module contains tests for the sbert.py script, which generates embeddings for anime and manga datasets using the Sentence-BERT model.</p> The tests verify <ul> <li>Successful execution of the script with valid command-line arguments</li> <li>Creation and validation of embedding files for both anime and manga datasets</li> <li>Proper saving and structure of evaluation results</li> <li>Correct dimensionality of generated embeddings</li> <li>Consistency between model parameters and evaluation data</li> </ul>"},{"location":"Tests/TestSbert/#tests.test_sbert.run_sbert_command_and_verify","title":"run_sbert_command_and_verify","text":"<pre><code>run_sbert_command_and_verify(model_name: str, dataset_type: str, expected_files: List[str]) -&gt; None\n</code></pre> <p>Run the SBERT command-line script and verify the generated embeddings and evaluation results.</p> This function <ol> <li>Executes the SBERT script with specified parameters</li> <li>Verifies script execution success</li> <li>Checks for creation of expected embedding files</li> <li>Validates embedding dimensions</li> <li>Verifies evaluation results structure and content</li> </ol> PARAMETER DESCRIPTION <code>model_name</code> <p>The name of the model to be used (e.g., 'sentence-transformers/all-mpnet-base-v2')</p> <p> TYPE: <code>str</code> </p> <code>dataset_type</code> <p>The type of dataset ('anime' or 'manga')</p> <p> TYPE: <code>str</code> </p> <code>expected_files</code> <p>List of expected embedding file names to be generated</p> <p> TYPE: <code>List[str]</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>If any of the following conditions are not met: - Script execution fails - Expected embedding files are not created - Embeddings have invalid dimensions - Evaluation results are missing or malformed - Model parameters don't match input parameters</p> Source code in <code>tests/test_sbert.py</code> <pre><code>def run_sbert_command_and_verify(\n    model_name: str, dataset_type: str, expected_files: List[str]\n) -&gt; None:\n    \"\"\"\n    Run the SBERT command-line script and verify the generated embeddings and\n    evaluation results.\n\n    This function:\n        1. Executes the SBERT script with specified parameters\n        2. Verifies script execution success\n        3. Checks for creation of expected embedding files\n        4. Validates embedding dimensions\n        5. Verifies evaluation results structure and content\n\n    Args:\n        model_name (str): The name of the model to be used (e.g.,\n            'sentence-transformers/all-mpnet-base-v2')\n        dataset_type (str): The type of dataset ('anime' or 'manga')\n        expected_files (List[str]): List of expected embedding file names to be\n            generated\n\n    Raises:\n        AssertionError: If any of the following conditions are not met:\n            - Script execution fails\n            - Expected embedding files are not created\n            - Embeddings have invalid dimensions\n            - Evaluation results are missing or malformed\n            - Model parameters don't match input parameters\n    \"\"\"\n    command = [\n        sys.executable,\n        \"-m\",\n        \"src.sbert\",\n        \"--model\",\n        model_name,\n        \"--type\",\n        dataset_type,\n    ]\n\n    result = subprocess.run(command, capture_output=True, text=True, check=True)\n    assert result.returncode == 0, f\"Script failed with stderr: {result.stderr}\"\n\n    embeddings_dir = (\n        f\"model/{dataset_type}/{model_name.replace('sentence-transformers/', '')}\"\n    )\n\n    for file_name in expected_files:\n        file_path = os.path.join(embeddings_dir, file_name)\n        assert os.path.exists(\n            file_path\n        ), f\"Embeddings file was not created at {file_path}.\"\n\n        embeddings = np.load(file_path)\n        assert embeddings.shape[1] &gt; 0, \"Embeddings should have a non-zero dimension.\"\n\n    evaluation_results_path = os.path.join(\"model/\", \"evaluation_results.json\")\n    assert os.path.exists(\n        evaluation_results_path\n    ), f\"Evaluation results were not saved at {evaluation_results_path}.\"\n\n    with open(evaluation_results_path, \"r\", encoding=\"utf-8\") as f:\n        evaluation_data = json.load(f)\n\n    if isinstance(evaluation_data, list):\n        evaluation_data = evaluation_data[-1]\n\n    assert (\n        evaluation_data[\"model_parameters\"][\"model_name\"] == model_name\n    ), \"Model name mismatch in evaluation data.\"\n    assert (\n        evaluation_data[\"type\"] == dataset_type\n    ), \"Dataset type mismatch in evaluation data.\"\n</code></pre>"},{"location":"Tests/TestSbert/#tests.test_sbert.test_run_sbert_command_line","title":"test_run_sbert_command_line","text":"<pre><code>test_run_sbert_command_line(model_name: str, dataset_type: str, expected_files: List[str]) -&gt; None\n</code></pre> <p>Test the SBERT command line script by running it with different dataset types and verifying the outputs.</p> This test <ol> <li>Tests both anime and manga datasets</li> <li>Verifies generation of dataset-specific embedding files</li> <li>Validates embedding file structure and content</li> <li>Checks evaluation results for each dataset type</li> </ol> PARAMETER DESCRIPTION <code>model_name</code> <p>The name of the model to be tested, provided by pytest fixture</p> <p> TYPE: <code>str</code> </p> <code>dataset_type</code> <p>The type of dataset being tested ('anime' or 'manga')</p> <p> TYPE: <code>str</code> </p> <code>expected_files</code> <p>List of expected embedding files for the dataset type</p> <p> TYPE: <code>List[str]</code> </p> <p>The test is parameterized to run separately for anime and manga datasets, with different expected output files for each type.</p> Source code in <code>tests/test_sbert.py</code> <pre><code>@pytest.mark.parametrize(\n    \"dataset_type, expected_files\",\n    [\n        (\n            \"anime\",\n            [\n                \"embeddings_synopsis.npy\",\n                \"embeddings_Synopsis_anime_270_Dataset.npy\",\n                \"embeddings_Synopsis_Anime_data_Dataset.npy\",\n                \"embeddings_Synopsis_anime_dataset_2023.npy\",\n                \"embeddings_Synopsis_anime2_Dataset.npy\",\n                \"embeddings_Synopsis_Anime-2022_Dataset.npy\",\n                \"embeddings_Synopsis_anime4500_Dataset.npy\",\n                \"embeddings_Synopsis_animes_dataset.npy\",\n                \"embeddings_Synopsis_mal_anime_Dataset.npy\",\n                \"embeddings_Synopsis_wykonos_Dataset.npy\",\n            ],\n        ),\n        (\n            \"manga\",\n            [\n                \"embeddings_synopsis.npy\",\n                \"embeddings_Synopsis_data_Dataset.npy\",\n                \"embeddings_Synopsis_jikan_Dataset.npy\",\n            ],\n        ),\n    ],\n)\n@pytest.mark.order(10)\ndef test_run_sbert_command_line(\n    model_name: str, dataset_type: str, expected_files: List[str]\n) -&gt; None:\n    \"\"\"\n    Test the SBERT command line script by running it with different dataset types\n    and verifying the outputs.\n\n    This test:\n        1. Tests both anime and manga datasets\n        2. Verifies generation of dataset-specific embedding files\n        3. Validates embedding file structure and content\n        4. Checks evaluation results for each dataset type\n\n    Args:\n        model_name (str): The name of the model to be tested, provided by pytest fixture\n        dataset_type (str): The type of dataset being tested ('anime' or 'manga')\n        expected_files (List[str]): List of expected embedding files for the dataset type\n\n    The test is parameterized to run separately for anime and manga datasets,\n    with different expected output files for each type.\n    \"\"\"\n    run_sbert_command_and_verify(model_name, dataset_type, expected_files)\n</code></pre>"},{"location":"Training/Common/DataUtils/","title":"DataUtils","text":"<p>Utility module for data handling and management in the anime/manga recommendation system.</p> <p>This module provides utility functions for:</p> <ol> <li> <p>Data persistence: Saving and loading training pairs</p> </li> <li> <p>Genre and theme management: Maintaining predefined sets of genres and themes    for both anime and manga datasets</p> </li> </ol> <p>The module supports two main data types:</p> <ul> <li> <p>Anime: Contains specific genres and themes for anime content</p> </li> <li> <p>Manga: Contains specific genres and themes for manga content</p> </li> </ul> FUNCTION DESCRIPTION <code>save_pairs_to_csv</code> <p>Save training pairs (text pairs and their similarity labels) to CSV</p> <code>get_genres_and_themes</code> <p>Retrieve predefined sets of genres and themes based on data type</p> <p>The genre and theme sets are carefully curated for each data type and are used for calculating semantic similarities between different entries in the dataset. These sets are essential for the training process and maintaining consistency in the recommendation system.</p> Note <p>The genres and themes are maintained as static sets within the module. Updates to these sets should be done with careful consideration of the impact on the overall recommendation system.</p>"},{"location":"Training/Common/DataUtils/#src.training.common.data_utils.DataType","title":"DataType  <code>module-attribute</code>","text":"<pre><code>DataType = Literal['anime', 'manga']\n</code></pre>"},{"location":"Training/Common/DataUtils/#src.training.common.data_utils.get_genres_and_themes","title":"get_genres_and_themes","text":"<pre><code>get_genres_and_themes(data_type: DataType) -&gt; Tuple[Set[str], Set[str]]\n</code></pre> <p>Get predefined sets of genres and themes for anime or manga data.</p> <p>This function returns two sets containing valid genres and themes based on the specified data type. The sets are curated specifically for each content type to ensure accurate semantic similarity calculations.</p> PARAMETER DESCRIPTION <code>data_type</code> <p>Type of data to get genres/themes for (\"anime\" or \"manga\")</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>Tuple[Set[str], Set[str]]</code> <p>Tuple[Set[str], Set[str]]: A tuple containing: - First element: Set of all valid genres for the data type - Second element: Set of all valid themes for the data type</p> RAISES DESCRIPTION <code>ValueError</code> <p>If data_type is not \"anime\" or \"manga\"</p> Source code in <code>src/training/common/data_utils.py</code> <pre><code>def get_genres_and_themes(data_type: DataType) -&gt; Tuple[Set[str], Set[str]]:\n    \"\"\"\n    Get predefined sets of genres and themes for anime or manga data.\n\n    This function returns two sets containing valid genres and themes based on the\n    specified data type. The sets are curated specifically for each content type\n    to ensure accurate semantic similarity calculations.\n\n    Args:\n        data_type (DataType): Type of data to get genres/themes for (\"anime\" or \"manga\")\n\n    Returns:\n        Tuple[Set[str], Set[str]]: A tuple containing:\n            - First element: Set of all valid genres for the data type\n            - Second element: Set of all valid themes for the data type\n\n    Raises:\n        ValueError: If data_type is not \"anime\" or \"manga\"\n    \"\"\"\n    all_genres: Set[str]\n    all_themes: Set[str]\n    if data_type == \"anime\":\n        all_genres = {\n            \"Slice of Life\",\n            \"Boys Love\",\n            \"Drama\",\n            \"Suspense\",\n            \"Gourmet\",\n            \"Erotica\",\n            \"Romance\",\n            \"Comedy\",\n            \"Hentai\",\n            \"Sports\",\n            \"Supernatural\",\n            \"Fantasy\",\n            \"Girls Love\",\n            \"Mystery\",\n            \"Adventure\",\n            \"Horror\",\n            \"Award Winning\",\n            \"Action\",\n            \"Avant Garde\",\n            \"Ecchi\",\n            \"Sci-Fi\",\n        }\n\n        all_themes = {\n            \"Military\",\n            \"Survival\",\n            \"Idols (Female)\",\n            \"High Stakes Game\",\n            \"Crossdressing\",\n            \"Delinquents\",\n            \"Vampire\",\n            \"Video Game\",\n            \"Action\",\n            \"Adventure\",\n            \"Comedy\",\n            \"Drama\",\n            \"Fantasy\",\n            \"Horror\",\n            \"Mystery\",\n            \"Romance\",\n            \"Sci-Fi\",\n            \"Slice of Life\",\n            \"Supernatural\",\n            \"Thriller\",\n            \"Sports\",\n            \"Magical Realism\",\n            \"Mecha\",\n            \"Psychological\",\n            \"Parody\",\n        }\n    elif data_type == \"manga\":\n        all_genres = {\n            \"Comedy\",\n            \"Romance\",\n            \"Gourmet\",\n            \"Action\",\n            \"Avant Garde\",\n            \"Fantasy\",\n            \"Sports\",\n            \"Sci-Fi\",\n            \"Suspense\",\n            \"Erotica\",\n            \"Adventure\",\n            \"Slice of Life\",\n            \"Ecchi\",\n            \"Supernatural\",\n            \"Horror\",\n            \"Girls Love\",\n            \"Mystery\",\n            \"Award Winning\",\n            \"Drama\",\n        }\n\n        all_themes = {\n            \"Martial Arts\",\n            \"Romantic Subtext\",\n            \"Music\",\n            \"Crossdressing\",\n            \"Workplace\",\n            \"Pets\",\n            \"Medical\",\n            \"Adult Cast\",\n            \"Combat Sports\",\n            \"Gag Humor\",\n            \"Reincarnation\",\n            \"Visual Arts\",\n            \"Showbiz\",\n            \"Racing\",\n            \"Iyashikei\",\n            \"Time Travel\",\n            \"CGDCT\",\n            \"Strategy Game\",\n            \"Villainess\",\n            \"Idols (Female)\",\n            \"Gore\",\n            \"Team Sports\",\n            \"Video Game\",\n            \"Super Power\",\n            \"Samurai\",\n            \"Organized Crime\",\n            \"Parody\",\n            \"Childcare\",\n            \"Magical Sex Shift\",\n            \"Love Polygon\",\n            \"Performing Arts\",\n            \"Anthropomorphic\",\n            \"Historical\",\n            \"Vampire\",\n            \"Reverse Harem\",\n            \"Isekai\",\n            \"Mecha\",\n            \"Delinquents\",\n            \"Detective\",\n            \"Idols (Male)\",\n            \"Otaku Culture\",\n            \"Mythology\",\n            \"Military\",\n            \"Mahou Shoujo\",\n            \"High Stakes Game\",\n            \"School\",\n            \"Space\",\n            \"Educational\",\n            \"Psychological\",\n            \"Harem\",\n            \"Memoir\",\n            \"Survival\",\n        }\n    else:\n        raise ValueError(f\"Unsupported data_type: {data_type}\")\n\n    return all_genres, all_themes\n</code></pre>"},{"location":"Training/Common/DataUtils/#src.training.common.data_utils.save_pairs_to_csv","title":"save_pairs_to_csv","text":"<pre><code>save_pairs_to_csv(pairs: List[InputExample], filename: Optional[str]) -&gt; None\n</code></pre> <p>Save pairs of texts and their similarity labels to a CSV file.</p> <p>This function takes a list of InputExample objects containing text pairs and their similarity labels and saves them to a CSV file. It creates the output directory if it doesn't exist.</p> PARAMETER DESCRIPTION <code>pairs</code> <p>List of text pairs and their similarity labels</p> <p> TYPE: <code>List[InputExample]</code> </p> <code>filename</code> <p>Path to save the CSV file</p> <p> TYPE: <code>Optional[str]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>TypeError</code> <p>If filename parameter is None</p> <code>OSError</code> <p>If directory creation fails or file cannot be written</p> Source code in <code>src/training/common/data_utils.py</code> <pre><code>def save_pairs_to_csv(pairs: List[InputExample], filename: Optional[str]) -&gt; None:\n    \"\"\"\n    Save pairs of texts and their similarity labels to a CSV file.\n\n    This function takes a list of InputExample objects containing text pairs and their\n    similarity labels and saves them to a CSV file. It creates the output directory\n    if it doesn't exist.\n\n    Args:\n        pairs (List[InputExample]): List of text pairs and their similarity labels\n        filename (Optional[str]): Path to save the CSV file\n\n    Returns:\n        None\n\n    Raises:\n        TypeError: If filename parameter is None\n        OSError: If directory creation fails or file cannot be written\n    \"\"\"\n    if filename is None:\n        raise TypeError(\"Filename cannot be None\")\n\n    # Ensure the directory exists\n    directory: str = os.path.dirname(filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    data: Dict[str, List[Union[str, float]]] = {\n        \"text_a\": [pair.texts[0] for pair in pairs],\n        \"text_b\": [pair.texts[1] for pair in pairs],\n        \"label\": [float(pair.label) for pair in pairs],\n    }\n    pairs_df: pd.DataFrame = pd.DataFrame(data)\n    pairs_df.to_csv(filename, index=False)\n    print(f\"Pairs saved to {filename}\")\n</code></pre>"},{"location":"Training/Common/EarlyStoppingCallback/","title":"EarlyStoppingCallback","text":"<p>Module implementing early stopping functionality for model training.</p> <p>This module provides an early stopping callback that monitors a metric during training and stops the training process if no improvement is seen for a specified number of evaluations. This helps prevent overfitting by stopping training when the model performance plateaus or starts to degrade on validation data.</p>"},{"location":"Training/Common/EarlyStoppingCallback/#src.training.common.early_stopping.EarlyStoppingCallback","title":"EarlyStoppingCallback","text":"<pre><code>EarlyStoppingCallback(patience: int = 3, min_delta: float = 0.0)\n</code></pre> <p>Callback to implement early stopping during training.</p> ATTRIBUTE DESCRIPTION <code>patience</code> <p>Number of evaluations with no improvement after which training will be stopped.</p> <p> TYPE: <code>int</code> </p> <code>min_delta</code> <p>Minimum change in the monitored metric to qualify as an improvement.</p> <p> TYPE: <code>float</code> </p> <code>best_score</code> <p>Best score observed so far.</p> <p> TYPE: <code>float</code> </p> <code>counter</code> <p>Number of consecutive evaluations with no improvement.</p> <p> TYPE: <code>int</code> </p> <code>stop_training</code> <p>Flag to indicate whether to stop training.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>src/training/common/early_stopping.py</code> <pre><code>def __init__(self, patience: int = 3, min_delta: float = 0.0):\n    self.patience = patience\n    self.min_delta = min_delta\n    self.best_score = None\n    self.counter = 0\n    self.stop_training = False\n</code></pre>"},{"location":"Training/Common/EarlyStoppingCallback/#src.training.common.early_stopping.EarlyStoppingCallback.best_score","title":"best_score  <code>instance-attribute</code>","text":"<pre><code>best_score = None\n</code></pre>"},{"location":"Training/Common/EarlyStoppingCallback/#src.training.common.early_stopping.EarlyStoppingCallback.counter","title":"counter  <code>instance-attribute</code>","text":"<pre><code>counter = 0\n</code></pre>"},{"location":"Training/Common/EarlyStoppingCallback/#src.training.common.early_stopping.EarlyStoppingCallback.min_delta","title":"min_delta  <code>instance-attribute</code>","text":"<pre><code>min_delta = min_delta\n</code></pre>"},{"location":"Training/Common/EarlyStoppingCallback/#src.training.common.early_stopping.EarlyStoppingCallback.patience","title":"patience  <code>instance-attribute</code>","text":"<pre><code>patience = patience\n</code></pre>"},{"location":"Training/Common/EarlyStoppingCallback/#src.training.common.early_stopping.EarlyStoppingCallback.stop_training","title":"stop_training  <code>instance-attribute</code>","text":"<pre><code>stop_training = False\n</code></pre>"},{"location":"Training/Common/EarlyStoppingCallback/#src.training.common.early_stopping.EarlyStoppingCallback.on_evaluate","title":"on_evaluate","text":"<pre><code>on_evaluate(score: float, epoch: int, steps: int)\n</code></pre> <p>Evaluate current score and update early stopping state.</p> PARAMETER DESCRIPTION <code>score</code> <p>Current evaluation score to compare against best score</p> <p> TYPE: <code>float</code> </p> <code>epoch</code> <p>Current training epoch number</p> <p> TYPE: <code>int</code> </p> <code>steps</code> <p>Current training step number</p> <p> TYPE: <code>int</code> </p> <p>This method compares the current score against the best score seen so far. If the score does not improve by at least min_delta for patience number of evaluations, it sets stop_training to True.</p> Source code in <code>src/training/common/early_stopping.py</code> <pre><code>def on_evaluate(self, score: float, epoch: int, steps: int):  # pylint: disable=unused-argument\n    \"\"\"\n    Evaluate current score and update early stopping state.\n\n    Args:\n        score (float): Current evaluation score to compare against best score\n        epoch (int): Current training epoch number\n        steps (int): Current training step number\n\n    This method compares the current score against the best score seen so far.\n    If the score does not improve by at least min_delta for patience number\n    of evaluations, it sets stop_training to True.\n    \"\"\"\n    if self.best_score is None:\n        self.best_score = score\n    elif score &lt; self.best_score + self.min_delta:\n        self.counter += 1\n        logging.info(\"EarlyStoppingCounter: %d/%d\", self.counter, self.patience)\n        if self.counter &gt;= self.patience:\n            logging.info(\"Early stopping triggered.\")\n            self.stop_training = True\n    else:\n        self.best_score = score\n        self.counter = 0\n</code></pre>"},{"location":"Training/Data/PairGeneration/","title":"PairGeneration","text":"<p>This module handles the generation of training pairs for a sentence transformer model.</p> <p>It provides functionality to create three types of pairs:</p> <ol> <li> <p>Positive pairs: Pairs of synopses from same entry with high similarity (&gt;=0.8)</p> </li> <li> <p>Partial positive pairs: Pairs from different entries with moderate similarity    (&gt;=0.5 and &lt;0.8)</p> </li> <li> <p>Negative pairs: Pairs from different entries with low similarity (&lt;0.5)</p> </li> </ol> <p>The similarity between entries is calculated based on their genres and themes using semantic embeddings. The module uses multiprocessing for efficient pair generation and includes functions for both single-row processing and batch processing.</p> FUNCTION DESCRIPTION <code>calculate_semantic_similarity</code> <p>Calculate weighted similarity between genres/themes</p> <code>create_positive_pairs</code> <p>Generate pairs from same-entry synopses with high sim</p> <code>generate_partial_positive_pairs</code> <p>Generate pairs from different entries with moderate similarity</p> <code>create_partial_positive_pairs</code> <p>Orchestrate partial positive pair generation</p> <code>generate_negative_pairs</code> <p>Generate pairs from different entries with low sim</p> <code>create_negative_pairs</code> <p>Orchestrate negative pair generation with multiprocessing</p> <p>The module supports saving generated pairs to CSV files and includes proper error handling and logging throughout the pair generation process. For all pair types, the shorter synopsis must be at least 50% the length of the longer synopsis.</p>"},{"location":"Training/Data/PairGeneration/#src.training.data.pair_generation.calculate_semantic_similarity","title":"calculate_semantic_similarity","text":"<pre><code>calculate_semantic_similarity(category_to_embedding: Dict[str, NDArray[float64]], genres_a: Set[str], genres_b: Set[str], themes_a: Set[str], themes_b: Set[str], genre_weight: float = 0.35, theme_weight: float = 0.65) -&gt; float\n</code></pre> <p>Calculate the semantic similarity between two sets of genres and themes.</p> PARAMETER DESCRIPTION <code>category_to_embedding</code> <p>Dictionary mapping categories to embeddings</p> <p> TYPE: <code>Dict[str, NDArray[float64]]</code> </p> <code>genres_a</code> <p>Set of genres for the first item</p> <p> TYPE: <code>Set[str]</code> </p> <code>genres_b</code> <p>Set of genres for the second item</p> <p> TYPE: <code>Set[str]</code> </p> <code>themes_a</code> <p>Set of themes for the first item</p> <p> TYPE: <code>Set[str]</code> </p> <code>themes_b</code> <p>Set of themes for the second item</p> <p> TYPE: <code>Set[str]</code> </p> <code>genre_weight</code> <p>Weight for genre similarity. Defaults to 0.35</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.35</code> </p> <code>theme_weight</code> <p>Weight for theme similarity. Defaults to 0.65</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.65</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Weighted semantic similarity score between 0 and 1</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/training/data/pair_generation.py</code> <pre><code>def calculate_semantic_similarity(\n    category_to_embedding: Dict[str, NDArray[np.float64]],\n    genres_a: Set[str],\n    genres_b: Set[str],\n    themes_a: Set[str],\n    themes_b: Set[str],\n    genre_weight: float = 0.35,\n    theme_weight: float = 0.65,\n) -&gt; float:\n    \"\"\"\n    Calculate the semantic similarity between two sets of genres and themes.\n\n    Args:\n        category_to_embedding (Dict[str, NDArray[np.float64]]): Dictionary mapping\n            categories to embeddings\n        genres_a (Set[str]): Set of genres for the first item\n        genres_b (Set[str]): Set of genres for the second item\n        themes_a (Set[str]): Set of themes for the first item\n        themes_b (Set[str]): Set of themes for the second item\n        genre_weight (float, optional): Weight for genre similarity.\n            Defaults to 0.35\n        theme_weight (float, optional): Weight for theme similarity.\n            Defaults to 0.65\n\n    Returns:\n        float: Weighted semantic similarity score between 0 and 1\n    \"\"\"\n    # Calculate cosine similarity for genres\n    try:\n        if len(genres_a) &gt; 0 and len(genres_b) &gt; 0:\n            genre_sim_values = [\n                cosine_similarity(\n                    [category_to_embedding[g1].astype(np.float64)],  # type: ignore\n                    [category_to_embedding[g2].astype(np.float64)],  # type: ignore\n                )[0][0]\n                for g1 in genres_a\n                for g2 in genres_b\n                if g1 in category_to_embedding and g2 in category_to_embedding\n            ]\n            genre_sim = float(np.mean(genre_sim_values)) if genre_sim_values else 0.0\n        else:\n            genre_sim = 0.0\n    except Exception as e:  # pylint: disable=broad-exception-caught\n        logging.error(\"Error calculating genre similarity: %s\", e)\n        genre_sim = 0.0\n\n    # Calculate cosine similarity for themes\n    try:\n        if len(themes_a) &gt; 0 and len(themes_b) &gt; 0:\n            theme_sim_values = [\n                cosine_similarity(\n                    [category_to_embedding[t1].astype(np.float64)],  # type: ignore\n                    [category_to_embedding[t2].astype(np.float64)],  # type: ignore\n                )[0][0]\n                for t1 in themes_a\n                for t2 in themes_b\n                if t1 in category_to_embedding and t2 in category_to_embedding\n            ]\n            theme_sim = float(np.mean(theme_sim_values)) if theme_sim_values else 0.0\n        else:\n            theme_sim = 0.0\n    except Exception as e:  # pylint: disable=broad-exception-caught\n        logging.error(\"Error calculating theme similarity: %s\", e)\n        theme_sim = 0.0\n\n    # Weighted similarity\n    similarity = (genre_weight * genre_sim) + (theme_weight * theme_sim)\n    return float(similarity)\n</code></pre>"},{"location":"Training/Data/PairGeneration/#src.training.data.pair_generation.create_negative_pairs","title":"create_negative_pairs","text":"<pre><code>create_negative_pairs(df: DataFrame, synopses_columns: List[str], partial_threshold: float, max_negative_per_row: int, negative_pairs_file: Optional[str], num_workers: int, category_to_embedding: Dict[str, NDArray[float64]])\n</code></pre> <p>Create negative pairs from the dataframe using multiprocessing.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing the data</p> <p> TYPE: <code>DataFrame</code> </p> <code>synopses_columns</code> <p>List of column names containing synopses</p> <p> TYPE: <code>List[str]</code> </p> <code>partial_threshold</code> <p>Maximum similarity threshold for negatives</p> <p> TYPE: <code>float</code> </p> <code>max_negative_per_row</code> <p>Maximum number of negative pairs per row</p> <p> TYPE: <code>int</code> </p> <code>negative_pairs_file</code> <p>Path to save pairs CSV, if provided</p> <p> TYPE: <code>Optional[str]</code> </p> <code>num_workers</code> <p>Number of worker processes for multiprocessing</p> <p> TYPE: <code>int</code> </p> <code>category_to_embedding</code> <p>Category embedding dict</p> <p> TYPE: <code>Dict[str, NDArray[float64]]</code> </p> RETURNS DESCRIPTION <p>List[InputExample]: List of negative pairs with similarity between 0.15 and</p> <p>partial_threshold-0.01. Each pair consists of synopses from different entries</p> <p>where the shorter synopsis is at least 50% the length of the longer one.</p> Source code in <code>src/training/data/pair_generation.py</code> <pre><code>def create_negative_pairs(\n    df: pd.DataFrame,\n    synopses_columns: List[str],\n    partial_threshold: float,\n    max_negative_per_row: int,\n    negative_pairs_file: Optional[str],\n    num_workers: int,\n    category_to_embedding: Dict[str, NDArray[np.float64]],\n):\n    \"\"\"\n    Create negative pairs from the dataframe using multiprocessing.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data\n        synopses_columns (List[str]): List of column names containing synopses\n        partial_threshold (float): Maximum similarity threshold for negatives\n        max_negative_per_row (int): Maximum number of negative pairs per row\n        negative_pairs_file (Optional[str]): Path to save pairs CSV, if provided\n        num_workers (int): Number of worker processes for multiprocessing\n        category_to_embedding (Dict[str, NDArray[np.float64]]): Category embedding dict\n\n    Returns:\n        List[InputExample]: List of negative pairs with similarity between 0.15 and\n        partial_threshold-0.01. Each pair consists of synopses from different entries\n        where the shorter synopsis is at least 50% the length of the longer one.\n    \"\"\"\n    row_indices = list(range(len(df)))\n    valid_indices = []\n\n    for j in tqdm(row_indices, desc=\"Processing rows for negative pairs\"):\n        genres = df.iloc[j][\"genres\"]\n        themes = df.iloc[j][\"themes\"]\n        if (isinstance(genres, str) and len(genres) &gt; 0) or (\n            isinstance(themes, str) and len(themes) &gt; 0\n        ):\n            genres_j = (\n                set(ast.literal_eval(genres)) if isinstance(genres, str) else set()\n            )\n            themes_j = (\n                set(ast.literal_eval(themes)) if isinstance(themes, str) else set()\n            )\n            if genres_j and themes_j:\n                valid_indices.append(j)\n\n    num_workers = max(num_workers - 2, 1)\n    with Pool(processes=num_workers) as pool:\n        negative_func = partial(\n            generate_negative_pairs,\n            df=df,\n            synopses_columns=synopses_columns,\n            partial_threshold=partial_threshold,\n            max_negative_per_row=max_negative_per_row,\n            category_to_embedding=category_to_embedding,\n            valid_indices=valid_indices,\n        )\n        negative_results = list(\n            tqdm(\n                pool.imap_unordered(negative_func, range(len(df))),\n                total=len(df),\n                desc=\"Creating negative pairs\",\n            )\n        )\n\n    negative_pairs = [pair for sublist in negative_results for pair in sublist]\n    save_pairs_to_csv(negative_pairs, negative_pairs_file)\n    return negative_pairs\n</code></pre>"},{"location":"Training/Data/PairGeneration/#src.training.data.pair_generation.create_partial_positive_pairs","title":"create_partial_positive_pairs","text":"<pre><code>create_partial_positive_pairs(df: DataFrame, synopses_columns: List[str], partial_threshold: float, max_partial_per_row: int, partial_positive_pairs_file: Optional[str], num_workers: int, category_to_embedding: Dict[str, NDArray[float64]]) -&gt; List[InputExample]\n</code></pre> <p>Create partial positive pairs from the dataframe using multiprocessing.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing the data</p> <p> TYPE: <code>DataFrame</code> </p> <code>synopses_columns</code> <p>List of column names containing synopses</p> <p> TYPE: <code>List[str]</code> </p> <code>partial_threshold</code> <p>Minimum similarity threshold for partial positives</p> <p> TYPE: <code>float</code> </p> <code>max_partial_per_row</code> <p>Maximum number of partial positive pairs per row</p> <p> TYPE: <code>int</code> </p> <code>partial_positive_pairs_file</code> <p>Path to save pairs CSV, if provided</p> <p> TYPE: <code>Optional[str]</code> </p> <code>num_workers</code> <p>Number of worker processes for multiprocessing</p> <p> TYPE: <code>int</code> </p> <code>category_to_embedding</code> <p>Category embedding dict</p> <p> TYPE: <code>Dict[str, NDArray[float64]]</code> </p> RETURNS DESCRIPTION <code>List[InputExample]</code> <p>List[InputExample]: List of partial positive pairs with similarity between</p> <code>List[InputExample]</code> <p>partial_threshold+0.01 and 0.8. Each pair consists of synopses from different</p> <code>List[InputExample]</code> <p>entries where the shorter synopsis is at least 50% the length of the longer one.</p> Source code in <code>src/training/data/pair_generation.py</code> <pre><code>def create_partial_positive_pairs(\n    df: pd.DataFrame,\n    synopses_columns: List[str],\n    partial_threshold: float,\n    max_partial_per_row: int,\n    partial_positive_pairs_file: Optional[str],\n    num_workers: int,\n    category_to_embedding: Dict[str, NDArray[np.float64]],\n) -&gt; List[InputExample]:\n    \"\"\"\n    Create partial positive pairs from the dataframe using multiprocessing.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data\n        synopses_columns (List[str]): List of column names containing synopses\n        partial_threshold (float): Minimum similarity threshold for partial positives\n        max_partial_per_row (int): Maximum number of partial positive pairs per row\n        partial_positive_pairs_file (Optional[str]): Path to save pairs CSV, if provided\n        num_workers (int): Number of worker processes for multiprocessing\n        category_to_embedding (Dict[str, NDArray[np.float64]]): Category embedding dict\n\n    Returns:\n        List[InputExample]: List of partial positive pairs with similarity between\n        partial_threshold+0.01 and 0.8. Each pair consists of synopses from different\n        entries where the shorter synopsis is at least 50% the length of the longer one.\n    \"\"\"\n    row_indices: List[int] = list(range(len(df)))\n    valid_indices: List[int] = []\n\n    for j in tqdm(row_indices, desc=\"Processing rows for partial positive pairs\"):\n        themes = df.iloc[j][\"themes\"]\n        if isinstance(themes, str) and len(themes) &gt; 0:\n            themes_j = (\n                set(ast.literal_eval(themes)) if isinstance(themes, str) else set()\n            )\n            if themes_j:\n                valid_indices.append(j)\n\n    num_workers = max(num_workers, 1)\n    with Pool(processes=num_workers) as pool:\n        partial_func = partial(\n            generate_partial_positive_pairs,\n            df=df,\n            synopses_columns=synopses_columns,\n            partial_threshold=partial_threshold,\n            max_partial_per_row=max_partial_per_row,\n            category_to_embedding=category_to_embedding,\n            valid_indices=valid_indices,\n        )\n        partial_results: List[List[InputExample]] = list(\n            tqdm(\n                pool.imap_unordered(partial_func, range(len(df))),\n                total=len(df),\n                desc=\"Creating partial positive pairs\",\n            )\n        )\n\n    partial_positive_pairs: List[InputExample] = [\n        pair for sublist in partial_results for pair in sublist\n    ]\n    save_pairs_to_csv(partial_positive_pairs, partial_positive_pairs_file)\n    return partial_positive_pairs\n</code></pre>"},{"location":"Training/Data/PairGeneration/#src.training.data.pair_generation.create_positive_pairs","title":"create_positive_pairs","text":"<pre><code>create_positive_pairs(df: DataFrame, synopses_columns: List[str], encoder_model: SentenceTransformer, positive_pairs_file: Optional[str]) -&gt; List[InputExample]\n</code></pre> <p>Create positive pairs of synopses from the same entry with high similarity.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing the data</p> <p> TYPE: <code>DataFrame</code> </p> <code>synopses_columns</code> <p>List of column names containing synopses</p> <p> TYPE: <code>List[str]</code> </p> <code>encoder_model</code> <p>Pre-trained sentence transformer model</p> <p> TYPE: <code>SentenceTransformer</code> </p> <code>positive_pairs_file</code> <p>Path to save positive pairs CSV, if provided</p> <p> TYPE: <code>Optional[str]</code> </p> RETURNS DESCRIPTION <code>List[InputExample]</code> <p>List[InputExample]: List of positive pairs with similarity scores &gt;= 0.8.</p> <code>List[InputExample]</code> <p>Each pair consists of synopses from the same entry where the shorter</p> <code>List[InputExample]</code> <p>synopsis is at least 50% the length of the longer one.</p> Source code in <code>src/training/data/pair_generation.py</code> <pre><code>def create_positive_pairs(\n    df: pd.DataFrame,\n    synopses_columns: List[str],\n    encoder_model: SentenceTransformer,\n    positive_pairs_file: Optional[str],\n) -&gt; List[InputExample]:\n    \"\"\"\n    Create positive pairs of synopses from the same entry with high similarity.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data\n        synopses_columns (List[str]): List of column names containing synopses\n        encoder_model (SentenceTransformer): Pre-trained sentence transformer model\n        positive_pairs_file (Optional[str]): Path to save positive pairs CSV, if provided\n\n    Returns:\n        List[InputExample]: List of positive pairs with similarity scores &gt;= 0.8.\n        Each pair consists of synopses from the same entry where the shorter\n        synopsis is at least 50% the length of the longer one.\n    \"\"\"\n    positive_pairs: List[InputExample] = []\n    for _, row in tqdm(df.iterrows(), desc=\"Creating positive pairs\", total=len(df)):\n        valid_synopses: List[str] = [\n            str(row[col]) for col in synopses_columns if pd.notnull(row[col])\n        ]\n        unique_synopses: List[str] = list(set(valid_synopses))  # Remove duplicates\n        if len(unique_synopses) &gt; 1:\n            # Encode all synopses\n            embeddings: NDArray[np.float64] = encoder_model.encode(\n                unique_synopses, convert_to_tensor=False\n            )\n            for i, embedding_i in enumerate(embeddings):\n                for j, embedding_j in enumerate(embeddings[i + 1 :], start=i + 1):\n                    # Check if the length condition is met\n                    longer_length: int = max(\n                        len(unique_synopses[i]), len(unique_synopses[j])\n                    )\n                    shorter_length: int = min(\n                        len(unique_synopses[i]), len(unique_synopses[j])\n                    )\n                    if shorter_length &gt;= 0.5 * longer_length:\n                        # Calculate cosine similarity\n                        similarity: float = util.pytorch_cos_sim(\n                            torch.tensor(embedding_i), torch.tensor(embedding_j)\n                        ).item()\n                        if similarity &gt;= 0.8:\n                            positive_pairs.append(\n                                InputExample(\n                                    texts=[unique_synopses[i], unique_synopses[j]],\n                                    label=float(similarity),\n                                )\n                            )  # Positive pair with semantic similarity score\n\n    # Save positive pairs\n    save_pairs_to_csv(positive_pairs, positive_pairs_file)\n    return positive_pairs\n</code></pre>"},{"location":"Training/Data/PairGeneration/#src.training.data.pair_generation.generate_negative_pairs","title":"generate_negative_pairs","text":"<pre><code>generate_negative_pairs(i, df, synopses_columns, partial_threshold, max_negative_per_row, category_to_embedding, valid_indices, max_attempts=50)\n</code></pre> <p>Generate negative pairs for a single row in the dataframe.</p> PARAMETER DESCRIPTION <code>i</code> <p>Index of the row to process</p> <p> TYPE: <code>int</code> </p> <code>df</code> <p>DataFrame containing the data</p> <p> TYPE: <code>DataFrame</code> </p> <code>synopses_columns</code> <p>List of column names containing synopses</p> <p> TYPE: <code>List[str]</code> </p> <code>partial_threshold</code> <p>Maximum similarity threshold for negatives</p> <p> TYPE: <code>float</code> </p> <code>max_negative_per_row</code> <p>Maximum number of negative pairs per row</p> <p> TYPE: <code>int</code> </p> <code>category_to_embedding</code> <p>Category embedding dict</p> <p> TYPE: <code>Dict[str, NDArray[float64]]</code> </p> <code>valid_indices</code> <p>List of valid row indices to sample from</p> <p> TYPE: <code>List[int]</code> </p> <code>max_attempts</code> <p>Max attempts to find pairs. Defaults to 50</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> RETURNS DESCRIPTION <p>List[InputExample]: List of negative pairs with similarity between 0.15 and</p> <p>partial_threshold-0.01. Each pair consists of synopses from different entries</p> <p>where the shorter synopsis is at least 50% the length of the longer one.</p> Source code in <code>src/training/data/pair_generation.py</code> <pre><code>def generate_negative_pairs(\n    i,\n    df,\n    synopses_columns,\n    partial_threshold,\n    max_negative_per_row,\n    category_to_embedding,\n    valid_indices,\n    max_attempts=50,\n):\n    \"\"\"\n    Generate negative pairs for a single row in the dataframe.\n\n    Args:\n        i (int): Index of the row to process\n        df (pd.DataFrame): DataFrame containing the data\n        synopses_columns (List[str]): List of column names containing synopses\n        partial_threshold (float): Maximum similarity threshold for negatives\n        max_negative_per_row (int): Maximum number of negative pairs per row\n        category_to_embedding (Dict[str, NDArray[np.float64]]): Category embedding dict\n        valid_indices (List[int]): List of valid row indices to sample from\n        max_attempts (int, optional): Max attempts to find pairs. Defaults to 50\n\n    Returns:\n        List[InputExample]: List of negative pairs with similarity between 0.15 and\n        partial_threshold-0.01. Each pair consists of synopses from different entries\n        where the shorter synopsis is at least 50% the length of the longer one.\n    \"\"\"\n    row_a = df.iloc[i]\n    pairs = []\n    negative_count = 0\n    row_a_negative_count = 0\n    attempts = 0\n    used_indices = set()\n\n    while attempts &lt; max_attempts and negative_count &lt; max_negative_per_row:\n        available_indices = list(set(valid_indices) - used_indices)\n        if not available_indices:\n            break\n        j = random.choice(available_indices)\n        used_indices.add(j)\n        row_b = df.iloc[j]\n        try:\n            # Check for NaN values before parsing\n            genres_a = row_a[\"genres\"]\n            genres_b = row_b[\"genres\"]\n            themes_a = row_a[\"themes\"]\n            themes_b = row_b[\"themes\"]\n\n            if (\n                pd.isna(genres_a)\n                or pd.isna(genres_b)\n                or pd.isna(themes_a)\n                or pd.isna(themes_b)\n            ):\n                continue  # Skip rows with NaN values\n\n            # Compute similarity\n            similarity = calculate_semantic_similarity(\n                category_to_embedding,\n                set(ast.literal_eval(genres_a)),\n                set(ast.literal_eval(genres_b)),\n                set(ast.literal_eval(themes_a)),\n                set(ast.literal_eval(themes_b)),\n            )\n\n            if similarity &lt;= partial_threshold - 0.01 and similarity &gt;= 0.15:\n                # If similarity is below a certain threshold, treat as a negative pair\n                valid_synopses_a = [\n                    col for col in synopses_columns if pd.notnull(row_a[col])\n                ]\n                valid_synopses_b = [\n                    col for col in synopses_columns if pd.notnull(row_b[col])\n                ]\n\n                # Only create a pair if both entries have at least one valid synopsis\n                if valid_synopses_a and valid_synopses_b:\n                    col_a = random.choice(valid_synopses_a)\n                    col_b = random.choice(valid_synopses_b)\n\n                    # Check if the length condition is met\n                    longer_length = max(len(row_a[col_a]), len(row_b[col_b]))\n                    shorter_length = min(len(row_a[col_a]), len(row_b[col_b]))\n                    if shorter_length &gt;= 0.5 * longer_length:\n                        pairs.append(\n                            InputExample(\n                                texts=[row_a[col_a], row_b[col_b]],\n                                label=similarity,  # type: ignore\n                            )\n                        )  # Partial or negative pair\n                        negative_count += 1\n                        row_a_negative_count += 1\n\n                        if row_a_negative_count &gt;= max_negative_per_row:\n                            break\n        except Exception as e:  # pylint: disable=broad-exception-caught\n            print(e)\n            continue\n        attempts += 1\n\n    return pairs\n</code></pre>"},{"location":"Training/Data/PairGeneration/#src.training.data.pair_generation.generate_partial_positive_pairs","title":"generate_partial_positive_pairs","text":"<pre><code>generate_partial_positive_pairs(i: int, df: DataFrame, synopses_columns: List[str], partial_threshold: float, max_partial_per_row: int, category_to_embedding: Dict[str, NDArray[float64]], valid_indices: List[int], max_attempts: int = 200) -&gt; List[InputExample]\n</code></pre> <p>Generate partial positive pairs for a single row in the dataframe.</p> PARAMETER DESCRIPTION <code>i</code> <p>Index of the row to process</p> <p> TYPE: <code>int</code> </p> <code>df</code> <p>DataFrame containing the data</p> <p> TYPE: <code>DataFrame</code> </p> <code>synopses_columns</code> <p>List of column names containing synopses</p> <p> TYPE: <code>List[str]</code> </p> <code>partial_threshold</code> <p>Minimum similarity threshold for partial positives</p> <p> TYPE: <code>float</code> </p> <code>max_partial_per_row</code> <p>Maximum number of partial positive pairs per row</p> <p> TYPE: <code>int</code> </p> <code>category_to_embedding</code> <p>Category embedding dict</p> <p> TYPE: <code>Dict[str, NDArray[float64]]</code> </p> <code>valid_indices</code> <p>List of valid row indices to sample from</p> <p> TYPE: <code>List[int]</code> </p> <code>max_attempts</code> <p>Max attempts to find pairs. Defaults to 200</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> RETURNS DESCRIPTION <code>List[InputExample]</code> <p>List[InputExample]: List of partial positive pairs with similarity between</p> <code>List[InputExample]</code> <p>partial_threshold+0.01 and 0.8. Each pair consists of synopses from different</p> <code>List[InputExample]</code> <p>entries where the shorter synopsis is at least 50% the length of the longer one.</p> Source code in <code>src/training/data/pair_generation.py</code> <pre><code>def generate_partial_positive_pairs(\n    i: int,\n    df: pd.DataFrame,\n    synopses_columns: List[str],\n    partial_threshold: float,\n    max_partial_per_row: int,\n    category_to_embedding: Dict[str, NDArray[np.float64]],\n    valid_indices: List[int],\n    max_attempts: int = 200,\n) -&gt; List[InputExample]:\n    \"\"\"\n    Generate partial positive pairs for a single row in the dataframe.\n\n    Args:\n        i (int): Index of the row to process\n        df (pd.DataFrame): DataFrame containing the data\n        synopses_columns (List[str]): List of column names containing synopses\n        partial_threshold (float): Minimum similarity threshold for partial positives\n        max_partial_per_row (int): Maximum number of partial positive pairs per row\n        category_to_embedding (Dict[str, NDArray[np.float64]]): Category embedding dict\n        valid_indices (List[int]): List of valid row indices to sample from\n        max_attempts (int, optional): Max attempts to find pairs. Defaults to 200\n\n    Returns:\n        List[InputExample]: List of partial positive pairs with similarity between\n        partial_threshold+0.01 and 0.8. Each pair consists of synopses from different\n        entries where the shorter synopsis is at least 50% the length of the longer one.\n    \"\"\"\n    row_a: pd.Series = df.iloc[i]\n    pairs: List[InputExample] = []\n    partial_count: int = 0\n    row_a_partial_count: int = 0\n    attempts: int = 0\n    used_indices: Set[int] = set()\n\n    while attempts &lt; max_attempts and partial_count &lt; max_partial_per_row:\n        available_indices: List[int] = list(set(valid_indices) - used_indices)\n        if not available_indices:\n            break\n        j: int = random.choice(available_indices)\n        used_indices.add(j)\n        row_b: pd.Series = df.iloc[j]\n        try:\n            genres_a: Set[str] = (\n                set(ast.literal_eval(row_a[\"genres\"]))\n                if pd.notnull(row_a[\"genres\"])\n                else set()\n            )\n            genres_b: Set[str] = (\n                set(ast.literal_eval(row_b[\"genres\"]))\n                if pd.notnull(row_b[\"genres\"])\n                else set()\n            )\n\n            themes_a: Set[str] = (\n                set(ast.literal_eval(row_a[\"themes\"]))\n                if pd.notnull(row_a[\"themes\"])\n                else set()\n            )\n            themes_b: Set[str] = (\n                set(ast.literal_eval(row_b[\"themes\"]))\n                if pd.notnull(row_b[\"themes\"])\n                else set()\n            )\n\n            # Calculate partial similarity based on genres and themes\n            similarity: float = calculate_semantic_similarity(\n                category_to_embedding, genres_a, genres_b, themes_a, themes_b\n            )\n\n            if similarity &gt;= partial_threshold + 0.01 and similarity &lt;= 0.8:\n                # If similarity is above a certain threshold, treat as a partial positive pair\n                valid_synopses_a: List[str] = [\n                    col for col in synopses_columns if pd.notnull(row_a[col])\n                ]\n                valid_synopses_b: List[str] = [\n                    col for col in synopses_columns if pd.notnull(row_b[col])\n                ]\n\n                # Only create a pair if both entries have at least one valid synopsis\n                if valid_synopses_a and valid_synopses_b:\n                    col_a: str = random.choice(valid_synopses_a)\n                    col_b: str = random.choice(valid_synopses_b)\n\n                    # Check if the length condition is met\n                    longer_length: int = max(len(row_a[col_a]), len(row_b[col_b]))\n                    shorter_length: int = min(len(row_a[col_a]), len(row_b[col_b]))\n                    if shorter_length &gt;= 0.5 * longer_length:\n                        pairs.append(\n                            InputExample(\n                                texts=[str(row_a[col_a]), str(row_b[col_b])],\n                                label=float(similarity),\n                            )\n                        )  # Partial positive pair\n                        partial_count += 1\n                        row_a_partial_count += 1\n\n                        if row_a_partial_count &gt;= max_partial_per_row:\n                            break\n        except Exception as e:  # pylint: disable=broad-exception-caught\n            logging.error(\"Error processing rows (%d, %d): %s\", i, j, e)\n            continue\n        attempts += 1\n\n    return pairs\n</code></pre>"},{"location":"Training/Models/Training/","title":"Training","text":"<p>This module provides utilities for configuring and training sentence transformer models.</p> <p>It includes functionality for:</p> <ol> <li> <p>Model initialization and configuration with optional custom transformers</p> </li> <li> <p>Evaluator setup for model validation with configurable precision</p> </li> <li> <p>Loss function selection and configuration</p> </li> </ol> The module supports multiple loss functions <ul> <li>Cosine Similarity Loss: Standard cosine similarity loss for sentence pairs</li> <li>CoSENT Loss: Contrastive sentence transformer loss</li> <li>AnglE Loss: Angular loss for sentence embeddings</li> </ul> <p>And provides flexible evaluation options with configurable precision levels.</p> FUNCTION DESCRIPTION <code>create_model</code> <p>Initialize and configure a SentenceTransformer model with optional custom transformer</p> <code>create_evaluator</code> <p>Set up an evaluator for model validation with configurable precision</p> <code>get_loss_function</code> <p>Get the appropriate loss function based on configuration name</p> <p>Type Definitions:</p> <pre><code>LossType: Union type for supported loss function instances\n\nLossFunctionType: Union type for loss function classes\n\nPrecisionType: Literal type for supported precision levels (float32, int8,\n    uint8, binary, ubinary)\n</code></pre> <p>This module is designed to work with the sentence-transformers library and supports various training configurations for fine-tuning transformer models on similarity tasks. The module allows for customization of model architecture, loss functions, and evaluation metrics.</p>"},{"location":"Training/Models/Training/#src.training.models.training.LossFunctionType","title":"LossFunctionType  <code>module-attribute</code>","text":"<pre><code>LossFunctionType = Type[Union[CosineSimilarityLoss, CoSENTLoss, AnglELoss]]\n</code></pre>"},{"location":"Training/Models/Training/#src.training.models.training.LossType","title":"LossType  <code>module-attribute</code>","text":"<pre><code>LossType = Union[CosineSimilarityLoss, CoSENTLoss, AnglELoss]\n</code></pre>"},{"location":"Training/Models/Training/#src.training.models.training.PrecisionType","title":"PrecisionType  <code>module-attribute</code>","text":"<pre><code>PrecisionType = Literal['float32', 'int8', 'uint8', 'binary', 'ubinary']\n</code></pre>"},{"location":"Training/Models/Training/#src.training.models.training.create_evaluator","title":"create_evaluator","text":"<pre><code>create_evaluator(val_pairs: List[InputExample], write_csv: bool = True, precision: PrecisionType = 'float32') -&gt; EmbeddingSimilarityEvaluator\n</code></pre> <p>Create an evaluator for model validation with configurable precision.</p> PARAMETER DESCRIPTION <code>val_pairs</code> <p>List of validation pairs containing text pairs and labels</p> <p> TYPE: <code>List[InputExample]</code> </p> <code>write_csv</code> <p>Whether to write evaluation results to CSV file</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>precision</code> <p>Precision level for similarity computation Supported values: float32, int8, uint8, binary, ubinary</p> <p> TYPE: <code>PrecisionType</code> DEFAULT: <code>'float32'</code> </p> RETURNS DESCRIPTION <code>EmbeddingSimilarityEvaluator</code> <p>Configured evaluator for computing embedding similarities</p> <p> TYPE: <code>EmbeddingSimilarityEvaluator</code> </p> Source code in <code>src/training/models/training.py</code> <pre><code>def create_evaluator(\n    val_pairs: List[InputExample],\n    write_csv: bool = True,\n    precision: PrecisionType = \"float32\",\n) -&gt; EmbeddingSimilarityEvaluator:\n    \"\"\"\n    Create an evaluator for model validation with configurable precision.\n\n    Args:\n        val_pairs (List[InputExample]): List of validation pairs containing text pairs and labels\n        write_csv (bool): Whether to write evaluation results to CSV file\n        precision (PrecisionType): Precision level for similarity computation\n            Supported values: float32, int8, uint8, binary, ubinary\n\n    Returns:\n        EmbeddingSimilarityEvaluator: Configured evaluator for computing embedding similarities\n    \"\"\"\n    val_sentences_1 = [pair.texts[0] for pair in val_pairs]\n    val_sentences_2 = [pair.texts[1] for pair in val_pairs]\n    val_labels = [float(pair.label) for pair in val_pairs]\n\n    return EmbeddingSimilarityEvaluator(\n        val_sentences_1,\n        val_sentences_2,\n        val_labels,\n        main_similarity=\"cosine\",\n        write_csv=write_csv,\n        precision=precision,\n    )\n</code></pre>"},{"location":"Training/Models/Training/#src.training.models.training.create_model","title":"create_model","text":"<pre><code>create_model(model_name: str, use_custom_transformer: bool = False, max_seq_length: int = 843, device: Optional[str] = None) -&gt; SentenceTransformer\n</code></pre> <p>Create a SentenceTransformer model, optionally using a custom transformer.</p> <p>The model architecture consists of: 1. Transformer layer (custom or default) 2. Pooling layer 3. Dense layer with identity activation 4. Normalization layer</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name or path of the pre-trained Transformer model</p> <p> TYPE: <code>str</code> </p> <code>use_custom_transformer</code> <p>Whether to use custom transformer with modified activations</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>max_seq_length</code> <p>Maximum sequence length for the Transformer</p> <p> TYPE: <code>int</code> DEFAULT: <code>843</code> </p> <code>device</code> <p>Device to run the model on (e.g. 'cuda', 'cpu')</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SentenceTransformer</code> <p>The initialized SentenceTransformer model with the specified architecture</p> <p> TYPE: <code>SentenceTransformer</code> </p> Source code in <code>src/training/models/training.py</code> <pre><code>def create_model(\n    model_name: str,\n    use_custom_transformer: bool = False,\n    max_seq_length: int = 843,\n    device: Optional[str] = None,\n) -&gt; SentenceTransformer:\n    \"\"\"\n    Create a SentenceTransformer model, optionally using a custom transformer.\n\n    The model architecture consists of:\n    1. Transformer layer (custom or default)\n    2. Pooling layer\n    3. Dense layer with identity activation\n    4. Normalization layer\n\n    Args:\n        model_name (str): Name or path of the pre-trained Transformer model\n        use_custom_transformer (bool): Whether to use custom transformer with\n            modified activations\n        max_seq_length (int): Maximum sequence length for the Transformer\n        device (Optional[str]): Device to run the model on (e.g. 'cuda', 'cpu')\n\n    Returns:\n        SentenceTransformer: The initialized SentenceTransformer model with the\n            specified architecture\n    \"\"\"\n    modules: List[nn.Module] = []\n\n    if use_custom_transformer:\n        # Instantiate the custom transformer\n        custom_transformer = CustomT5EncoderModel(\n            model_name_or_path=model_name,\n            model_args={},\n            max_seq_length=max_seq_length,\n            do_lower_case=False,\n        )\n        modules.append(custom_transformer)\n    else:\n        # Default transformer\n        transformer = models.Transformer(model_name, max_seq_length=max_seq_length)\n        modules.append(transformer)\n\n    # Add the Pooling layer\n    pooling = models.Pooling(modules[0].get_word_embedding_dimension())\n    modules.append(pooling)\n\n    # Add the Dense layer\n    dense = models.Dense(\n        in_features=modules[0].get_word_embedding_dimension(),\n        out_features=modules[0].get_word_embedding_dimension(),\n        activation_function=nn.Identity(),  # type: ignore\n        bias=False,\n    )\n    modules.append(dense)\n\n    # Add the Normalize layer\n    normalize = models.Normalize()\n    modules.append(normalize)\n\n    # Build the SentenceTransformer model\n    model = SentenceTransformer(modules=modules, device=device)\n    return model\n</code></pre>"},{"location":"Training/Models/Training/#src.training.models.training.get_loss_function","title":"get_loss_function","text":"<pre><code>get_loss_function(loss_name: str, model: SentenceTransformer) -&gt; LossType\n</code></pre> <p>Get the appropriate loss function based on the specified name.</p> PARAMETER DESCRIPTION <code>loss_name</code> <p>Name of the loss function to use Supported values: 'cosine', 'cosent', 'angle'</p> <p> TYPE: <code>str</code> </p> <code>model</code> <p>The model to associate with the loss function</p> <p> TYPE: <code>SentenceTransformer</code> </p> RETURNS DESCRIPTION <code>LossType</code> <p>Configured loss function instance</p> <p> TYPE: <code>LossType</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If an unsupported loss function name is provided</p> Source code in <code>src/training/models/training.py</code> <pre><code>def get_loss_function(loss_name: str, model: SentenceTransformer) -&gt; LossType:\n    \"\"\"\n    Get the appropriate loss function based on the specified name.\n\n    Args:\n        loss_name (str): Name of the loss function to use\n            Supported values: 'cosine', 'cosent', 'angle'\n        model (SentenceTransformer): The model to associate with the loss function\n\n    Returns:\n        LossType: Configured loss function instance\n\n    Raises:\n        ValueError: If an unsupported loss function name is provided\n    \"\"\"\n    loss_functions: Dict[str, LossFunctionType] = {\n        \"cosine\": losses.CosineSimilarityLoss,\n        \"cosent\": losses.CoSENTLoss,\n        \"angle\": losses.AnglELoss,\n    }\n\n    if loss_name not in loss_functions:\n        raise ValueError(f\"Unsupported loss function: {loss_name}\")\n\n    return loss_functions[loss_name](model=model)\n</code></pre>"}]}